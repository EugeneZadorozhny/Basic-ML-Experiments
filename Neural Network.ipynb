{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Abalone\n",
    "`load_abalone` loads the csv file from the provided path and sets the column names. Also deletes unwanted features from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_abalone(path):\n",
    "    df = pd.read_csv(path, header = None)\n",
    "    df.set_axis([\"sex\", \"length\", \"diameter\", \"height\", \"whole weight\", \"shucked weight\",\n",
    "                 \"viscera weight\", \"shell weight\", \"rings\"], axis=1, inplace=True)\n",
    "    df = df.drop(columns = ['sex'])\n",
    "    df = normalize(df, list(df.columns), 0, 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Breast Cancer\n",
    "`load_breast_cancer` loads the csv file from the provided path and sets the column names. Also deletes unwanted features from the dataset and imputes missing values with the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_breast_cancer(path):\n",
    "    df = pd.read_csv(path, header = None)\n",
    "    df.set_axis([\"sample code number\", \"clump thickness\", \"uniformity of cell size\", \"uniformity of cell shape\",\n",
    "                 \"marginal adhesion\", \"single epithelial cell size\",\n",
    "                 \"bare nuclei\", \"bland chromatin\", \"normal nucleoli\", \"mitoses\", \"class\"], axis=1, inplace=True)\n",
    "    df = df.drop(columns = [\"sample code number\"])\n",
    "    df = df.replace('?', np.nan)\n",
    "    df['bare nuclei'] = df['bare nuclei'].astype(float)\n",
    "    df = impute_mean(df, 'bare nuclei', int)\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = one_hot_encode(df, list(df.columns))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Car\n",
    "`load_car` loads the csv file from the provided path and sets the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_car(path):\n",
    "    df = pd.read_csv(path, header = None)\n",
    "    df.set_axis([\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\", \"class\"], axis=1, inplace=True)\n",
    "    df = one_hot_encode(df, list(df.columns))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Forest Fires\n",
    "`load_forest_fires` loads the csv file from the provided path and sets the column names. Also deletes unwanted features from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_forest_fires(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.drop(columns=['X', 'Y', 'month', 'day'])\n",
    "    df = normalize(df, list(df.columns), 0, 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load House Votes\n",
    "`load_house_votes` loads the csv file from the provided path and sets the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_house_votes(path):\n",
    "    df = pd.read_csv(path, header = None)\n",
    "    df.set_axis([\"class\", \"handicapped infants\", \"water project cost sharing\", \"adoption of the budget resolution\",\n",
    "                 \"physician fee freeze\", \"el salvador aid\", \"religious groups in schools\", \"anti satellite test ban\",\n",
    "                 \"aid to nicaraguan contras\", \"mx missile\", \"immigration\", \"synfuels corporation cutback\",\n",
    "                 \"education spending\", \"superfund right to sue\", \"crime\", \"duty free exports\",\n",
    "                 \"export administration act south africa\"], axis=1, inplace=True)\n",
    "    df = one_hot_encode(df, list(df.columns))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Machine\n",
    "`load_machine` loads the csv file from the provided path and sets the column names. Also deletes unwanted features from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_machine(path):\n",
    "    df = pd.read_csv(path, header = None)\n",
    "    df.set_axis([\"vendor\", \"model\", \"MYCT\", \"MMIN\", \"MMAX\", \"CACH\", \"CHMIN\", \"CHMAX\", \"PRP\", \"ERP\"], axis=1, inplace=True)\n",
    "    df = df.drop([\"vendor\",\"model\", \"ERP\"], axis=1)\n",
    "    df = normalize(df, [\"MYCT\", \"MMIN\", \"MMAX\", \"CACH\", \"CHMIN\", \"CHMAX\", \"PRP\"], 0, 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute Mean\n",
    "`impute_mean` updates the missing values in a column of the dataset with the mean of the specified column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_mean(data, column, data_type):\n",
    "    _data = deepcopy(data)\n",
    "    mean = _data[column].mean()\n",
    "    _data[column] = _data[column].fillna(mean)\n",
    "    if data_type == int:\n",
    "        _data[column] = _data[column].astype(np.int64)\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Ordinal\n",
    "`encode_ordinal` encodes ordinal values of a feature based on the relationship list provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_ordinal(data, column, relationship):\n",
    "    _data = deepcopy(data)\n",
    "    for index, r in enumerate(relationship):\n",
    "        _data[column] = _data[column].replace(r, index)\n",
    "        print(f\"encoding column: {column} - value: {r} as: {index}\")\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encode\n",
    "`one_hot_encode` encodes features of the desired columns with one hot values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(data, columns):\n",
    "    _data = deepcopy(data)\n",
    "    for _column in columns:\n",
    "        _data2 = pd.get_dummies(_data[_column]).groupby(level=0,axis=1).max().add_prefix(_column + ' - ')\n",
    "        _data = pd.concat([_data, _data2], axis=1)\n",
    "        _data = _data.drop([_column], axis=1)\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretize Feature\n",
    "`discretize_feature` bins feature values based on the desired bin type. Frequency type tries to ensure the same number of data points are in each bin and bins can vary in width. Width type specifies the bin width so the number on points in each bin can vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_feature(data, columns, number_bins, type_bins):\n",
    "    _data = deepcopy(data)\n",
    "    for _column in columns:\n",
    "        if type_bins == \"frequency\":\n",
    "            _data2 = pd.qcut(_data[_column], q = number_bins, precision=0, duplicates='drop')\n",
    "            _data[_column] = _data2\n",
    "\n",
    "        if type_bins == \"width\":\n",
    "            _data2 = pd.cut(_data[_column], number_bins)\n",
    "            _data[_column] = _data2\n",
    "\n",
    "    return _data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize\n",
    "`standardize` updates values in training data and test data to the z standard using the mean and standard deviation from the training set for both calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(training_data, test_data, columns):\n",
    "    _data_train = deepcopy(training_data)\n",
    "    _data_test = deepcopy(test_data)\n",
    "    for _column in columns:\n",
    "        mean = _data_train[_column].mean()\n",
    "        std = _data_train[_column].std()\n",
    "        z_train = (_data_train[_column] - mean)/std\n",
    "        z_test = (_data_test[_column] - mean)/std\n",
    "\n",
    "        _data_train[_column] = z_train\n",
    "        _data_test[_column] = z_test\n",
    "    return _data_train, _data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def normalize(data, columns, min, max):\n",
    "    _data = deepcopy(data)\n",
    "    for c_index, _column in enumerate(columns):\n",
    "        x_max = _data[_column].max()\n",
    "        x_min = _data[_column].min()\n",
    "        _data[_column] = min + ((_data[_column] - x_min)*(max - min)/(x_max - x_min))\n",
    "    return _data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Validation Set\n",
    "`extract_validation_set` extracts a random 20% sample for the validation set. Evenly samples the groupings of the output. Randomly shuffles the remaining 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_validation_set(data, class_column):\n",
    "    _data = deepcopy(data)\n",
    "    y = class_column\n",
    "\n",
    "    stratify_20 = _data.groupby(y, group_keys=False).sample(frac=0.20)\n",
    "    stratify_80 = _data.drop(stratify_20.index).sample(frac=1)\n",
    "\n",
    "    return stratify_80, stratify_20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train Test\n",
    "`create_train_test` takes in the folds of a k-fold to create a test set from 1 fold and a training set from the remaing 4 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(folds, index):\n",
    "    training = pd.DataFrame()\n",
    "    test = []\n",
    "    for i, fold in enumerate(folds):\n",
    "        if i == index:\n",
    "            test = fold\n",
    "        else:\n",
    "            training = pd.concat([training, fold])\n",
    "    return training, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified K Fold\n",
    "`stratified_k_fold` takes the provided dataset and creates a specified number of evenly distributed folds of that data while maintaining output grouping distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_k_fold(data, k, class_column):\n",
    "    y = class_column\n",
    "    split = []\n",
    "    unique_keys = data.value_counts(subset=y, normalize=True).keys()\n",
    "    split_class = [data.loc[data[y] == keys] for keys in unique_keys]\n",
    "\n",
    "    for class_value in split_class:\n",
    "        d, m = divmod(len(class_value), k)\n",
    "        split.append(list(class_value[i * d + min(i, m):(i + 1) * d + min(i + 1, m)] for i in range(k)))\n",
    "\n",
    "    folds = [pd.concat([split[i][c] for i in range(len(unique_keys))]) for c in range(k)]\n",
    "\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Fold\n",
    "`k_fold` takes the provided dataset and creates a specified number of evenly distributed folds of that data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(data, k):\n",
    "\n",
    "    d, m = divmod(len(data), k)\n",
    "    folds = list(data[i * d + min(i, m):(i + 1) * d + min(i + 1, m)] for i in range(k))\n",
    "\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Fold Cross Validation Sets\n",
    "`k_fold_cross_validation_sets` splits the data into validation and train/test sets. Folds the sets k times. Can fold with stratification or without stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation_sets(data, k, class_column, stratified=True, validation=True):\n",
    "    if validation:\n",
    "        train, validation = extract_validation_set(data, class_column)\n",
    "        if stratified:\n",
    "            train = stratified_k_fold(train, k, class_column)\n",
    "            #validation = stratified_k_fold(validation, k, class_column)\n",
    "        else:\n",
    "            train = k_fold(train, k)\n",
    "            #validation = k_fold(validation, k)\n",
    "        return train, validation\n",
    "    else:\n",
    "        if stratified:\n",
    "            train = stratified_k_fold(data, k, class_column)\n",
    "        else:\n",
    "            train = k_fold(data, k)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "`evaluation` applies an evaluation metric on the predicted values. Able to use a classification score by comparing ground truth to the predicted values to determine how many are correct. Able to use Mean Square Error to determine the distance between the ground truth and the predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(ground_truth, predicted_values, metric):\n",
    "    if metric == 'classification_score':\n",
    "        count = 0\n",
    "        for index, value in enumerate(ground_truth):\n",
    "            if predicted_values[index] == value:\n",
    "                count += 1\n",
    "        count = count/len(ground_truth)\n",
    "        return count\n",
    "    if metric == 'mse':\n",
    "        error = sum((np.array(ground_truth) - np.array(predicted_values))**2)/len(ground_truth)\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Linear Classification Network\n",
    "`train_linear_classification_network` trains a single softmax layer for the output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_classification_network(train_features, train_class, eta = 0.2, epochs = 20):\n",
    "    # initialize weights\n",
    "    weights = np.random.uniform(-0.01, 0.01, (len(list(train_class.columns)), (len(list(train_features.columns)))))\n",
    "    # run linear network training\n",
    "    result = 0\n",
    "    epoch = 0\n",
    "    while (result < 1.0) and (epoch < epochs):\n",
    "        train_features = train_features.sample(frac=1)\n",
    "        for index, x in train_features.iterrows():\n",
    "            y = np.array(train_class.loc[index]).ravel()\n",
    "            # determine output for each class prediction\n",
    "            activity = np.array(x) * weights\n",
    "            output_classes = np.exp(activity.sum(axis=1))/np.sum(np.exp(activity.sum(axis=1)))\n",
    "\n",
    "            # update weights\n",
    "            for i, value in enumerate(weights):\n",
    "                weights[i] = weights[i] + eta*(y[i] - output_classes[i]) * x\n",
    "\n",
    "        # calculate total error\n",
    "        prediction = predict_linear_network(train_features.sort_index(), weights)\n",
    "        error = cross_entropy(prediction, np.array(train_class.sort_index()))\n",
    "        result = classification_result(prediction, np.array(train_class.sort_index()))\n",
    "        print(f'training epoch {epoch+1}: error: {error} classification score: {result}')\n",
    "        epoch += 1\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Linear Regression Network\n",
    "`train_linear_regression_network` trains a single linear perceptron for the output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def train_linear_regression_network(train_features, train_class, eta = 0.002, epochs = 20):\n",
    "    # initialize weights\n",
    "    weights = np.random.uniform(-0.01, 0.01, (len(list(train_class.columns)), (len(list(train_features.columns)))))\n",
    "    # run linear network training\n",
    "    epoch = 0\n",
    "    error = 1\n",
    "    while (error > 0) and (epoch < epochs):\n",
    "        train_features = train_features.sample(frac=1)\n",
    "        for index, x in train_features.iterrows():\n",
    "            y = np.array(train_class.loc[index]).ravel()\n",
    "            x = np.array(x)\n",
    "            # determine output for each class prediction\n",
    "            activity = np.sum(x * weights)\n",
    "            weights = weights + eta*(y - activity) * x\n",
    "\n",
    "        # calculate total error\n",
    "        prediction = predict_regression_network(train_features.sort_index(), weights)\n",
    "        error = mean_squared_error(prediction, np.array(train_class.sort_index()))\n",
    "        print(f'training epoch {epoch+1}: mse: {error}')\n",
    "        epoch += 1\n",
    "    return weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialize Weights\n",
    "`initialize_weights` set random weights between -0.01 and 0.01 for the network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(train_features, train_class, hidden_layer_sizes = (10,)):\n",
    "    weights = []\n",
    "    activation = []\n",
    "    for layer, size in enumerate(hidden_layer_sizes):\n",
    "        # weights for input layer\n",
    "        if layer == 0:\n",
    "            weights.append(np.random.uniform(-0.01, 0.01, (size, len(list(train_features.columns)))))\n",
    "            activation.append(np.zeros(size))\n",
    "        # weights for hidden layer\n",
    "        else:\n",
    "            weights.append(np.random.uniform(-0.01, 0.01, (size, len(weights[layer-1]))))\n",
    "            activation.append(np.zeros(size))\n",
    "    # weights for output layer\n",
    "    weights.append(np.random.uniform(-0.01,0.01, (len(list(train_class.columns)), hidden_layer_sizes[-1])))\n",
    "    activation.append(np.zeros(len(list(train_class.columns))))\n",
    "    return weights, activation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feed Forward\n",
    "`feed_forward` feed a sample through the network to get an output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(x, weights, activation):\n",
    "    for layer, layer_weights in enumerate(weights):\n",
    "        # calculate input layer activation\n",
    "        if layer == 0:\n",
    "            activation[layer] = 1/(1 + np.exp(-1 * (np.sum(layer_weights * x, axis=1))))\n",
    "        # calculate softmax layer output\n",
    "        elif layer == len(weights)-1:\n",
    "            activation[layer] = np.sum(layer_weights * activation[layer-1], axis=1)\n",
    "            activation[layer] = np.exp(activation[layer])/np.sum(np.exp(activation[layer]))\n",
    "        # calculate hidden layer activation\n",
    "        else:\n",
    "            activation[layer] = 1/(1 + np.exp(-1 * (np.sum(layer_weights * activation[layer-1], axis=1))))\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Update Weights\n",
    "`update_weights` update the weights of the network based on the prediction and known output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(weights, activation, x, y, eta):\n",
    "    delta_weights = deepcopy(weights)\n",
    "    for layer, layer_weights in enumerate(reversed(weights)):\n",
    "        layer_index = -layer-1\n",
    "        # output layer update\n",
    "        if layer_index == -1:\n",
    "            delta = y - activation[layer_index]\n",
    "            for index, w in enumerate(weights[layer_index]):\n",
    "                delta_weights[layer_index][index] = (eta * delta[index] * activation[layer_index-1])\n",
    "        # input layer update\n",
    "        elif layer_index == -len(weights):\n",
    "            delta = np.sum(np.transpose(np.transpose(weights[layer_index+1])*delta), axis=0) * (1 - activation[layer_index])*activation[layer_index]\n",
    "            for dw_index, dw_layer in enumerate(activation[layer_index]):\n",
    "                delta_weights[layer_index][dw_index] = eta * delta[dw_index] * x\n",
    "        # hidden layer update\n",
    "        else:\n",
    "            delta = np.sum(np.transpose(np.transpose(weights[layer_index+1])*delta), axis=0) * (1 - activation[layer_index])*activation[layer_index]\n",
    "            for dw_index, dw_layer in enumerate(activation[layer_index]):\n",
    "                delta_weights[layer_index][dw_index] = eta * delta[dw_index] * activation[layer_index-1]\n",
    "    for index, layers in enumerate(weights):\n",
    "        weights[index] = weights[index] + delta_weights[index]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Prediction\n",
    "`get_prediction` return a list of predictions from a list of samples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def get_prediction(features, weights, activation):\n",
    "    prediction = []\n",
    "    for x in features:\n",
    "        output = feed_forward(x, weights, activation)\n",
    "        prediction.append(output[-1])\n",
    "    return prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Layered Classification Network\n",
    "`train_layered_classification_network` train a network of a specified size for classification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_layered_classification_network(_train_features, _train_class, hidden_layer_sizes, eta = 0.2, epochs = 20):\n",
    "    result = 0\n",
    "    epoch = 0\n",
    "    weights, activation = initialize_weights(_train_features, _train_class, hidden_layer_sizes)\n",
    "    train_features = _train_features.to_numpy()\n",
    "    train_class = _train_class.to_numpy()\n",
    "\n",
    "    while (result < 1.0) and (epoch < epochs):\n",
    "        # random sample indexes\n",
    "        sample = np.random.choice(len(train_features), len(train_features), replace=False)\n",
    "        for index, x in enumerate(train_features):\n",
    "            y = train_class[sample[index]]\n",
    "            x = train_features[sample[index]]\n",
    "            # determine output for each class prediction\n",
    "            activation = feed_forward(x, weights, activation)\n",
    "            weights = update_weights(weights, activation, x, y, eta)\n",
    "        # calculate total error\n",
    "        prediction = get_prediction(train_features, weights, activation)\n",
    "        error = cross_entropy(prediction, train_class)\n",
    "        result = classification_result(prediction, train_class)\n",
    "        print(f'training epoch {epoch+1}: error: {error} classification score: {result}')\n",
    "        epoch += 1\n",
    "    return weights, activation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feed Forward Regression\n",
    "`feed_forward_regression` feed a sample through the regression network to get an output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def feed_forward_regression(x, weights, activation):\n",
    "    for layer, layer_weights in enumerate(weights):\n",
    "        # calculate input layer activation\n",
    "        if layer == 0:\n",
    "            activation[layer] = 1/(1 + np.exp(-1 * (np.sum(layer_weights * x, axis=1))))\n",
    "        # calculate softmax layer output\n",
    "        elif layer == len(weights)-1:\n",
    "            activation[layer] = np.sum(layer_weights * activation[layer-1], axis=1)\n",
    "        # calculate hidden layer activation\n",
    "        else:\n",
    "            activation[layer] = 1/(1 + np.exp(-1 * (np.sum(layer_weights * activation[layer-1], axis=1))))\n",
    "    return activation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Prediction Regression\n",
    "`get_prediction_regression` return a list of predictions from a list of samples for the regression network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def get_prediction_regression(features, weights, activation):\n",
    "    prediction = []\n",
    "    for x in features:\n",
    "        output = feed_forward_regression(x, weights, activation)\n",
    "        prediction.append(output[-1])\n",
    "    return prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Layered Regression Network\n",
    "`train_layered_regression_network` train a network of a specified size for regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def train_layered_regression_network(_train_features, _train_class, hidden_layer_sizes, eta = 0.2, epochs = 20):\n",
    "    result = 0\n",
    "    epoch = 0\n",
    "    weights, activation = initialize_weights(_train_features, _train_class, hidden_layer_sizes)\n",
    "    train_features = _train_features.to_numpy()\n",
    "    train_class = _train_class.to_numpy()\n",
    "\n",
    "    while (result < 1.0) and (epoch < epochs):\n",
    "        # random sample indexes\n",
    "        sample = np.random.choice(len(train_features), len(train_features), replace=False)\n",
    "        for index, x in enumerate(train_features):\n",
    "            y = train_class[sample[index]]\n",
    "            x = train_features[sample[index]]\n",
    "            # determine output for each class prediction\n",
    "            activation = feed_forward_regression(x, weights, activation)\n",
    "            weights = update_weights(weights, activation, x, y, eta)\n",
    "        # calculate total error\n",
    "        prediction = get_prediction_regression(train_features, weights, activation)\n",
    "        error = mean_squared_error(prediction, train_class)\n",
    "        print(f'training epoch {epoch+1}: error: {error}')\n",
    "        epoch += 1\n",
    "    return weights, activation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predict linear network\n",
    "`predict_linear_network` softmax output for linear network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_linear_network(data, network):\n",
    "    predict = []\n",
    "    for index, value in data.iterrows():\n",
    "        activation = []\n",
    "        activity = np.dot(network, np.array(value))\n",
    "        for p in activity:\n",
    "            activation.append(np.exp(p)/np.sum(np.exp(activity)))\n",
    "        predict.append(activation)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predict regression network\n",
    "`predict_regression_network` regression output for linear network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def predict_regression_network(data, network):\n",
    "    predict = []\n",
    "    for index, value in data.iterrows():\n",
    "        activation = np.dot(network, np.array(value))\n",
    "        predict.append(activation)\n",
    "    return predict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross Entropy\n",
    "`cross_entropy` calculate cross entropy loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(prediction, ground_truth):\n",
    "    error = 0\n",
    "    for index, truth in enumerate(ground_truth):\n",
    "        entropy = -1*np.sum(truth*np.log2(prediction[index]))\n",
    "        error = error + entropy\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mean Squared Error\n",
    "`mean_squared_error` calculate mean squared error loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def mean_squared_error(prediction, ground_truth):\n",
    "    mse = 0.5 * np.sum((prediction - ground_truth)**2)\n",
    "    return mse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification Result\n",
    "`classification_result` determine if output classified sample correctly"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_result(prediction, ground_truth):\n",
    "    count =0\n",
    "    for index, value in enumerate(prediction):\n",
    "        predict = np.argmax(value)\n",
    "        if ground_truth[index][predict] == 1:\n",
    "            count += 1\n",
    "    return count/len(ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialize Autoencoder Weights\n",
    "`initialize_autoencoder_weights` set random weights between -0.01 and 0.01 for the network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def initialize_autoencoder_weights(train_features, hidden_layer_sizes = (10,)):\n",
    "    weights = []\n",
    "    activation = []\n",
    "\n",
    "    weights.append(np.random.uniform(-0.01, 0.01, (hidden_layer_sizes[0], len(list(train_features.columns)))))\n",
    "    activation.append(np.zeros(hidden_layer_sizes[0]))\n",
    "\n",
    "    weights.append(np.random.uniform(-0.01,0.01, (len(list(train_features.columns)), hidden_layer_sizes[0])))\n",
    "    activation.append(np.zeros(len(list(train_features.columns))))\n",
    "    return weights, activation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def feed_forward_autoencoder(x, weights, activation):\n",
    "    for layer, layer_weights in enumerate(weights):\n",
    "        # calculate input layer activation\n",
    "        if layer == 0:\n",
    "            activation[layer] = 1/(1 + np.exp(-1 * (np.sum(layer_weights * x, axis=1))))\n",
    "        # calculate output layer\n",
    "        else:\n",
    "            activation[layer] = 1/(1 + np.exp(-1 * (np.sum(layer_weights * activation[layer-1], axis=1))))\n",
    "    return activation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def get_prediction_autoencoder(features, weights, activation):\n",
    "    prediction = []\n",
    "    for x in features:\n",
    "        output = feed_forward_autoencoder(x, weights, activation)\n",
    "        prediction.append(output[-1])\n",
    "    return prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def autoencoder_result(prediction, ground_truth):\n",
    "    distance = []\n",
    "    for index, value in enumerate(prediction):\n",
    "        for i, x_hat in enumerate(value):\n",
    "            if x_hat > 0.5:\n",
    "                value[i] = 1\n",
    "            else:\n",
    "                value[i] = 0\n",
    "        distance.append(abs(ground_truth[index] - value))\n",
    "    distance = np.sum(distance)\n",
    "    return distance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "def autoencoder_result_regression(prediction, ground_truth):\n",
    "    distance = []\n",
    "    for index, value in enumerate(prediction):\n",
    "        distance.append(abs(ground_truth[index] - value))\n",
    "    distance = np.sum(distance)\n",
    "    return distance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def train_autoencoder_network(_train_features, _train_class, hidden_layer_sizes, eta = 0.2, epochs = 20):\n",
    "    auto_result = 1\n",
    "    result = 0\n",
    "    epoch = 0\n",
    "    auto_weights, auto_activation = initialize_autoencoder_weights(_train_features, hidden_layer_sizes)\n",
    "    weights, activation = initialize_weights(_train_features, _train_class, hidden_layer_sizes)\n",
    "    train_features = _train_features.to_numpy()\n",
    "    train_class = _train_class.to_numpy()\n",
    "\n",
    "    while (auto_result > 0) and (epoch < 200):\n",
    "        sample = np.random.choice(len(train_features), len(train_features), replace=False)\n",
    "        for index, x in enumerate(train_features):\n",
    "            x = train_features[sample[index]]\n",
    "            # determine output for each class prediction\n",
    "            auto_activation = feed_forward_autoencoder(x, auto_weights, auto_activation)\n",
    "            auto_weights = update_weights(auto_weights, auto_activation, x, x, eta)\n",
    "        prediction = get_prediction_autoencoder(train_features, auto_weights, auto_activation)\n",
    "        auto_result = autoencoder_result(prediction, train_features)\n",
    "        print(f'autoencoder training epoch {epoch+1}: decode layer difference: {auto_result}')\n",
    "        epoch += 1\n",
    "\n",
    "    weights[0] = auto_weights[0]\n",
    "    epoch = 0\n",
    "\n",
    "    while (result < 1.0) and (epoch < epochs):\n",
    "        sample = np.random.choice(len(train_features), len(train_features), replace=False)\n",
    "        for index, x in enumerate(train_features):\n",
    "            y = train_class[sample[index]]\n",
    "            x = train_features[sample[index]]\n",
    "            # determine output for each class prediction\n",
    "            activation = feed_forward(x, weights, activation)\n",
    "            weights = update_weights(weights, activation, x, y, eta)\n",
    "        # calculate total error\n",
    "        prediction = get_prediction(train_features, weights, activation)\n",
    "        error = cross_entropy(prediction, train_class)\n",
    "        result = classification_result(prediction, train_class)\n",
    "        print(f'training epoch {epoch+1}: error: {error} classification score: {result}')\n",
    "        epoch += 1\n",
    "    return weights, activation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def train_autoencoder_network_regression(_train_features, _train_class, hidden_layer_sizes, eta = 0.2, epochs = 20):\n",
    "    auto_result = 1\n",
    "    result = 0\n",
    "    epoch = 0\n",
    "    auto_weights, auto_activation = initialize_autoencoder_weights(_train_features, hidden_layer_sizes)\n",
    "    weights, activation = initialize_weights(_train_features, _train_class, hidden_layer_sizes)\n",
    "    train_features = _train_features.to_numpy()\n",
    "    train_class = _train_class.to_numpy()\n",
    "\n",
    "    while (auto_result > 0) and (epoch < 200):\n",
    "        sample = np.random.choice(len(train_features), len(train_features), replace=False)\n",
    "        for index, x in enumerate(train_features):\n",
    "            x = train_features[sample[index]]\n",
    "            # determine output for each class prediction\n",
    "            auto_activation = feed_forward_autoencoder(x, auto_weights, auto_activation)\n",
    "            auto_weights = update_weights(auto_weights, auto_activation, x, x, eta)\n",
    "        prediction = get_prediction_autoencoder(train_features, auto_weights, auto_activation)\n",
    "        auto_result = autoencoder_result(prediction, train_features)\n",
    "        print(f'autoencoder training epoch {epoch+1}: decode layer difference: {auto_result}')\n",
    "        epoch += 1\n",
    "\n",
    "    weights[0] = auto_weights[0]\n",
    "    epoch = 0\n",
    "\n",
    "    while (result < 1.0) and (epoch < epochs):\n",
    "        sample = np.random.choice(len(train_features), len(train_features), replace=False)\n",
    "        for index, x in enumerate(train_features):\n",
    "            y = train_class[sample[index]]\n",
    "            x = train_features[sample[index]]\n",
    "            # determine output for each class prediction\n",
    "            activation = feed_forward_regression(x, weights, activation)\n",
    "            weights = update_weights(weights, activation, x, y, eta)\n",
    "        # calculate total error\n",
    "        prediction = get_prediction_regression(train_features, weights, activation)\n",
    "        error = mean_squared_error(prediction, train_class)\n",
    "        print(f'training epoch {epoch+1}: error: {error}')\n",
    "        epoch += 1\n",
    "    return weights, activation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(data, class_column, z_norm=False):\n",
    "    error = []\n",
    "    result = []\n",
    "    #creates the validation and test/train sets\n",
    "    if z_norm:\n",
    "        folds, validation = k_fold_cross_validation_sets(data, 5, list(data.columns[data.columns.str.contains(class_column)])[0], stratified=False)\n",
    "    else:\n",
    "        folds, validation = k_fold_cross_validation_sets(data, 5, list(data.columns[data.columns.str.contains(class_column)])[0])\n",
    "    #run each fold as the test set\n",
    "    for index, fold in enumerate(folds):\n",
    "        #create train test sets from folds\n",
    "        train, test = create_train_test(folds, index)\n",
    "        if z_norm:\n",
    "            x_train = train[train.columns[~train.columns.str.contains(class_column)]]\n",
    "            y_train = train[train.columns[train.columns.str.contains(class_column)]]\n",
    "            x_test = test[test.columns[~test.columns.str.contains(class_column)]]\n",
    "            y_test = test[test.columns[test.columns.str.contains(class_column)]]\n",
    "            network = train_linear_regression_network(x_train, y_train)\n",
    "            prediction = predict_regression_network(x_test, network)\n",
    "            error.append(mean_squared_error(prediction, np.array(y_test)))\n",
    "            print(f'\\nfold {index+1}: mse: {error[index]}\\n')\n",
    "        else:\n",
    "            x_train = train[train.columns[~train.columns.str.contains(class_column)]]\n",
    "            y_train = train[train.columns[train.columns.str.contains(class_column)]]\n",
    "            x_test = test[test.columns[~test.columns.str.contains(class_column)]]\n",
    "            y_test = test[test.columns[test.columns.str.contains(class_column)]]\n",
    "            network = train_linear_classification_network(x_train, y_train)\n",
    "            prediction = predict_linear_network(x_test, network)\n",
    "            error.append(cross_entropy(prediction, np.array(y_test)))\n",
    "            result.append(classification_result(prediction, np.array(y_test)))\n",
    "            print(f'\\nfold {index+1}: error: {error[index]} classification score: {result[index]}\\n')\n",
    "    if z_norm:\n",
    "        average_error = sum(error)/len(error)\n",
    "        print(f'average error: {average_error}')\n",
    "    else:\n",
    "        #determine the average of the performance metric over all folds\n",
    "        average_class = sum(result)/len(result)\n",
    "        average_error = sum(error)/len(error)\n",
    "        print(f'average error: {average_error} average classification score {average_class}')\n",
    "    return average_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def cross_validation_layered(data, class_column, classifier, network_size, z_norm=False):\n",
    "    error = []\n",
    "    result = []\n",
    "    #creates the validation and test/train sets\n",
    "    if z_norm:\n",
    "        folds, validation = k_fold_cross_validation_sets(data, 5, list(data.columns[data.columns.str.contains(class_column)])[0], stratified=False)\n",
    "    else:\n",
    "        folds, validation = k_fold_cross_validation_sets(data, 5, list(data.columns[data.columns.str.contains(class_column)])[0])\n",
    "    #run each fold as the test set\n",
    "    for index, fold in enumerate(folds):\n",
    "        #create train test sets from folds\n",
    "        train, test = create_train_test(folds, index)\n",
    "        if z_norm:\n",
    "            x_train = train[train.columns[~train.columns.str.contains(class_column)]]\n",
    "            y_train = train[train.columns[train.columns.str.contains(class_column)]]\n",
    "            x_test = test[test.columns[~test.columns.str.contains(class_column)]]\n",
    "            y_test = test[test.columns[test.columns.str.contains(class_column)]]\n",
    "            network, activation = classifier(x_train, y_train, network_size)\n",
    "            prediction = get_prediction_regression(np.array(x_test), network, activation)\n",
    "            error.append(mean_squared_error(prediction, np.array(y_test)))\n",
    "            print(f'\\nfold {index+1}: mse: {error[index]}\\n')\n",
    "        else:\n",
    "            x_train = train[train.columns[~train.columns.str.contains(class_column)]]\n",
    "            y_train = train[train.columns[train.columns.str.contains(class_column)]]\n",
    "            x_test = test[test.columns[~test.columns.str.contains(class_column)]]\n",
    "            y_test = test[test.columns[test.columns.str.contains(class_column)]]\n",
    "            network, activation = classifier(x_train, y_train, network_size)\n",
    "            prediction = get_prediction(np.array(x_test), network, activation)\n",
    "            error.append(cross_entropy(prediction, np.array(y_test)))\n",
    "            result.append(classification_result(prediction, np.array(y_test)))\n",
    "            print(f'\\nfold {index+1}: error: {error[index]} classification score: {result[index]}\\n')\n",
    "\n",
    "    if z_norm:\n",
    "        average_error = sum(error)/len(error)\n",
    "        print(f'average error: {average_error}')\n",
    "    else:\n",
    "        #determine the average of the performance metric over all folds\n",
    "        average_class = sum(result)/len(result)\n",
    "        average_error = sum(error)/len(error)\n",
    "        print(f'average error: {average_error} average classification score {average_class}')\n",
    "    return average_error"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_data = load_breast_cancer('datasets/breast-cancer-wisconsin.data')\n",
    "cancer_data.name = 'Breast Cancer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "     clump thickness - 1  clump thickness - 2  clump thickness - 3  \\\n0                      0                    0                    0   \n1                      0                    0                    0   \n2                      0                    0                    1   \n3                      0                    0                    0   \n4                      0                    0                    0   \n..                   ...                  ...                  ...   \n457                    0                    0                    1   \n458                    0                    0                    1   \n459                    0                    0                    0   \n460                    0                    0                    0   \n461                    0                    0                    0   \n\n     clump thickness - 4  clump thickness - 5  clump thickness - 6  \\\n0                      0                    1                    0   \n1                      0                    1                    0   \n2                      0                    0                    0   \n3                      0                    0                    1   \n4                      1                    0                    0   \n..                   ...                  ...                  ...   \n457                    0                    0                    0   \n458                    0                    0                    0   \n459                    0                    1                    0   \n460                    1                    0                    0   \n461                    1                    0                    0   \n\n     clump thickness - 7  clump thickness - 8  clump thickness - 9  \\\n0                      0                    0                    0   \n1                      0                    0                    0   \n2                      0                    0                    0   \n3                      0                    0                    0   \n4                      0                    0                    0   \n..                   ...                  ...                  ...   \n457                    0                    0                    0   \n458                    0                    0                    0   \n459                    0                    0                    0   \n460                    0                    0                    0   \n461                    0                    0                    0   \n\n     clump thickness - 10  ...  mitoses - 2  mitoses - 3  mitoses - 4  \\\n0                       0  ...            0            0            0   \n1                       0  ...            0            0            0   \n2                       0  ...            0            0            0   \n3                       0  ...            0            0            0   \n4                       0  ...            0            0            0   \n..                    ...  ...          ...          ...          ...   \n457                     0  ...            1            0            0   \n458                     0  ...            0            0            0   \n459                     0  ...            1            0            0   \n460                     0  ...            0            0            0   \n461                     0  ...            0            0            0   \n\n     mitoses - 5  mitoses - 6  mitoses - 7  mitoses - 8  mitoses - 10  \\\n0              0            0            0            0             0   \n1              0            0            0            0             0   \n2              0            0            0            0             0   \n3              0            0            0            0             0   \n4              0            0            0            0             0   \n..           ...          ...          ...          ...           ...   \n457            0            0            0            0             0   \n458            0            0            0            0             0   \n459            0            0            0            0             0   \n460            0            0            0            0             0   \n461            0            0            0            0             0   \n\n     class - 2  class - 4  \n0            1          0  \n1            1          0  \n2            1          0  \n3            1          0  \n4            1          0  \n..         ...        ...  \n457          1          0  \n458          1          0  \n459          0          1  \n460          0          1  \n461          0          1  \n\n[462 rows x 91 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>clump thickness - 1</th>\n      <th>clump thickness - 2</th>\n      <th>clump thickness - 3</th>\n      <th>clump thickness - 4</th>\n      <th>clump thickness - 5</th>\n      <th>clump thickness - 6</th>\n      <th>clump thickness - 7</th>\n      <th>clump thickness - 8</th>\n      <th>clump thickness - 9</th>\n      <th>clump thickness - 10</th>\n      <th>...</th>\n      <th>mitoses - 2</th>\n      <th>mitoses - 3</th>\n      <th>mitoses - 4</th>\n      <th>mitoses - 5</th>\n      <th>mitoses - 6</th>\n      <th>mitoses - 7</th>\n      <th>mitoses - 8</th>\n      <th>mitoses - 10</th>\n      <th>class - 2</th>\n      <th>class - 4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>457</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>458</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>459</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>460</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>461</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>462 rows  91 columns</p>\n</div>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoencoder training epoch 1: decode layer difference: 2070.0\n",
      "autoencoder training epoch 2: decode layer difference: 1880.0\n",
      "autoencoder training epoch 3: decode layer difference: 1494.0\n",
      "autoencoder training epoch 4: decode layer difference: 1217.0\n",
      "autoencoder training epoch 5: decode layer difference: 1039.0\n",
      "autoencoder training epoch 6: decode layer difference: 802.0\n",
      "autoencoder training epoch 7: decode layer difference: 660.0\n",
      "autoencoder training epoch 8: decode layer difference: 546.0\n",
      "autoencoder training epoch 9: decode layer difference: 453.0\n",
      "autoencoder training epoch 10: decode layer difference: 338.0\n",
      "autoencoder training epoch 11: decode layer difference: 289.0\n",
      "autoencoder training epoch 12: decode layer difference: 245.0\n",
      "autoencoder training epoch 13: decode layer difference: 182.0\n",
      "autoencoder training epoch 14: decode layer difference: 166.0\n",
      "autoencoder training epoch 15: decode layer difference: 142.0\n",
      "autoencoder training epoch 16: decode layer difference: 122.0\n",
      "autoencoder training epoch 17: decode layer difference: 94.0\n",
      "autoencoder training epoch 18: decode layer difference: 74.0\n",
      "autoencoder training epoch 19: decode layer difference: 64.0\n",
      "autoencoder training epoch 20: decode layer difference: 60.0\n",
      "autoencoder training epoch 21: decode layer difference: 53.0\n",
      "autoencoder training epoch 22: decode layer difference: 37.0\n",
      "autoencoder training epoch 23: decode layer difference: 33.0\n",
      "autoencoder training epoch 24: decode layer difference: 24.0\n",
      "autoencoder training epoch 25: decode layer difference: 23.0\n",
      "autoencoder training epoch 26: decode layer difference: 16.0\n",
      "autoencoder training epoch 27: decode layer difference: 13.0\n",
      "autoencoder training epoch 28: decode layer difference: 11.0\n",
      "autoencoder training epoch 29: decode layer difference: 18.0\n",
      "autoencoder training epoch 30: decode layer difference: 7.0\n",
      "autoencoder training epoch 31: decode layer difference: 10.0\n",
      "autoencoder training epoch 32: decode layer difference: 11.0\n",
      "autoencoder training epoch 33: decode layer difference: 3.0\n",
      "autoencoder training epoch 34: decode layer difference: 3.0\n",
      "autoencoder training epoch 35: decode layer difference: 3.0\n",
      "autoencoder training epoch 36: decode layer difference: 6.0\n",
      "autoencoder training epoch 37: decode layer difference: 4.0\n",
      "autoencoder training epoch 38: decode layer difference: 3.0\n",
      "autoencoder training epoch 39: decode layer difference: 3.0\n",
      "autoencoder training epoch 40: decode layer difference: 1.0\n",
      "autoencoder training epoch 41: decode layer difference: 1.0\n",
      "autoencoder training epoch 42: decode layer difference: 2.0\n",
      "autoencoder training epoch 43: decode layer difference: 1.0\n",
      "autoencoder training epoch 44: decode layer difference: 1.0\n",
      "autoencoder training epoch 45: decode layer difference: 0.0\n",
      "training epoch 1: error: 102.76092655149216 classification score: 0.9084745762711864\n",
      "training epoch 2: error: 43.84201904980797 classification score: 0.9491525423728814\n",
      "training epoch 3: error: 31.34399442482168 classification score: 0.976271186440678\n",
      "training epoch 4: error: 20.665199120917844 classification score: 0.9898305084745763\n",
      "training epoch 5: error: 44.51252740153001 classification score: 0.9559322033898305\n",
      "training epoch 6: error: 11.722218791506963 classification score: 0.9932203389830508\n",
      "training epoch 7: error: 5.632171829724887 classification score: 0.9966101694915255\n",
      "training epoch 8: error: 2.4992102936558815 classification score: 1.0\n",
      "\n",
      "fold 1: error: 35.51522787991269 classification score: 0.9324324324324325\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 2174.0\n",
      "autoencoder training epoch 2: decode layer difference: 1826.0\n",
      "autoencoder training epoch 3: decode layer difference: 1400.0\n",
      "autoencoder training epoch 4: decode layer difference: 1189.0\n",
      "autoencoder training epoch 5: decode layer difference: 931.0\n",
      "autoencoder training epoch 6: decode layer difference: 789.0\n",
      "autoencoder training epoch 7: decode layer difference: 639.0\n",
      "autoencoder training epoch 8: decode layer difference: 528.0\n",
      "autoencoder training epoch 9: decode layer difference: 435.0\n",
      "autoencoder training epoch 10: decode layer difference: 335.0\n",
      "autoencoder training epoch 11: decode layer difference: 273.0\n",
      "autoencoder training epoch 12: decode layer difference: 222.0\n",
      "autoencoder training epoch 13: decode layer difference: 181.0\n",
      "autoencoder training epoch 14: decode layer difference: 144.0\n",
      "autoencoder training epoch 15: decode layer difference: 114.0\n",
      "autoencoder training epoch 16: decode layer difference: 100.0\n",
      "autoencoder training epoch 17: decode layer difference: 66.0\n",
      "autoencoder training epoch 18: decode layer difference: 67.0\n",
      "autoencoder training epoch 19: decode layer difference: 58.0\n",
      "autoencoder training epoch 20: decode layer difference: 27.0\n",
      "autoencoder training epoch 21: decode layer difference: 26.0\n",
      "autoencoder training epoch 22: decode layer difference: 17.0\n",
      "autoencoder training epoch 23: decode layer difference: 17.0\n",
      "autoencoder training epoch 24: decode layer difference: 11.0\n",
      "autoencoder training epoch 25: decode layer difference: 10.0\n",
      "autoencoder training epoch 26: decode layer difference: 7.0\n",
      "autoencoder training epoch 27: decode layer difference: 1.0\n",
      "autoencoder training epoch 28: decode layer difference: 3.0\n",
      "autoencoder training epoch 29: decode layer difference: 6.0\n",
      "autoencoder training epoch 30: decode layer difference: 2.0\n",
      "autoencoder training epoch 31: decode layer difference: 5.0\n",
      "autoencoder training epoch 32: decode layer difference: 0.0\n",
      "training epoch 1: error: 118.97092267083978 classification score: 0.8949152542372881\n",
      "training epoch 2: error: 52.76342236944438 classification score: 0.9491525423728814\n",
      "training epoch 3: error: 73.32572580941017 classification score: 0.9288135593220339\n",
      "training epoch 4: error: 28.503206761693797 classification score: 0.9728813559322034\n",
      "training epoch 5: error: 23.51297314140928 classification score: 0.9830508474576272\n",
      "training epoch 6: error: 16.385167572644825 classification score: 0.9932203389830508\n",
      "training epoch 7: error: 10.78434751169098 classification score: 0.9932203389830508\n",
      "training epoch 8: error: 9.037960442695063 classification score: 1.0\n",
      "\n",
      "fold 2: error: 9.369191428187197 classification score: 0.9594594594594594\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 2088.0\n",
      "autoencoder training epoch 2: decode layer difference: 1833.0\n",
      "autoencoder training epoch 3: decode layer difference: 1513.0\n",
      "autoencoder training epoch 4: decode layer difference: 1238.0\n",
      "autoencoder training epoch 5: decode layer difference: 1019.0\n",
      "autoencoder training epoch 6: decode layer difference: 793.0\n",
      "autoencoder training epoch 7: decode layer difference: 652.0\n",
      "autoencoder training epoch 8: decode layer difference: 512.0\n",
      "autoencoder training epoch 9: decode layer difference: 393.0\n",
      "autoencoder training epoch 10: decode layer difference: 323.0\n",
      "autoencoder training epoch 11: decode layer difference: 263.0\n",
      "autoencoder training epoch 12: decode layer difference: 204.0\n",
      "autoencoder training epoch 13: decode layer difference: 174.0\n",
      "autoencoder training epoch 14: decode layer difference: 129.0\n",
      "autoencoder training epoch 15: decode layer difference: 102.0\n",
      "autoencoder training epoch 16: decode layer difference: 95.0\n",
      "autoencoder training epoch 17: decode layer difference: 71.0\n",
      "autoencoder training epoch 18: decode layer difference: 69.0\n",
      "autoencoder training epoch 19: decode layer difference: 48.0\n",
      "autoencoder training epoch 20: decode layer difference: 40.0\n",
      "autoencoder training epoch 21: decode layer difference: 27.0\n",
      "autoencoder training epoch 22: decode layer difference: 21.0\n",
      "autoencoder training epoch 23: decode layer difference: 10.0\n",
      "autoencoder training epoch 24: decode layer difference: 15.0\n",
      "autoencoder training epoch 25: decode layer difference: 11.0\n",
      "autoencoder training epoch 26: decode layer difference: 8.0\n",
      "autoencoder training epoch 27: decode layer difference: 7.0\n",
      "autoencoder training epoch 28: decode layer difference: 3.0\n",
      "autoencoder training epoch 29: decode layer difference: 8.0\n",
      "autoencoder training epoch 30: decode layer difference: 6.0\n",
      "autoencoder training epoch 31: decode layer difference: 3.0\n",
      "autoencoder training epoch 32: decode layer difference: 6.0\n",
      "autoencoder training epoch 33: decode layer difference: 4.0\n",
      "autoencoder training epoch 34: decode layer difference: 2.0\n",
      "autoencoder training epoch 35: decode layer difference: 1.0\n",
      "autoencoder training epoch 36: decode layer difference: 1.0\n",
      "autoencoder training epoch 37: decode layer difference: 0.0\n",
      "training epoch 1: error: 111.31116210087947 classification score: 0.9525423728813559\n",
      "training epoch 2: error: 129.59343644358137 classification score: 0.8915254237288136\n",
      "training epoch 3: error: 44.70147215452933 classification score: 0.9661016949152542\n",
      "training epoch 4: error: 26.133022088161614 classification score: 0.976271186440678\n",
      "training epoch 5: error: 20.402281671475322 classification score: 0.9830508474576272\n",
      "training epoch 6: error: 35.95411974171457 classification score: 0.9694915254237289\n",
      "training epoch 7: error: 10.305192331177487 classification score: 0.9932203389830508\n",
      "training epoch 8: error: 6.360771333420869 classification score: 0.9966101694915255\n",
      "training epoch 9: error: 4.864305045458725 classification score: 1.0\n",
      "\n",
      "fold 3: error: 49.54208063589992 classification score: 0.8783783783783784\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 2171.0\n",
      "autoencoder training epoch 2: decode layer difference: 2031.0\n",
      "autoencoder training epoch 3: decode layer difference: 1481.0\n",
      "autoencoder training epoch 4: decode layer difference: 1182.0\n",
      "autoencoder training epoch 5: decode layer difference: 981.0\n",
      "autoencoder training epoch 6: decode layer difference: 830.0\n",
      "autoencoder training epoch 7: decode layer difference: 638.0\n",
      "autoencoder training epoch 8: decode layer difference: 505.0\n",
      "autoencoder training epoch 9: decode layer difference: 404.0\n",
      "autoencoder training epoch 10: decode layer difference: 332.0\n",
      "autoencoder training epoch 11: decode layer difference: 279.0\n",
      "autoencoder training epoch 12: decode layer difference: 234.0\n",
      "autoencoder training epoch 13: decode layer difference: 191.0\n",
      "autoencoder training epoch 14: decode layer difference: 168.0\n",
      "autoencoder training epoch 15: decode layer difference: 139.0\n",
      "autoencoder training epoch 16: decode layer difference: 100.0\n",
      "autoencoder training epoch 17: decode layer difference: 75.0\n",
      "autoencoder training epoch 18: decode layer difference: 70.0\n",
      "autoencoder training epoch 19: decode layer difference: 54.0\n",
      "autoencoder training epoch 20: decode layer difference: 47.0\n",
      "autoencoder training epoch 21: decode layer difference: 46.0\n",
      "autoencoder training epoch 22: decode layer difference: 27.0\n",
      "autoencoder training epoch 23: decode layer difference: 29.0\n",
      "autoencoder training epoch 24: decode layer difference: 22.0\n",
      "autoencoder training epoch 25: decode layer difference: 8.0\n",
      "autoencoder training epoch 26: decode layer difference: 15.0\n",
      "autoencoder training epoch 27: decode layer difference: 10.0\n",
      "autoencoder training epoch 28: decode layer difference: 6.0\n",
      "autoencoder training epoch 29: decode layer difference: 6.0\n",
      "autoencoder training epoch 30: decode layer difference: 13.0\n",
      "autoencoder training epoch 31: decode layer difference: 5.0\n",
      "autoencoder training epoch 32: decode layer difference: 6.0\n",
      "autoencoder training epoch 33: decode layer difference: 5.0\n",
      "autoencoder training epoch 34: decode layer difference: 5.0\n",
      "autoencoder training epoch 35: decode layer difference: 6.0\n",
      "autoencoder training epoch 36: decode layer difference: 2.0\n",
      "autoencoder training epoch 37: decode layer difference: 3.0\n",
      "autoencoder training epoch 38: decode layer difference: 1.0\n",
      "autoencoder training epoch 39: decode layer difference: 1.0\n",
      "autoencoder training epoch 40: decode layer difference: 1.0\n",
      "autoencoder training epoch 41: decode layer difference: 2.0\n",
      "autoencoder training epoch 42: decode layer difference: 1.0\n",
      "autoencoder training epoch 43: decode layer difference: 1.0\n",
      "autoencoder training epoch 44: decode layer difference: 0.0\n",
      "training epoch 1: error: 128.11705953059106 classification score: 0.9186440677966101\n",
      "training epoch 2: error: 105.11631826552868 classification score: 0.9254237288135593\n",
      "training epoch 3: error: 82.59077991487626 classification score: 0.9152542372881356\n",
      "training epoch 4: error: 27.82468869157716 classification score: 0.9728813559322034\n",
      "training epoch 5: error: 18.626407505379504 classification score: 0.9830508474576272\n",
      "training epoch 6: error: 11.641173996119516 classification score: 0.9966101694915255\n",
      "training epoch 7: error: 80.97471138550446 classification score: 0.9050847457627119\n",
      "training epoch 8: error: 4.334062841917658 classification score: 1.0\n",
      "\n",
      "fold 4: error: 3.7223673392214476 classification score: 0.9864864864864865\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 2068.0\n",
      "autoencoder training epoch 2: decode layer difference: 1838.0\n",
      "autoencoder training epoch 3: decode layer difference: 1572.0\n",
      "autoencoder training epoch 4: decode layer difference: 1229.0\n",
      "autoencoder training epoch 5: decode layer difference: 1025.0\n",
      "autoencoder training epoch 6: decode layer difference: 807.0\n",
      "autoencoder training epoch 7: decode layer difference: 637.0\n",
      "autoencoder training epoch 8: decode layer difference: 529.0\n",
      "autoencoder training epoch 9: decode layer difference: 458.0\n",
      "autoencoder training epoch 10: decode layer difference: 341.0\n",
      "autoencoder training epoch 11: decode layer difference: 269.0\n",
      "autoencoder training epoch 12: decode layer difference: 247.0\n",
      "autoencoder training epoch 13: decode layer difference: 193.0\n",
      "autoencoder training epoch 14: decode layer difference: 144.0\n",
      "autoencoder training epoch 15: decode layer difference: 133.0\n",
      "autoencoder training epoch 16: decode layer difference: 90.0\n",
      "autoencoder training epoch 17: decode layer difference: 83.0\n",
      "autoencoder training epoch 18: decode layer difference: 55.0\n",
      "autoencoder training epoch 19: decode layer difference: 46.0\n",
      "autoencoder training epoch 20: decode layer difference: 36.0\n",
      "autoencoder training epoch 21: decode layer difference: 32.0\n",
      "autoencoder training epoch 22: decode layer difference: 27.0\n",
      "autoencoder training epoch 23: decode layer difference: 18.0\n",
      "autoencoder training epoch 24: decode layer difference: 14.0\n",
      "autoencoder training epoch 25: decode layer difference: 14.0\n",
      "autoencoder training epoch 26: decode layer difference: 13.0\n",
      "autoencoder training epoch 27: decode layer difference: 3.0\n",
      "autoencoder training epoch 28: decode layer difference: 4.0\n",
      "autoencoder training epoch 29: decode layer difference: 3.0\n",
      "autoencoder training epoch 30: decode layer difference: 4.0\n",
      "autoencoder training epoch 31: decode layer difference: 1.0\n",
      "autoencoder training epoch 32: decode layer difference: 2.0\n",
      "autoencoder training epoch 33: decode layer difference: 1.0\n",
      "autoencoder training epoch 34: decode layer difference: 2.0\n",
      "autoencoder training epoch 35: decode layer difference: 3.0\n",
      "autoencoder training epoch 36: decode layer difference: 3.0\n",
      "autoencoder training epoch 37: decode layer difference: 2.0\n",
      "autoencoder training epoch 38: decode layer difference: 1.0\n",
      "autoencoder training epoch 39: decode layer difference: 0.0\n",
      "training epoch 1: error: 117.00163295702636 classification score: 0.9087837837837838\n",
      "training epoch 2: error: 44.63273086305068 classification score: 0.9628378378378378\n",
      "training epoch 3: error: 31.138459865408144 classification score: 0.9797297297297297\n",
      "training epoch 4: error: 27.730147467737915 classification score: 0.9763513513513513\n",
      "training epoch 5: error: 23.174227689592286 classification score: 0.9831081081081081\n",
      "training epoch 6: error: 17.174389714991683 classification score: 0.9898648648648649\n",
      "training epoch 7: error: 7.759475581951787 classification score: 0.9966216216216216\n",
      "training epoch 8: error: 4.325518187372098 classification score: 1.0\n",
      "\n",
      "fold 5: error: 23.06463550550255 classification score: 0.9315068493150684\n",
      "\n",
      "average error: 24.24270055774476 average classification score 0.9376527212143649\n",
      "CPU times: total: 17.3 s\n",
      "Wall time: 20.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "24.24270055774476"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_layered(cancer_data, 'class', train_autoencoder_network, (25,90))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1: error: 301.5794858460312 classification score: 0.5152542372881356\n",
      "training epoch 2: error: 275.13083313374796 classification score: 0.5152542372881356\n",
      "training epoch 3: error: 237.40695731381157 classification score: 0.5152542372881356\n",
      "training epoch 4: error: 213.70636734852985 classification score: 0.5152542372881356\n",
      "training epoch 5: error: 70.87884175133611 classification score: 0.9457627118644067\n",
      "training epoch 6: error: 40.47113967866148 classification score: 0.9694915254237289\n",
      "training epoch 7: error: 25.477038180982557 classification score: 0.9796610169491525\n",
      "training epoch 8: error: 20.448001065544627 classification score: 0.9898305084745763\n",
      "training epoch 9: error: 15.5417668993827 classification score: 0.9932203389830508\n",
      "training epoch 10: error: 12.397950319179099 classification score: 0.9932203389830508\n",
      "training epoch 11: error: 7.694284604463291 classification score: 1.0\n",
      "\n",
      "fold 1: error: 24.91423909746244 classification score: 0.9459459459459459\n",
      "\n",
      "training epoch 1: error: 340.92281944854096 classification score: 0.4847457627118644\n",
      "training epoch 2: error: 278.3489660934368 classification score: 0.5152542372881356\n",
      "training epoch 3: error: 242.44910463673722 classification score: 0.5152542372881356\n",
      "training epoch 4: error: 217.08214375084034 classification score: 0.5152542372881356\n",
      "training epoch 5: error: 201.34820729236327 classification score: 0.5152542372881356\n",
      "training epoch 6: error: 115.05522020691124 classification score: 0.911864406779661\n",
      "training epoch 7: error: 57.02282033301461 classification score: 0.9728813559322034\n",
      "training epoch 8: error: 35.64828077513933 classification score: 0.9796610169491525\n",
      "training epoch 9: error: 26.524224004160757 classification score: 0.9864406779661017\n",
      "training epoch 10: error: 22.412308645162476 classification score: 0.9898305084745763\n",
      "training epoch 11: error: 26.022382906804722 classification score: 0.9864406779661017\n",
      "training epoch 12: error: 16.71782803667584 classification score: 0.9966101694915255\n",
      "training epoch 13: error: 15.997047775943285 classification score: 0.9932203389830508\n",
      "training epoch 14: error: 14.442319349567438 classification score: 0.9966101694915255\n",
      "training epoch 15: error: 13.598008561521675 classification score: 0.9966101694915255\n",
      "training epoch 16: error: 14.046338782036848 classification score: 0.9966101694915255\n",
      "training epoch 17: error: 12.998726656306989 classification score: 0.9966101694915255\n",
      "training epoch 18: error: 13.243356260057388 classification score: 0.9966101694915255\n",
      "training epoch 19: error: 12.775168606514264 classification score: 0.9966101694915255\n",
      "training epoch 20: error: 12.164972920189802 classification score: 0.9966101694915255\n",
      "\n",
      "fold 2: error: 15.919261434055889 classification score: 0.9594594594594594\n",
      "\n",
      "training epoch 1: error: 291.32610110407876 classification score: 0.5152542372881356\n",
      "training epoch 2: error: 270.7966920303781 classification score: 0.5152542372881356\n",
      "training epoch 3: error: 240.07010219411146 classification score: 0.5152542372881356\n",
      "training epoch 4: error: 215.99614343498183 classification score: 0.5152542372881356\n",
      "training epoch 5: error: 199.60320204043174 classification score: 0.5152542372881356\n",
      "training epoch 6: error: 81.35507158685957 classification score: 0.9389830508474576\n",
      "training epoch 7: error: 50.71526759722627 classification score: 0.9457627118644067\n",
      "training epoch 8: error: 31.369649877596427 classification score: 0.976271186440678\n",
      "training epoch 9: error: 23.23547696108374 classification score: 0.9898305084745763\n",
      "training epoch 10: error: 19.146378280483372 classification score: 0.9898305084745763\n",
      "training epoch 11: error: 16.616307747744397 classification score: 0.9966101694915255\n",
      "training epoch 12: error: 14.464920677019627 classification score: 0.9966101694915255\n",
      "training epoch 13: error: 13.921528132979068 classification score: 0.9966101694915255\n",
      "training epoch 14: error: 14.89550757772218 classification score: 0.9966101694915255\n",
      "training epoch 15: error: 12.452633675497005 classification score: 0.9966101694915255\n",
      "training epoch 16: error: 12.41804231815223 classification score: 0.9966101694915255\n",
      "training epoch 17: error: 11.738067983015664 classification score: 0.9966101694915255\n",
      "training epoch 18: error: 11.90336011775985 classification score: 0.9966101694915255\n",
      "training epoch 19: error: 12.006286426335693 classification score: 0.9966101694915255\n",
      "training epoch 20: error: 10.88847565244519 classification score: 0.9966101694915255\n",
      "\n",
      "fold 3: error: 22.43067234096805 classification score: 0.9459459459459459\n",
      "\n",
      "training epoch 1: error: 290.2168953758071 classification score: 0.5152542372881356\n",
      "training epoch 2: error: 270.79665633491214 classification score: 0.5152542372881356\n",
      "training epoch 3: error: 239.4871392553598 classification score: 0.5152542372881356\n",
      "training epoch 4: error: 217.9790240353908 classification score: 0.5152542372881356\n",
      "training epoch 5: error: 200.7320078446456 classification score: 0.5152542372881356\n",
      "training epoch 6: error: 70.58446464943275 classification score: 0.9491525423728814\n",
      "training epoch 7: error: 41.69550682804642 classification score: 0.9661016949152542\n",
      "training epoch 8: error: 45.245637943747084 classification score: 0.9694915254237289\n",
      "training epoch 9: error: 30.677233268916485 classification score: 0.976271186440678\n",
      "training epoch 10: error: 23.887257678241244 classification score: 0.9864406779661017\n",
      "training epoch 11: error: 30.660340814689775 classification score: 0.9661016949152542\n",
      "training epoch 12: error: 17.878378859885377 classification score: 0.9966101694915255\n",
      "training epoch 13: error: 18.421899057046673 classification score: 0.9898305084745763\n",
      "training epoch 14: error: 15.077526950199195 classification score: 0.9966101694915255\n",
      "training epoch 15: error: 14.970419164168884 classification score: 0.9966101694915255\n",
      "training epoch 16: error: 14.107490185766427 classification score: 0.9966101694915255\n",
      "training epoch 17: error: 13.592639069917073 classification score: 0.9966101694915255\n",
      "training epoch 18: error: 13.387966433497613 classification score: 0.9966101694915255\n",
      "training epoch 19: error: 13.016669370869629 classification score: 0.9966101694915255\n",
      "training epoch 20: error: 12.564634730622414 classification score: 0.9966101694915255\n",
      "\n",
      "fold 4: error: 6.774323725056908 classification score: 0.9594594594594594\n",
      "\n",
      "training epoch 1: error: 293.3100640140419 classification score: 0.5135135135135135\n",
      "training epoch 2: error: 270.4215731186312 classification score: 0.5135135135135135\n",
      "training epoch 3: error: 241.3978245553838 classification score: 0.5135135135135135\n",
      "training epoch 4: error: 213.0400670437968 classification score: 0.5135135135135135\n",
      "training epoch 5: error: 74.77780650182383 classification score: 0.918918918918919\n",
      "training epoch 6: error: 40.200489329021536 classification score: 0.972972972972973\n",
      "training epoch 7: error: 29.324141956655968 classification score: 0.9864864864864865\n",
      "training epoch 8: error: 23.810589056873106 classification score: 0.9864864864864865\n",
      "training epoch 9: error: 28.77374148820743 classification score: 0.9797297297297297\n",
      "training epoch 10: error: 20.77212374950511 classification score: 0.9932432432432432\n",
      "training epoch 11: error: 16.0656252755666 classification score: 0.9966216216216216\n",
      "training epoch 12: error: 15.046582902667488 classification score: 0.9966216216216216\n",
      "training epoch 13: error: 14.150646293856877 classification score: 0.9966216216216216\n",
      "training epoch 14: error: 14.260860864090326 classification score: 0.9966216216216216\n",
      "training epoch 15: error: 13.746215415083434 classification score: 0.9966216216216216\n",
      "training epoch 16: error: 13.12204483308566 classification score: 0.9966216216216216\n",
      "training epoch 17: error: 12.929837506297199 classification score: 0.9966216216216216\n",
      "training epoch 18: error: 12.786031020014557 classification score: 0.9966216216216216\n",
      "training epoch 19: error: 12.653002306843426 classification score: 0.9966216216216216\n",
      "training epoch 20: error: 12.453134180064678 classification score: 0.9966216216216216\n",
      "\n",
      "fold 5: error: 15.609528925193162 classification score: 0.9178082191780822\n",
      "\n",
      "average error: 17.129605104547288 average classification score 0.9457238059977785\n",
      "CPU times: total: 6.8 s\n",
      "Wall time: 8.78 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "17.129605104547288"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_layered(cancer_data, 'class', train_layered_classification_network, (25,90))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1: error: 38.37712054509941 classification score: 0.976271186440678\n",
      "training epoch 2: error: 34.259439460880024 classification score: 0.9796610169491525\n",
      "training epoch 3: error: 22.539628588861472 classification score: 0.9864406779661017\n",
      "training epoch 4: error: 20.71132154642081 classification score: 0.9932203389830508\n",
      "training epoch 5: error: 17.433355031378074 classification score: 0.9932203389830508\n",
      "training epoch 6: error: 16.65664792919012 classification score: 0.9932203389830508\n",
      "training epoch 7: error: 15.321319056558318 classification score: 0.9932203389830508\n",
      "training epoch 8: error: 13.298291909277868 classification score: 0.9966101694915255\n",
      "training epoch 9: error: 12.207396241441199 classification score: 0.9932203389830508\n",
      "training epoch 10: error: 11.282688801500086 classification score: 0.9932203389830508\n",
      "training epoch 11: error: 10.320242925792982 classification score: 0.9966101694915255\n",
      "training epoch 12: error: 9.592125784449388 classification score: 0.9932203389830508\n",
      "training epoch 13: error: 9.599203606150658 classification score: 1.0\n",
      "\n",
      "fold 1: error: 22.733685163664113 classification score: 0.918918918918919\n",
      "\n",
      "training epoch 1: error: 25.27707066280715 classification score: 0.9796610169491525\n",
      "training epoch 2: error: 23.254867227281455 classification score: 0.9830508474576272\n",
      "training epoch 3: error: 16.421304494906984 classification score: 0.9864406779661017\n",
      "training epoch 4: error: 10.909923011332946 classification score: 0.9966101694915255\n",
      "training epoch 5: error: 9.227324151005766 classification score: 1.0\n",
      "\n",
      "fold 2: error: 55.02071660189885 classification score: 0.8783783783783784\n",
      "\n",
      "training epoch 1: error: 54.5888852173584 classification score: 0.9491525423728814\n",
      "training epoch 2: error: 37.460500459121874 classification score: 0.9830508474576272\n",
      "training epoch 3: error: 30.013912079059487 classification score: 0.9796610169491525\n",
      "training epoch 4: error: 26.741608777648555 classification score: 0.9830508474576272\n",
      "training epoch 5: error: 23.74384925620901 classification score: 0.9864406779661017\n",
      "training epoch 6: error: 21.91823519593625 classification score: 0.9864406779661017\n",
      "training epoch 7: error: 20.963549271730518 classification score: 0.9898305084745763\n",
      "training epoch 8: error: 17.824818635819724 classification score: 0.9864406779661017\n",
      "training epoch 9: error: 17.54426441778609 classification score: 0.9898305084745763\n",
      "training epoch 10: error: 24.12193259375847 classification score: 0.9796610169491525\n",
      "training epoch 11: error: 14.911334612025698 classification score: 0.9898305084745763\n",
      "training epoch 12: error: 14.099407769376242 classification score: 0.9898305084745763\n",
      "training epoch 13: error: 16.29865110263653 classification score: 0.9898305084745763\n",
      "training epoch 14: error: 13.662566396002681 classification score: 0.9966101694915255\n",
      "training epoch 15: error: 12.53002854021122 classification score: 0.9932203389830508\n",
      "training epoch 16: error: 11.21869375507648 classification score: 0.9966101694915255\n",
      "training epoch 17: error: 11.149584379308473 classification score: 0.9932203389830508\n",
      "training epoch 18: error: 11.29853468292994 classification score: 0.9966101694915255\n",
      "training epoch 19: error: 11.772911725092024 classification score: 0.9966101694915255\n",
      "training epoch 20: error: 9.492681695917017 classification score: 0.9966101694915255\n",
      "\n",
      "fold 3: error: 2.8541865099976635 classification score: 0.9864864864864865\n",
      "\n",
      "training epoch 1: error: 40.992434185715744 classification score: 0.9661016949152542\n",
      "training epoch 2: error: 31.05946509036063 classification score: 0.9796610169491525\n",
      "training epoch 3: error: 26.04288280172854 classification score: 0.9830508474576272\n",
      "training epoch 4: error: 20.99300879350375 classification score: 0.9898305084745763\n",
      "training epoch 5: error: 22.323379663387016 classification score: 0.9864406779661017\n",
      "training epoch 6: error: 18.0330816049714 classification score: 0.9898305084745763\n",
      "training epoch 7: error: 14.9549604553673 classification score: 0.9898305084745763\n",
      "training epoch 8: error: 15.751024588175055 classification score: 0.9932203389830508\n",
      "training epoch 9: error: 12.931733804407207 classification score: 0.9932203389830508\n",
      "training epoch 10: error: 11.883154234259742 classification score: 0.9932203389830508\n",
      "training epoch 11: error: 10.241794999709416 classification score: 0.9932203389830508\n",
      "training epoch 12: error: 11.686559127970936 classification score: 0.9966101694915255\n",
      "training epoch 13: error: 9.566479654520078 classification score: 1.0\n",
      "\n",
      "fold 4: error: 16.4665433118191 classification score: 0.9459459459459459\n",
      "\n",
      "training epoch 1: error: 32.407262592040084 classification score: 0.9797297297297297\n",
      "training epoch 2: error: 23.45254297391198 classification score: 0.9864864864864865\n",
      "training epoch 3: error: 18.064536228612685 classification score: 0.9898648648648649\n",
      "training epoch 4: error: 17.023288674977966 classification score: 0.9932432432432432\n",
      "training epoch 5: error: 14.651020956997845 classification score: 0.9898648648648649\n",
      "training epoch 6: error: 12.112633089005431 classification score: 0.9932432432432432\n",
      "training epoch 7: error: 10.78830620129955 classification score: 0.9966216216216216\n",
      "training epoch 8: error: 9.404827478021849 classification score: 0.9966216216216216\n",
      "training epoch 9: error: 9.33948812286306 classification score: 1.0\n",
      "\n",
      "fold 5: error: 34.845830845536334 classification score: 0.9178082191780822\n",
      "\n",
      "average error: 26.38419248658321 average classification score 0.9295075897815623\n",
      "CPU times: total: 6.81 s\n",
      "Wall time: 8.59 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "26.38419248658321"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation(cancer_data, 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data = load_car('datasets/car.data')\n",
    "car_data.name = 'car'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "      buying - high  buying - low  buying - med  buying - vhigh  maint - high  \\\n0                 0             0             0               1             0   \n1                 0             0             0               1             0   \n2                 0             0             0               1             0   \n3                 0             0             0               1             0   \n4                 0             0             0               1             0   \n...             ...           ...           ...             ...           ...   \n1723              0             1             0               0             0   \n1724              0             1             0               0             0   \n1725              0             1             0               0             0   \n1726              0             1             0               0             0   \n1727              0             1             0               0             0   \n\n      maint - low  maint - med  maint - vhigh  doors - 2  doors - 3  ...  \\\n0               0            0              1          1          0  ...   \n1               0            0              1          1          0  ...   \n2               0            0              1          1          0  ...   \n3               0            0              1          1          0  ...   \n4               0            0              1          1          0  ...   \n...           ...          ...            ...        ...        ...  ...   \n1723            1            0              0          0          0  ...   \n1724            1            0              0          0          0  ...   \n1725            1            0              0          0          0  ...   \n1726            1            0              0          0          0  ...   \n1727            1            0              0          0          0  ...   \n\n      lug_boot - big  lug_boot - med  lug_boot - small  safety - high  \\\n0                  0               0                 1              0   \n1                  0               0                 1              0   \n2                  0               0                 1              1   \n3                  0               1                 0              0   \n4                  0               1                 0              0   \n...              ...             ...               ...            ...   \n1723               0               1                 0              0   \n1724               0               1                 0              1   \n1725               1               0                 0              0   \n1726               1               0                 0              0   \n1727               1               0                 0              1   \n\n      safety - low  safety - med  class - acc  class - good  class - unacc  \\\n0                1             0            0             0              1   \n1                0             1            0             0              1   \n2                0             0            0             0              1   \n3                1             0            0             0              1   \n4                0             1            0             0              1   \n...            ...           ...          ...           ...            ...   \n1723             0             1            0             1              0   \n1724             0             0            0             0              0   \n1725             1             0            0             0              1   \n1726             0             1            0             1              0   \n1727             0             0            0             0              0   \n\n      class - vgood  \n0                 0  \n1                 0  \n2                 0  \n3                 0  \n4                 0  \n...             ...  \n1723              0  \n1724              1  \n1725              0  \n1726              0  \n1727              1  \n\n[1728 rows x 25 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>buying - high</th>\n      <th>buying - low</th>\n      <th>buying - med</th>\n      <th>buying - vhigh</th>\n      <th>maint - high</th>\n      <th>maint - low</th>\n      <th>maint - med</th>\n      <th>maint - vhigh</th>\n      <th>doors - 2</th>\n      <th>doors - 3</th>\n      <th>...</th>\n      <th>lug_boot - big</th>\n      <th>lug_boot - med</th>\n      <th>lug_boot - small</th>\n      <th>safety - high</th>\n      <th>safety - low</th>\n      <th>safety - med</th>\n      <th>class - acc</th>\n      <th>class - good</th>\n      <th>class - unacc</th>\n      <th>class - vgood</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1723</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1724</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1725</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1726</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1727</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>1728 rows  25 columns</p>\n</div>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoencoder training epoch 1: decode layer difference: 590.0\n",
      "autoencoder training epoch 2: decode layer difference: 45.0\n",
      "autoencoder training epoch 3: decode layer difference: 24.0\n",
      "autoencoder training epoch 4: decode layer difference: 7.0\n",
      "autoencoder training epoch 5: decode layer difference: 3.0\n",
      "autoencoder training epoch 6: decode layer difference: 1.0\n",
      "autoencoder training epoch 7: decode layer difference: 0.0\n",
      "training epoch 1: error: 830.4109671020148 classification score: 0.702262443438914\n",
      "training epoch 2: error: 490.4576957449076 classification score: 0.8687782805429864\n",
      "training epoch 3: error: 403.27329261184883 classification score: 0.8805429864253393\n",
      "training epoch 4: error: 293.2526822894209 classification score: 0.9122171945701357\n",
      "training epoch 5: error: 272.4040189895977 classification score: 0.92579185520362\n",
      "training epoch 6: error: 214.9996122985251 classification score: 0.9375565610859729\n",
      "training epoch 7: error: 162.97491334640884 classification score: 0.9484162895927601\n",
      "training epoch 8: error: 179.8189149047728 classification score: 0.9574660633484163\n",
      "training epoch 9: error: 203.76031642636428 classification score: 0.9393665158371041\n",
      "training epoch 10: error: 113.01547634417706 classification score: 0.9701357466063348\n",
      "training epoch 11: error: 115.55792481930516 classification score: 0.9746606334841629\n",
      "training epoch 12: error: 242.33616070242277 classification score: 0.9466063348416289\n",
      "training epoch 13: error: 80.9162713211704 classification score: 0.9891402714932127\n",
      "training epoch 14: error: 73.16983308942329 classification score: 0.9909502262443439\n",
      "training epoch 15: error: 42.90749331403719 classification score: 0.9963800904977376\n",
      "training epoch 16: error: 47.67187285707138 classification score: 0.995475113122172\n",
      "training epoch 17: error: 33.24979557133496 classification score: 0.9972850678733032\n",
      "training epoch 18: error: 19.828876739873298 classification score: 0.9990950226244344\n",
      "training epoch 19: error: 21.790807361164802 classification score: 0.9981900452488688\n",
      "training epoch 20: error: 20.245912502643165 classification score: 0.9981900452488688\n",
      "\n",
      "fold 1: error: 4.229137117663931 classification score: 1.0\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 432.0\n",
      "autoencoder training epoch 2: decode layer difference: 63.0\n",
      "autoencoder training epoch 3: decode layer difference: 38.0\n",
      "autoencoder training epoch 4: decode layer difference: 16.0\n",
      "autoencoder training epoch 5: decode layer difference: 21.0\n",
      "autoencoder training epoch 6: decode layer difference: 7.0\n",
      "autoencoder training epoch 7: decode layer difference: 0.0\n",
      "training epoch 1: error: 784.0685370072363 classification score: 0.7104072398190046\n",
      "training epoch 2: error: 467.49038801630593 classification score: 0.8805429864253393\n",
      "training epoch 3: error: 361.79516227382703 classification score: 0.8959276018099548\n",
      "training epoch 4: error: 328.0200239133871 classification score: 0.8950226244343892\n",
      "training epoch 5: error: 238.58546195318877 classification score: 0.9366515837104072\n",
      "training epoch 6: error: 174.5687282674168 classification score: 0.9574660633484163\n",
      "training epoch 7: error: 221.1836400181519 classification score: 0.9493212669683257\n",
      "training epoch 8: error: 140.04828914570354 classification score: 0.9592760180995475\n",
      "training epoch 9: error: 134.4095416900565 classification score: 0.9592760180995475\n",
      "training epoch 10: error: 189.81398665906508 classification score: 0.9583710407239819\n",
      "training epoch 11: error: 100.84174478201737 classification score: 0.9819004524886877\n",
      "training epoch 12: error: 50.66443363092457 classification score: 0.9945701357466064\n",
      "training epoch 13: error: 83.3396325655264 classification score: 0.9846153846153847\n",
      "training epoch 14: error: 33.39388576307732 classification score: 0.995475113122172\n",
      "training epoch 15: error: 19.28326893987881 classification score: 1.0\n",
      "\n",
      "fold 2: error: 17.641562543450654 classification score: 0.9783393501805054\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 441.0\n",
      "autoencoder training epoch 2: decode layer difference: 64.0\n",
      "autoencoder training epoch 3: decode layer difference: 21.0\n",
      "autoencoder training epoch 4: decode layer difference: 12.0\n",
      "autoencoder training epoch 5: decode layer difference: 7.0\n",
      "autoencoder training epoch 6: decode layer difference: 3.0\n",
      "autoencoder training epoch 7: decode layer difference: 1.0\n",
      "autoencoder training epoch 8: decode layer difference: 0.0\n",
      "training epoch 1: error: 835.4154534423453 classification score: 0.7070524412296564\n",
      "training epoch 2: error: 461.4839325884706 classification score: 0.8896925858951176\n",
      "training epoch 3: error: 343.142794583027 classification score: 0.9095840867992767\n",
      "training epoch 4: error: 260.45895027949354 classification score: 0.9267631103074141\n",
      "training epoch 5: error: 233.0227665848387 classification score: 0.9358047016274864\n",
      "training epoch 6: error: 218.68754277781122 classification score: 0.9439421338155516\n",
      "training epoch 7: error: 219.03516068704403 classification score: 0.945750452079566\n",
      "training epoch 8: error: 196.52374488252545 classification score: 0.945750452079566\n",
      "training epoch 9: error: 207.46341892559587 classification score: 0.9349005424954792\n",
      "training epoch 10: error: 168.85368346377948 classification score: 0.9566003616636528\n",
      "training epoch 11: error: 159.47069106219712 classification score: 0.9448462929475587\n",
      "training epoch 12: error: 198.17934354486485 classification score: 0.9520795660036167\n",
      "training epoch 13: error: 96.1594475377405 classification score: 0.9746835443037974\n",
      "training epoch 14: error: 73.52654627538257 classification score: 0.9810126582278481\n",
      "training epoch 15: error: 57.92640733206343 classification score: 0.9918625678119349\n",
      "training epoch 16: error: 44.93067507798085 classification score: 0.9918625678119349\n",
      "training epoch 17: error: 24.168982751965295 classification score: 0.9981916817359855\n",
      "training epoch 18: error: 24.367411240233398 classification score: 0.9972875226039783\n",
      "training epoch 19: error: 22.08659963363414 classification score: 0.9963833634719711\n",
      "training epoch 20: error: 15.185858512395567 classification score: 1.0\n",
      "\n",
      "fold 3: error: 18.884383383546343 classification score: 0.9818840579710145\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 506.0\n",
      "autoencoder training epoch 2: decode layer difference: 75.0\n",
      "autoencoder training epoch 3: decode layer difference: 30.0\n",
      "autoencoder training epoch 4: decode layer difference: 25.0\n",
      "autoencoder training epoch 5: decode layer difference: 5.0\n",
      "autoencoder training epoch 6: decode layer difference: 1.0\n",
      "autoencoder training epoch 7: decode layer difference: 3.0\n",
      "autoencoder training epoch 8: decode layer difference: 1.0\n",
      "autoencoder training epoch 9: decode layer difference: 1.0\n",
      "autoencoder training epoch 10: decode layer difference: 0.0\n",
      "training epoch 1: error: 858.0385424981692 classification score: 0.701627486437613\n",
      "training epoch 2: error: 517.0090234946224 classification score: 0.8589511754068716\n",
      "training epoch 3: error: 371.6191508790967 classification score: 0.895117540687161\n",
      "training epoch 4: error: 306.0694710325885 classification score: 0.918625678119349\n",
      "training epoch 5: error: 233.55853540559124 classification score: 0.9394213381555153\n",
      "training epoch 6: error: 200.04733322098383 classification score: 0.9349005424954792\n",
      "training epoch 7: error: 205.7619095187826 classification score: 0.9421338155515371\n",
      "training epoch 8: error: 188.4956400040655 classification score: 0.9412296564195298\n",
      "training epoch 9: error: 197.20754296113483 classification score: 0.9566003616636528\n",
      "training epoch 10: error: 135.21689210349263 classification score: 0.972875226039783\n",
      "training epoch 11: error: 139.0746474452995 classification score: 0.9656419529837251\n",
      "training epoch 12: error: 84.92292067303433 classification score: 0.9837251356238698\n",
      "training epoch 13: error: 103.93742085232527 classification score: 0.9810126582278481\n",
      "training epoch 14: error: 211.77038420404935 classification score: 0.945750452079566\n",
      "training epoch 15: error: 53.363784501324375 classification score: 0.9909584086799277\n",
      "training epoch 16: error: 29.235348826284756 classification score: 0.9972875226039783\n",
      "training epoch 17: error: 29.377420193463145 classification score: 0.9954792043399638\n",
      "training epoch 18: error: 14.65352787496685 classification score: 1.0\n",
      "\n",
      "fold 4: error: 16.01299098093547 classification score: 0.9855072463768116\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 496.0\n",
      "autoencoder training epoch 2: decode layer difference: 19.0\n",
      "autoencoder training epoch 3: decode layer difference: 6.0\n",
      "autoencoder training epoch 4: decode layer difference: 0.0\n",
      "training epoch 1: error: 858.0897292402507 classification score: 0.7061482820976492\n",
      "training epoch 2: error: 521.737730748443 classification score: 0.8670886075949367\n",
      "training epoch 3: error: 376.4643665623606 classification score: 0.8960216998191681\n",
      "training epoch 4: error: 348.1990203457774 classification score: 0.891500904159132\n",
      "training epoch 5: error: 311.8932130572125 classification score: 0.8978300180831826\n",
      "training epoch 6: error: 295.6275966154038 classification score: 0.8969258589511754\n",
      "training epoch 7: error: 189.19973183364777 classification score: 0.9502712477396021\n",
      "training epoch 8: error: 166.85745842009064 classification score: 0.9520795660036167\n",
      "training epoch 9: error: 164.60586932511134 classification score: 0.9520795660036167\n",
      "training epoch 10: error: 129.78849829023844 classification score: 0.9737793851717902\n",
      "training epoch 11: error: 132.63842684198863 classification score: 0.9556962025316456\n",
      "training epoch 12: error: 183.19944358398135 classification score: 0.9349005424954792\n",
      "training epoch 13: error: 102.34749337102818 classification score: 0.9864376130198915\n",
      "training epoch 14: error: 98.28352294730489 classification score: 0.9629294755877035\n",
      "training epoch 15: error: 76.94354319374877 classification score: 0.9864376130198915\n",
      "training epoch 16: error: 58.61875760506982 classification score: 0.9918625678119349\n",
      "training epoch 17: error: 54.09407274293398 classification score: 0.9927667269439421\n",
      "training epoch 18: error: 100.49453686123357 classification score: 0.9819168173598554\n",
      "training epoch 19: error: 31.700523970246888 classification score: 0.9963833634719711\n",
      "training epoch 20: error: 20.174280931838283 classification score: 0.9981916817359855\n",
      "\n",
      "fold 5: error: 33.9175561968061 classification score: 0.9818840579710145\n",
      "\n",
      "average error: 18.1371260444805 average classification score 0.9855229424998692\n",
      "CPU times: total: 20.8 s\n",
      "Wall time: 24.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "18.1371260444805"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_layered(car_data, 'class', train_autoencoder_network, (15, 15))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1: error: 916.1785804892687 classification score: 0.702262443438914\n",
      "training epoch 2: error: 768.6892766639793 classification score: 0.702262443438914\n",
      "training epoch 3: error: 717.9231709908964 classification score: 0.702262443438914\n",
      "training epoch 4: error: 651.7780831074484 classification score: 0.7547511312217194\n",
      "training epoch 5: error: 491.2555840907267 classification score: 0.8760180995475113\n",
      "training epoch 6: error: 401.3472454753957 classification score: 0.8850678733031674\n",
      "training epoch 7: error: 440.40355661055753 classification score: 0.8787330316742081\n",
      "training epoch 8: error: 392.10001637573004 classification score: 0.9076923076923077\n",
      "training epoch 9: error: 366.62285063590866 classification score: 0.9031674208144796\n",
      "training epoch 10: error: 332.0445653142864 classification score: 0.9031674208144796\n",
      "training epoch 11: error: 323.2937414031431 classification score: 0.9122171945701357\n",
      "training epoch 12: error: 322.4988886968269 classification score: 0.9248868778280543\n",
      "training epoch 13: error: 265.27674818483115 classification score: 0.951131221719457\n",
      "training epoch 14: error: 271.14235834472845 classification score: 0.9447963800904977\n",
      "training epoch 15: error: 220.145018371382 classification score: 0.9484162895927601\n",
      "training epoch 16: error: 225.35900025616482 classification score: 0.9529411764705882\n",
      "training epoch 17: error: 232.05965026016858 classification score: 0.9475113122171945\n",
      "training epoch 18: error: 196.85433751141116 classification score: 0.9520361990950226\n",
      "training epoch 19: error: 197.7396943523793 classification score: 0.9592760180995475\n",
      "training epoch 20: error: 159.3874429847485 classification score: 0.9773755656108597\n",
      "\n",
      "fold 1: error: 54.745676357022916 classification score: 0.9602888086642599\n",
      "\n",
      "training epoch 1: error: 914.8873814242237 classification score: 0.7049773755656109\n",
      "training epoch 2: error: 759.9387415261176 classification score: 0.7049773755656109\n",
      "training epoch 3: error: 711.3262542145022 classification score: 0.7049773755656109\n",
      "training epoch 4: error: 608.3917050575485 classification score: 0.8063348416289593\n",
      "training epoch 5: error: 475.7225002125787 classification score: 0.8868778280542986\n",
      "training epoch 6: error: 429.41123534297054 classification score: 0.8941176470588236\n",
      "training epoch 7: error: 435.909643641106 classification score: 0.8678733031674208\n",
      "training epoch 8: error: 360.7474212810206 classification score: 0.9095022624434389\n",
      "training epoch 9: error: 389.7905837511265 classification score: 0.8950226244343892\n",
      "training epoch 10: error: 341.53431608332534 classification score: 0.9194570135746606\n",
      "training epoch 11: error: 332.1479454348502 classification score: 0.9212669683257918\n",
      "training epoch 12: error: 347.3709075536193 classification score: 0.9058823529411765\n",
      "training epoch 13: error: 371.3046677494095 classification score: 0.9140271493212669\n",
      "training epoch 14: error: 279.44719932256993 classification score: 0.9366515837104072\n",
      "training epoch 15: error: 265.7227805090569 classification score: 0.9366515837104072\n",
      "training epoch 16: error: 242.368779327823 classification score: 0.9529411764705882\n",
      "training epoch 17: error: 216.80961164330537 classification score: 0.9656108597285068\n",
      "training epoch 18: error: 213.40059925247732 classification score: 0.9475113122171945\n",
      "training epoch 19: error: 186.57564740268867 classification score: 0.9520361990950226\n",
      "training epoch 20: error: 186.0601677163925 classification score: 0.9520361990950226\n",
      "\n",
      "fold 2: error: 68.38510403434081 classification score: 0.9350180505415162\n",
      "\n",
      "training epoch 1: error: 929.6001869539389 classification score: 0.6989150090415913\n",
      "training epoch 2: error: 770.171154765453 classification score: 0.6989150090415913\n",
      "training epoch 3: error: 721.1403168878466 classification score: 0.6989150090415913\n",
      "training epoch 4: error: 636.114458477577 classification score: 0.8200723327305606\n",
      "training epoch 5: error: 514.5733839281568 classification score: 0.8553345388788427\n",
      "training epoch 6: error: 408.8544128981077 classification score: 0.9014466546112115\n",
      "training epoch 7: error: 385.5822644752113 classification score: 0.9086799276672695\n",
      "training epoch 8: error: 398.8951473606797 classification score: 0.8824593128390597\n",
      "training epoch 9: error: 342.8226621838623 classification score: 0.8933092224231465\n",
      "training epoch 10: error: 363.1185234310089 classification score: 0.9050632911392406\n",
      "training epoch 11: error: 340.28427388465605 classification score: 0.9014466546112115\n",
      "training epoch 12: error: 303.33644366346414 classification score: 0.9222423146473779\n",
      "training epoch 13: error: 286.6469351292285 classification score: 0.9231464737793852\n",
      "training epoch 14: error: 265.2552651541281 classification score: 0.9349005424954792\n",
      "training epoch 15: error: 276.209178675216 classification score: 0.9294755877034359\n",
      "training epoch 16: error: 240.07413845198215 classification score: 0.9412296564195298\n",
      "training epoch 17: error: 234.50266120569307 classification score: 0.9511754068716094\n",
      "training epoch 18: error: 246.46345918180964 classification score: 0.9502712477396021\n",
      "training epoch 19: error: 215.4173264770431 classification score: 0.9430379746835443\n",
      "training epoch 20: error: 206.68810966731843 classification score: 0.9484629294755877\n",
      "\n",
      "fold 3: error: 74.2654631143476 classification score: 0.9239130434782609\n",
      "\n",
      "training epoch 1: error: 923.2464295340682 classification score: 0.7025316455696202\n",
      "training epoch 2: error: 760.1332391820646 classification score: 0.7025316455696202\n",
      "training epoch 3: error: 712.7169576301284 classification score: 0.7025316455696202\n",
      "training epoch 4: error: 570.6863421424907 classification score: 0.8887884267631103\n",
      "training epoch 5: error: 453.9655090152635 classification score: 0.8752260397830018\n",
      "training epoch 6: error: 416.2372933163072 classification score: 0.9041591320072333\n",
      "training epoch 7: error: 362.6474061194132 classification score: 0.9132007233273056\n",
      "training epoch 8: error: 356.9050221872655 classification score: 0.9023508137432188\n",
      "training epoch 9: error: 339.86390671502977 classification score: 0.9086799276672695\n",
      "training epoch 10: error: 303.657817578288 classification score: 0.9321880650994575\n",
      "training epoch 11: error: 316.15538433729165 classification score: 0.9168173598553345\n",
      "training epoch 12: error: 301.4358294404533 classification score: 0.9204339963833634\n",
      "training epoch 13: error: 257.39073483881236 classification score: 0.9376130198915009\n",
      "training epoch 14: error: 227.59570342590362 classification score: 0.9502712477396021\n",
      "training epoch 15: error: 210.14439268305364 classification score: 0.9547920433996383\n",
      "training epoch 16: error: 197.50100185922665 classification score: 0.9547920433996383\n",
      "training epoch 17: error: 192.78619136252252 classification score: 0.9556962025316456\n",
      "training epoch 18: error: 183.89492994994833 classification score: 0.9647377938517179\n",
      "training epoch 19: error: 177.12890239360047 classification score: 0.9575045207956601\n",
      "training epoch 20: error: 172.6681501779371 classification score: 0.9674502712477396\n",
      "\n",
      "fold 4: error: 76.48591031963029 classification score: 0.9456521739130435\n",
      "\n",
      "training epoch 1: error: 956.4444341501077 classification score: 0.7007233273056058\n",
      "training epoch 2: error: 769.1026954535678 classification score: 0.7007233273056058\n",
      "training epoch 3: error: 717.4856056720739 classification score: 0.7007233273056058\n",
      "training epoch 4: error: 567.3026466549757 classification score: 0.8625678119349005\n",
      "training epoch 5: error: 456.38328399803464 classification score: 0.8725135623869801\n",
      "training epoch 6: error: 416.61676750281026 classification score: 0.8806509945750453\n",
      "training epoch 7: error: 402.6421810023715 classification score: 0.9086799276672695\n",
      "training epoch 8: error: 380.2421223020791 classification score: 0.906871609403255\n",
      "training epoch 9: error: 357.74148777035975 classification score: 0.9104882459312839\n",
      "training epoch 10: error: 347.7450326812677 classification score: 0.9050632911392406\n",
      "training epoch 11: error: 316.7662278238755 classification score: 0.9285714285714286\n",
      "training epoch 12: error: 312.94321667391705 classification score: 0.9204339963833634\n",
      "training epoch 13: error: 287.90745273269897 classification score: 0.9267631103074141\n",
      "training epoch 14: error: 345.9920583173252 classification score: 0.9077757685352622\n",
      "training epoch 15: error: 249.13200677420596 classification score: 0.9439421338155516\n",
      "training epoch 16: error: 236.1526009251029 classification score: 0.9466546112115732\n",
      "training epoch 17: error: 225.22897451693387 classification score: 0.9448462929475587\n",
      "training epoch 18: error: 196.9135349537933 classification score: 0.969258589511754\n",
      "training epoch 19: error: 207.33560967090156 classification score: 0.953887884267631\n",
      "training epoch 20: error: 204.1442508625259 classification score: 0.9502712477396021\n",
      "\n",
      "fold 5: error: 58.52975509603205 classification score: 0.9456521739130435\n",
      "\n",
      "average error: 66.48238178427474 average classification score 0.9421048501020248\n",
      "CPU times: total: 17.2 s\n",
      "Wall time: 19.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "66.48238178427474"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_layered(car_data, 'class', train_layered_classification_network, (15,15))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1: error: 460.11644001103906 classification score: 0.8714932126696833\n",
      "training epoch 2: error: 392.1570898507258 classification score: 0.8923076923076924\n",
      "training epoch 3: error: 381.0680316511004 classification score: 0.902262443438914\n",
      "training epoch 4: error: 331.7993409788624 classification score: 0.9104072398190045\n",
      "training epoch 5: error: 328.0852456557689 classification score: 0.902262443438914\n",
      "training epoch 6: error: 321.3073456468773 classification score: 0.9239819004524887\n",
      "training epoch 7: error: 294.66456477378307 classification score: 0.9266968325791856\n",
      "training epoch 8: error: 314.3482558390268 classification score: 0.9004524886877828\n",
      "training epoch 9: error: 283.70960474425203 classification score: 0.9194570135746606\n",
      "training epoch 10: error: 283.2443598741395 classification score: 0.9239819004524887\n",
      "training epoch 11: error: 318.9353835923742 classification score: 0.9158371040723982\n",
      "training epoch 12: error: 306.311635366196 classification score: 0.9230769230769231\n",
      "training epoch 13: error: 265.5103988316718 classification score: 0.9294117647058824\n",
      "training epoch 14: error: 278.51017449481185 classification score: 0.92579185520362\n",
      "training epoch 15: error: 276.7672743920531 classification score: 0.930316742081448\n",
      "training epoch 16: error: 253.15814698421542 classification score: 0.9321266968325792\n",
      "training epoch 17: error: 295.3493363048326 classification score: 0.9276018099547512\n",
      "training epoch 18: error: 263.9741294469818 classification score: 0.9384615384615385\n",
      "training epoch 19: error: 262.18211011050283 classification score: 0.930316742081448\n",
      "training epoch 20: error: 273.39138727324394 classification score: 0.92579185520362\n",
      "\n",
      "fold 1: error: 71.04444967698888 classification score: 0.9061371841155235\n",
      "\n",
      "training epoch 1: error: 478.46770429778115 classification score: 0.8660633484162896\n",
      "training epoch 2: error: 383.26314079777717 classification score: 0.8959276018099548\n",
      "training epoch 3: error: 356.29853869876325 classification score: 0.9067873303167421\n",
      "training epoch 4: error: 317.61211377853095 classification score: 0.9194570135746606\n",
      "training epoch 5: error: 385.62390219859475 classification score: 0.9049773755656109\n",
      "training epoch 6: error: 308.00952749238337 classification score: 0.9158371040723982\n",
      "training epoch 7: error: 297.65210209120727 classification score: 0.9212669683257918\n",
      "training epoch 8: error: 286.17004155097237 classification score: 0.9212669683257918\n",
      "training epoch 9: error: 265.8969120556421 classification score: 0.9339366515837104\n",
      "training epoch 10: error: 275.11016913333157 classification score: 0.9330316742081448\n",
      "training epoch 11: error: 272.27570106399577 classification score: 0.92579185520362\n",
      "training epoch 12: error: 267.81663629328597 classification score: 0.9357466063348416\n",
      "training epoch 13: error: 296.16399308788226 classification score: 0.9131221719457013\n",
      "training epoch 14: error: 258.5955612120294 classification score: 0.9294117647058824\n",
      "training epoch 15: error: 258.58431994572766 classification score: 0.9330316742081448\n",
      "training epoch 16: error: 269.52209128935 classification score: 0.930316742081448\n",
      "training epoch 17: error: 249.8442892628717 classification score: 0.9420814479638009\n",
      "training epoch 18: error: 251.08333598090152 classification score: 0.9312217194570136\n",
      "training epoch 19: error: 261.81273080258813 classification score: 0.9402714932126697\n",
      "training epoch 20: error: 250.23714613797924 classification score: 0.9402714932126697\n",
      "\n",
      "fold 2: error: 78.35832385936642 classification score: 0.9205776173285198\n",
      "\n",
      "training epoch 1: error: 452.8826828209018 classification score: 0.8679927667269439\n",
      "training epoch 2: error: 405.3002889223849 classification score: 0.8878842676311031\n",
      "training epoch 3: error: 350.61329191806993 classification score: 0.9095840867992767\n",
      "training epoch 4: error: 350.7019000155402 classification score: 0.9113924050632911\n",
      "training epoch 5: error: 297.93600881003863 classification score: 0.9213381555153707\n",
      "training epoch 6: error: 296.4815907531881 classification score: 0.9240506329113924\n",
      "training epoch 7: error: 291.2478076122007 classification score: 0.933996383363472\n",
      "training epoch 8: error: 314.77758116695475 classification score: 0.9195298372513563\n",
      "training epoch 9: error: 275.72625417425013 classification score: 0.933996383363472\n",
      "training epoch 10: error: 323.75384887373093 classification score: 0.9159132007233273\n",
      "training epoch 11: error: 268.94383039791745 classification score: 0.9330922242314648\n",
      "training epoch 12: error: 257.0366328158757 classification score: 0.9376130198915009\n",
      "training epoch 13: error: 267.9053096723827 classification score: 0.9421338155515371\n",
      "training epoch 14: error: 270.4560206465149 classification score: 0.9367088607594937\n",
      "training epoch 15: error: 275.9775254820472 classification score: 0.9349005424954792\n",
      "training epoch 16: error: 256.9035344933276 classification score: 0.930379746835443\n",
      "training epoch 17: error: 294.95520379869924 classification score: 0.9195298372513563\n",
      "training epoch 18: error: 249.7997954215886 classification score: 0.9358047016274864\n",
      "training epoch 19: error: 275.5027820699742 classification score: 0.9376130198915009\n",
      "training epoch 20: error: 263.6625680987296 classification score: 0.930379746835443\n",
      "\n",
      "fold 3: error: 85.69388429634951 classification score: 0.8913043478260869\n",
      "\n",
      "training epoch 1: error: 482.3469278453612 classification score: 0.864376130198915\n",
      "training epoch 2: error: 410.8820436843842 classification score: 0.891500904159132\n",
      "training epoch 3: error: 341.5667411790932 classification score: 0.9077757685352622\n",
      "training epoch 4: error: 341.4975486711814 classification score: 0.903254972875226\n",
      "training epoch 5: error: 357.0771633309965 classification score: 0.903254972875226\n",
      "training epoch 6: error: 302.9959612240173 classification score: 0.9213381555153707\n",
      "training epoch 7: error: 312.8648880754733 classification score: 0.9077757685352622\n",
      "training epoch 8: error: 287.9302182604581 classification score: 0.9249547920433996\n",
      "training epoch 9: error: 298.7570577171529 classification score: 0.9177215189873418\n",
      "training epoch 10: error: 318.8647068728665 classification score: 0.9267631103074141\n",
      "training epoch 11: error: 279.413731705585 classification score: 0.9312839059674503\n",
      "training epoch 12: error: 289.74422682880663 classification score: 0.9249547920433996\n",
      "training epoch 13: error: 275.71860817081523 classification score: 0.9358047016274864\n",
      "training epoch 14: error: 339.5268713346697 classification score: 0.9168173598553345\n",
      "training epoch 15: error: 266.6345011005397 classification score: 0.930379746835443\n",
      "training epoch 16: error: 273.2341503727382 classification score: 0.9321880650994575\n",
      "training epoch 17: error: 310.988500575956 classification score: 0.9177215189873418\n",
      "training epoch 18: error: 304.4316103276968 classification score: 0.9141048824593129\n",
      "training epoch 19: error: 253.0605943867064 classification score: 0.9312839059674503\n",
      "training epoch 20: error: 255.56710507218685 classification score: 0.9330922242314648\n",
      "\n",
      "fold 4: error: 68.98939603133522 classification score: 0.9130434782608695\n",
      "\n",
      "training epoch 1: error: 430.3077253622354 classification score: 0.8969258589511754\n",
      "training epoch 2: error: 397.895305823015 classification score: 0.8960216998191681\n",
      "training epoch 3: error: 388.3774108722271 classification score: 0.8978300180831826\n",
      "training epoch 4: error: 354.26302390114256 classification score: 0.9095840867992767\n",
      "training epoch 5: error: 315.81907681205945 classification score: 0.9150090415913201\n",
      "training epoch 6: error: 307.3186259187905 classification score: 0.9240506329113924\n",
      "training epoch 7: error: 297.5804854986044 classification score: 0.9204339963833634\n",
      "training epoch 8: error: 287.58932247818535 classification score: 0.930379746835443\n",
      "training epoch 9: error: 283.08053979279816 classification score: 0.9177215189873418\n",
      "training epoch 10: error: 273.05763041736327 classification score: 0.9321880650994575\n",
      "training epoch 11: error: 266.73424993525504 classification score: 0.9376130198915009\n",
      "training epoch 12: error: 286.91040761489495 classification score: 0.9222423146473779\n",
      "training epoch 13: error: 279.6831018191274 classification score: 0.9385171790235082\n",
      "training epoch 14: error: 250.49718027784377 classification score: 0.9321880650994575\n",
      "training epoch 15: error: 250.05467039575257 classification score: 0.933996383363472\n",
      "training epoch 16: error: 258.7807703994374 classification score: 0.9330922242314648\n",
      "training epoch 17: error: 254.73384555919318 classification score: 0.9285714285714286\n",
      "training epoch 18: error: 315.52973672445694 classification score: 0.9104882459312839\n",
      "training epoch 19: error: 248.39556353528556 classification score: 0.9421338155515371\n",
      "training epoch 20: error: 257.401052984125 classification score: 0.9394213381555153\n",
      "\n",
      "fold 5: error: 93.31900977601171 classification score: 0.9021739130434783\n",
      "\n",
      "average error: 79.48101272801034 average classification score 0.9066473081148956\n",
      "CPU times: total: 1min 17s\n",
      "Wall time: 1min 24s\n"
     ]
    },
    {
     "data": {
      "text/plain": "79.48101272801034"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation(car_data, 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_votes_data = load_house_votes('datasets/house-votes-84.data')\n",
    "house_votes_data.name = 'house votes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "     class - democrat  class - republican  handicapped infants - ?  \\\n0                   0                   1                        0   \n1                   0                   1                        0   \n2                   1                   0                        1   \n3                   1                   0                        0   \n4                   1                   0                        0   \n..                ...                 ...                      ...   \n430                 0                   1                        0   \n431                 1                   0                        0   \n432                 0                   1                        0   \n433                 0                   1                        0   \n434                 0                   1                        0   \n\n     handicapped infants - n  handicapped infants - y  \\\n0                          1                        0   \n1                          1                        0   \n2                          0                        0   \n3                          1                        0   \n4                          0                        1   \n..                       ...                      ...   \n430                        1                        0   \n431                        1                        0   \n432                        1                        0   \n433                        1                        0   \n434                        1                        0   \n\n     water project cost sharing - ?  water project cost sharing - n  \\\n0                                 0                               0   \n1                                 0                               0   \n2                                 0                               0   \n3                                 0                               0   \n4                                 0                               0   \n..                              ...                             ...   \n430                               0                               1   \n431                               0                               1   \n432                               1                               0   \n433                               0                               1   \n434                               0                               0   \n\n     water project cost sharing - y  adoption of the budget resolution - ?  \\\n0                                 1                                      0   \n1                                 1                                      0   \n2                                 1                                      0   \n3                                 1                                      0   \n4                                 1                                      0   \n..                              ...                                    ...   \n430                               0                                      0   \n431                               0                                      0   \n432                               0                                      0   \n433                               0                                      0   \n434                               1                                      0   \n\n     adoption of the budget resolution - n  ...  superfund right to sue - y  \\\n0                                        1  ...                           1   \n1                                        1  ...                           1   \n2                                        0  ...                           1   \n3                                        0  ...                           1   \n4                                        0  ...                           1   \n..                                     ...  ...                         ...   \n430                                      0  ...                           1   \n431                                      0  ...                           0   \n432                                      1  ...                           1   \n433                                      1  ...                           1   \n434                                      1  ...                           1   \n\n     crime - ?  crime - n  crime - y  duty free exports - ?  \\\n0            0          0          1                      0   \n1            0          0          1                      0   \n2            0          0          1                      0   \n3            0          1          0                      0   \n4            0          0          1                      0   \n..         ...        ...        ...                    ...   \n430          0          0          1                      0   \n431          0          1          0                      0   \n432          0          0          1                      0   \n433          0          0          1                      0   \n434          0          0          1                      1   \n\n     duty free exports - n  duty free exports - y  \\\n0                        1                      0   \n1                        1                      0   \n2                        1                      0   \n3                        1                      0   \n4                        0                      1   \n..                     ...                    ...   \n430                      1                      0   \n431                      1                      0   \n432                      1                      0   \n433                      1                      0   \n434                      0                      0   \n\n     export administration act south africa - ?  \\\n0                                             0   \n1                                             1   \n2                                             0   \n3                                             0   \n4                                             0   \n..                                          ...   \n430                                           0   \n431                                           0   \n432                                           0   \n433                                           0   \n434                                           0   \n\n     export administration act south africa - n  \\\n0                                             0   \n1                                             0   \n2                                             1   \n3                                             0   \n4                                             0   \n..                                          ...   \n430                                           0   \n431                                           0   \n432                                           0   \n433                                           0   \n434                                           1   \n\n     export administration act south africa - y  \n0                                             1  \n1                                             0  \n2                                             0  \n3                                             1  \n4                                             1  \n..                                          ...  \n430                                           1  \n431                                           1  \n432                                           1  \n433                                           1  \n434                                           0  \n\n[435 rows x 50 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class - democrat</th>\n      <th>class - republican</th>\n      <th>handicapped infants - ?</th>\n      <th>handicapped infants - n</th>\n      <th>handicapped infants - y</th>\n      <th>water project cost sharing - ?</th>\n      <th>water project cost sharing - n</th>\n      <th>water project cost sharing - y</th>\n      <th>adoption of the budget resolution - ?</th>\n      <th>adoption of the budget resolution - n</th>\n      <th>...</th>\n      <th>superfund right to sue - y</th>\n      <th>crime - ?</th>\n      <th>crime - n</th>\n      <th>crime - y</th>\n      <th>duty free exports - ?</th>\n      <th>duty free exports - n</th>\n      <th>duty free exports - y</th>\n      <th>export administration act south africa - ?</th>\n      <th>export administration act south africa - n</th>\n      <th>export administration act south africa - y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>430</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>431</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>432</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>433</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>434</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>435 rows  50 columns</p>\n</div>"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_votes_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoencoder training epoch 1: decode layer difference: 1686.0\n",
      "autoencoder training epoch 2: decode layer difference: 980.0\n",
      "autoencoder training epoch 3: decode layer difference: 718.0\n",
      "autoencoder training epoch 4: decode layer difference: 472.0\n",
      "autoencoder training epoch 5: decode layer difference: 333.0\n",
      "autoencoder training epoch 6: decode layer difference: 241.0\n",
      "autoencoder training epoch 7: decode layer difference: 194.0\n",
      "autoencoder training epoch 8: decode layer difference: 173.0\n",
      "autoencoder training epoch 9: decode layer difference: 156.0\n",
      "autoencoder training epoch 10: decode layer difference: 118.0\n",
      "autoencoder training epoch 11: decode layer difference: 94.0\n",
      "autoencoder training epoch 12: decode layer difference: 72.0\n",
      "autoencoder training epoch 13: decode layer difference: 56.0\n",
      "autoencoder training epoch 14: decode layer difference: 46.0\n",
      "autoencoder training epoch 15: decode layer difference: 42.0\n",
      "autoencoder training epoch 16: decode layer difference: 32.0\n",
      "autoencoder training epoch 17: decode layer difference: 28.0\n",
      "autoencoder training epoch 18: decode layer difference: 13.0\n",
      "autoencoder training epoch 19: decode layer difference: 11.0\n",
      "autoencoder training epoch 20: decode layer difference: 12.0\n",
      "autoencoder training epoch 21: decode layer difference: 12.0\n",
      "autoencoder training epoch 22: decode layer difference: 12.0\n",
      "autoencoder training epoch 23: decode layer difference: 11.0\n",
      "autoencoder training epoch 24: decode layer difference: 11.0\n",
      "autoencoder training epoch 25: decode layer difference: 10.0\n",
      "autoencoder training epoch 26: decode layer difference: 9.0\n",
      "autoencoder training epoch 27: decode layer difference: 7.0\n",
      "autoencoder training epoch 28: decode layer difference: 7.0\n",
      "autoencoder training epoch 29: decode layer difference: 6.0\n",
      "autoencoder training epoch 30: decode layer difference: 7.0\n",
      "autoencoder training epoch 31: decode layer difference: 7.0\n",
      "autoencoder training epoch 32: decode layer difference: 7.0\n",
      "autoencoder training epoch 33: decode layer difference: 8.0\n",
      "autoencoder training epoch 34: decode layer difference: 8.0\n",
      "autoencoder training epoch 35: decode layer difference: 6.0\n",
      "autoencoder training epoch 36: decode layer difference: 5.0\n",
      "autoencoder training epoch 37: decode layer difference: 5.0\n",
      "autoencoder training epoch 38: decode layer difference: 5.0\n",
      "autoencoder training epoch 39: decode layer difference: 5.0\n",
      "autoencoder training epoch 40: decode layer difference: 5.0\n",
      "autoencoder training epoch 41: decode layer difference: 5.0\n",
      "autoencoder training epoch 42: decode layer difference: 5.0\n",
      "autoencoder training epoch 43: decode layer difference: 5.0\n",
      "autoencoder training epoch 44: decode layer difference: 5.0\n",
      "autoencoder training epoch 45: decode layer difference: 7.0\n",
      "autoencoder training epoch 46: decode layer difference: 6.0\n",
      "autoencoder training epoch 47: decode layer difference: 7.0\n",
      "autoencoder training epoch 48: decode layer difference: 4.0\n",
      "autoencoder training epoch 49: decode layer difference: 5.0\n",
      "autoencoder training epoch 50: decode layer difference: 5.0\n",
      "autoencoder training epoch 51: decode layer difference: 7.0\n",
      "autoencoder training epoch 52: decode layer difference: 4.0\n",
      "autoencoder training epoch 53: decode layer difference: 5.0\n",
      "autoencoder training epoch 54: decode layer difference: 4.0\n",
      "autoencoder training epoch 55: decode layer difference: 5.0\n",
      "autoencoder training epoch 56: decode layer difference: 5.0\n",
      "autoencoder training epoch 57: decode layer difference: 4.0\n",
      "autoencoder training epoch 58: decode layer difference: 5.0\n",
      "autoencoder training epoch 59: decode layer difference: 5.0\n",
      "autoencoder training epoch 60: decode layer difference: 4.0\n",
      "autoencoder training epoch 61: decode layer difference: 5.0\n",
      "autoencoder training epoch 62: decode layer difference: 4.0\n",
      "autoencoder training epoch 63: decode layer difference: 4.0\n",
      "autoencoder training epoch 64: decode layer difference: 3.0\n",
      "autoencoder training epoch 65: decode layer difference: 3.0\n",
      "autoencoder training epoch 66: decode layer difference: 3.0\n",
      "autoencoder training epoch 67: decode layer difference: 3.0\n",
      "autoencoder training epoch 68: decode layer difference: 2.0\n",
      "autoencoder training epoch 69: decode layer difference: 2.0\n",
      "autoencoder training epoch 70: decode layer difference: 3.0\n",
      "autoencoder training epoch 71: decode layer difference: 4.0\n",
      "autoencoder training epoch 72: decode layer difference: 3.0\n",
      "autoencoder training epoch 73: decode layer difference: 5.0\n",
      "autoencoder training epoch 74: decode layer difference: 2.0\n",
      "autoencoder training epoch 75: decode layer difference: 4.0\n",
      "autoencoder training epoch 76: decode layer difference: 4.0\n",
      "autoencoder training epoch 77: decode layer difference: 4.0\n",
      "autoencoder training epoch 78: decode layer difference: 1.0\n",
      "autoencoder training epoch 79: decode layer difference: 3.0\n",
      "autoencoder training epoch 80: decode layer difference: 2.0\n",
      "autoencoder training epoch 81: decode layer difference: 0.0\n",
      "training epoch 1: error: 77.3868059987994 classification score: 0.9388489208633094\n",
      "training epoch 2: error: 31.95609107263013 classification score: 0.9712230215827338\n",
      "training epoch 3: error: 40.598602302601854 classification score: 0.9640287769784173\n",
      "training epoch 4: error: 18.39509623474181 classification score: 0.9784172661870504\n",
      "training epoch 5: error: 45.54476743679881 classification score: 0.9568345323741008\n",
      "training epoch 6: error: 14.635410174603438 classification score: 0.9820143884892086\n",
      "training epoch 7: error: 104.23614918512277 classification score: 0.8597122302158273\n",
      "training epoch 8: error: 7.236113508586024 classification score: 0.9928057553956835\n",
      "training epoch 9: error: 151.02030985619942 classification score: 0.8669064748201439\n",
      "training epoch 10: error: 24.89072724439613 classification score: 0.9820143884892086\n",
      "training epoch 11: error: 5.482590650534091 classification score: 0.9964028776978417\n",
      "training epoch 12: error: 3.431387407563274 classification score: 0.9964028776978417\n",
      "training epoch 13: error: 2.335168283624941 classification score: 1.0\n",
      "\n",
      "fold 1: error: 16.31724230314206 classification score: 0.9571428571428572\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 1674.0\n",
      "autoencoder training epoch 2: decode layer difference: 1089.0\n",
      "autoencoder training epoch 3: decode layer difference: 699.0\n",
      "autoencoder training epoch 4: decode layer difference: 560.0\n",
      "autoencoder training epoch 5: decode layer difference: 367.0\n",
      "autoencoder training epoch 6: decode layer difference: 301.0\n",
      "autoencoder training epoch 7: decode layer difference: 237.0\n",
      "autoencoder training epoch 8: decode layer difference: 189.0\n",
      "autoencoder training epoch 9: decode layer difference: 132.0\n",
      "autoencoder training epoch 10: decode layer difference: 115.0\n",
      "autoencoder training epoch 11: decode layer difference: 99.0\n",
      "autoencoder training epoch 12: decode layer difference: 81.0\n",
      "autoencoder training epoch 13: decode layer difference: 61.0\n",
      "autoencoder training epoch 14: decode layer difference: 41.0\n",
      "autoencoder training epoch 15: decode layer difference: 42.0\n",
      "autoencoder training epoch 16: decode layer difference: 31.0\n",
      "autoencoder training epoch 17: decode layer difference: 24.0\n",
      "autoencoder training epoch 18: decode layer difference: 25.0\n",
      "autoencoder training epoch 19: decode layer difference: 17.0\n",
      "autoencoder training epoch 20: decode layer difference: 15.0\n",
      "autoencoder training epoch 21: decode layer difference: 10.0\n",
      "autoencoder training epoch 22: decode layer difference: 6.0\n",
      "autoencoder training epoch 23: decode layer difference: 5.0\n",
      "autoencoder training epoch 24: decode layer difference: 2.0\n",
      "autoencoder training epoch 25: decode layer difference: 4.0\n",
      "autoencoder training epoch 26: decode layer difference: 5.0\n",
      "autoencoder training epoch 27: decode layer difference: 1.0\n",
      "autoencoder training epoch 28: decode layer difference: 0.0\n",
      "training epoch 1: error: 75.88566750636173 classification score: 0.9280575539568345\n",
      "training epoch 2: error: 37.551058366632205 classification score: 0.960431654676259\n",
      "training epoch 3: error: 37.06025523989141 classification score: 0.9640287769784173\n",
      "training epoch 4: error: 28.008836717876097 classification score: 0.9640287769784173\n",
      "training epoch 5: error: 84.75809660198091 classification score: 0.9100719424460432\n",
      "training epoch 6: error: 26.62530805412005 classification score: 0.9712230215827338\n",
      "training epoch 7: error: 13.107660487222084 classification score: 0.9892086330935251\n",
      "training epoch 8: error: 11.309285584063396 classification score: 0.9892086330935251\n",
      "training epoch 9: error: 11.40073035886353 classification score: 0.9892086330935251\n",
      "training epoch 10: error: 13.367112058653333 classification score: 0.9892086330935251\n",
      "training epoch 11: error: 5.858171476798659 classification score: 0.9928057553956835\n",
      "training epoch 12: error: 5.842031405877068 classification score: 0.9928057553956835\n",
      "training epoch 13: error: 10.324789301851297 classification score: 0.9928057553956835\n",
      "training epoch 14: error: 4.543501236272492 classification score: 0.9964028776978417\n",
      "training epoch 15: error: 4.020493457218447 classification score: 0.9928057553956835\n",
      "training epoch 16: error: 2.807265401075854 classification score: 0.9964028776978417\n",
      "training epoch 17: error: 3.0977175784775195 classification score: 0.9964028776978417\n",
      "training epoch 18: error: 1.9342033676507175 classification score: 0.9964028776978417\n",
      "training epoch 19: error: 1.6680980678997146 classification score: 1.0\n",
      "\n",
      "fold 2: error: 11.473268299518566 classification score: 0.9714285714285714\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 1749.0\n",
      "autoencoder training epoch 2: decode layer difference: 970.0\n",
      "autoencoder training epoch 3: decode layer difference: 721.0\n",
      "autoencoder training epoch 4: decode layer difference: 518.0\n",
      "autoencoder training epoch 5: decode layer difference: 373.0\n",
      "autoencoder training epoch 6: decode layer difference: 284.0\n",
      "autoencoder training epoch 7: decode layer difference: 200.0\n",
      "autoencoder training epoch 8: decode layer difference: 169.0\n",
      "autoencoder training epoch 9: decode layer difference: 144.0\n",
      "autoencoder training epoch 10: decode layer difference: 114.0\n",
      "autoencoder training epoch 11: decode layer difference: 90.0\n",
      "autoencoder training epoch 12: decode layer difference: 73.0\n",
      "autoencoder training epoch 13: decode layer difference: 62.0\n",
      "autoencoder training epoch 14: decode layer difference: 48.0\n",
      "autoencoder training epoch 15: decode layer difference: 39.0\n",
      "autoencoder training epoch 16: decode layer difference: 59.0\n",
      "autoencoder training epoch 17: decode layer difference: 22.0\n",
      "autoencoder training epoch 18: decode layer difference: 18.0\n",
      "autoencoder training epoch 19: decode layer difference: 19.0\n",
      "autoencoder training epoch 20: decode layer difference: 17.0\n",
      "autoencoder training epoch 21: decode layer difference: 10.0\n",
      "autoencoder training epoch 22: decode layer difference: 12.0\n",
      "autoencoder training epoch 23: decode layer difference: 11.0\n",
      "autoencoder training epoch 24: decode layer difference: 6.0\n",
      "autoencoder training epoch 25: decode layer difference: 8.0\n",
      "autoencoder training epoch 26: decode layer difference: 6.0\n",
      "autoencoder training epoch 27: decode layer difference: 4.0\n",
      "autoencoder training epoch 28: decode layer difference: 7.0\n",
      "autoencoder training epoch 29: decode layer difference: 3.0\n",
      "autoencoder training epoch 30: decode layer difference: 3.0\n",
      "autoencoder training epoch 31: decode layer difference: 3.0\n",
      "autoencoder training epoch 32: decode layer difference: 3.0\n",
      "autoencoder training epoch 33: decode layer difference: 2.0\n",
      "autoencoder training epoch 34: decode layer difference: 2.0\n",
      "autoencoder training epoch 35: decode layer difference: 2.0\n",
      "autoencoder training epoch 36: decode layer difference: 2.0\n",
      "autoencoder training epoch 37: decode layer difference: 3.0\n",
      "autoencoder training epoch 38: decode layer difference: 4.0\n",
      "autoencoder training epoch 39: decode layer difference: 5.0\n",
      "autoencoder training epoch 40: decode layer difference: 4.0\n",
      "autoencoder training epoch 41: decode layer difference: 1.0\n",
      "autoencoder training epoch 42: decode layer difference: 0.0\n",
      "training epoch 1: error: 69.36319289421267 classification score: 0.9244604316546763\n",
      "training epoch 2: error: 44.54025047918379 classification score: 0.9568345323741008\n",
      "training epoch 3: error: 22.92151427480163 classification score: 0.9856115107913669\n",
      "training epoch 4: error: 31.573558678689544 classification score: 0.9748201438848921\n",
      "training epoch 5: error: 15.884910023411791 classification score: 0.9856115107913669\n",
      "training epoch 6: error: 16.322490582122086 classification score: 0.9856115107913669\n",
      "training epoch 7: error: 18.275090922408094 classification score: 0.9820143884892086\n",
      "training epoch 8: error: 8.553425886078681 classification score: 0.9964028776978417\n",
      "training epoch 9: error: 12.489641278269433 classification score: 0.9892086330935251\n",
      "training epoch 10: error: 7.852458213459089 classification score: 0.9928057553956835\n",
      "training epoch 11: error: 6.514405558116329 classification score: 0.9928057553956835\n",
      "training epoch 12: error: 8.652903052743738 classification score: 0.9928057553956835\n",
      "training epoch 13: error: 9.743664058174572 classification score: 0.9928057553956835\n",
      "training epoch 14: error: 7.181017337551647 classification score: 0.9928057553956835\n",
      "training epoch 15: error: 4.181559361483941 classification score: 0.9964028776978417\n",
      "training epoch 16: error: 8.337323148101726 classification score: 0.9892086330935251\n",
      "training epoch 17: error: 4.698548129114946 classification score: 0.9928057553956835\n",
      "training epoch 18: error: 8.284741158903127 classification score: 0.9892086330935251\n",
      "training epoch 19: error: 2.7897221393573837 classification score: 0.9964028776978417\n",
      "training epoch 20: error: 5.115582192034189 classification score: 0.9964028776978417\n",
      "\n",
      "fold 3: error: 23.653189548862237 classification score: 0.9571428571428572\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 1570.0\n",
      "autoencoder training epoch 2: decode layer difference: 1011.0\n",
      "autoencoder training epoch 3: decode layer difference: 712.0\n",
      "autoencoder training epoch 4: decode layer difference: 566.0\n",
      "autoencoder training epoch 5: decode layer difference: 437.0\n",
      "autoencoder training epoch 6: decode layer difference: 310.0\n",
      "autoencoder training epoch 7: decode layer difference: 231.0\n",
      "autoencoder training epoch 8: decode layer difference: 200.0\n",
      "autoencoder training epoch 9: decode layer difference: 170.0\n",
      "autoencoder training epoch 10: decode layer difference: 135.0\n",
      "autoencoder training epoch 11: decode layer difference: 118.0\n",
      "autoencoder training epoch 12: decode layer difference: 106.0\n",
      "autoencoder training epoch 13: decode layer difference: 96.0\n",
      "autoencoder training epoch 14: decode layer difference: 78.0\n",
      "autoencoder training epoch 15: decode layer difference: 49.0\n",
      "autoencoder training epoch 16: decode layer difference: 45.0\n",
      "autoencoder training epoch 17: decode layer difference: 38.0\n",
      "autoencoder training epoch 18: decode layer difference: 34.0\n",
      "autoencoder training epoch 19: decode layer difference: 22.0\n",
      "autoencoder training epoch 20: decode layer difference: 39.0\n",
      "autoencoder training epoch 21: decode layer difference: 16.0\n",
      "autoencoder training epoch 22: decode layer difference: 15.0\n",
      "autoencoder training epoch 23: decode layer difference: 17.0\n",
      "autoencoder training epoch 24: decode layer difference: 12.0\n",
      "autoencoder training epoch 25: decode layer difference: 12.0\n",
      "autoencoder training epoch 26: decode layer difference: 8.0\n",
      "autoencoder training epoch 27: decode layer difference: 5.0\n",
      "autoencoder training epoch 28: decode layer difference: 5.0\n",
      "autoencoder training epoch 29: decode layer difference: 4.0\n",
      "autoencoder training epoch 30: decode layer difference: 4.0\n",
      "autoencoder training epoch 31: decode layer difference: 4.0\n",
      "autoencoder training epoch 32: decode layer difference: 2.0\n",
      "autoencoder training epoch 33: decode layer difference: 5.0\n",
      "autoencoder training epoch 34: decode layer difference: 3.0\n",
      "autoencoder training epoch 35: decode layer difference: 3.0\n",
      "autoencoder training epoch 36: decode layer difference: 3.0\n",
      "autoencoder training epoch 37: decode layer difference: 2.0\n",
      "autoencoder training epoch 38: decode layer difference: 5.0\n",
      "autoencoder training epoch 39: decode layer difference: 2.0\n",
      "autoencoder training epoch 40: decode layer difference: 3.0\n",
      "autoencoder training epoch 41: decode layer difference: 2.0\n",
      "autoencoder training epoch 42: decode layer difference: 3.0\n",
      "autoencoder training epoch 43: decode layer difference: 4.0\n",
      "autoencoder training epoch 44: decode layer difference: 2.0\n",
      "autoencoder training epoch 45: decode layer difference: 4.0\n",
      "autoencoder training epoch 46: decode layer difference: 0.0\n",
      "training epoch 1: error: 115.52524233590894 classification score: 0.8776978417266187\n",
      "training epoch 2: error: 62.50072728897839 classification score: 0.920863309352518\n",
      "training epoch 3: error: 33.86848587300402 classification score: 0.9712230215827338\n",
      "training epoch 4: error: 8.447545607756526 classification score: 0.9964028776978417\n",
      "training epoch 5: error: 4.490586181376243 classification score: 1.0\n",
      "\n",
      "fold 4: error: 20.340812474260527 classification score: 0.9571428571428572\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 1751.0\n",
      "autoencoder training epoch 2: decode layer difference: 1110.0\n",
      "autoencoder training epoch 3: decode layer difference: 814.0\n",
      "autoencoder training epoch 4: decode layer difference: 671.0\n",
      "autoencoder training epoch 5: decode layer difference: 473.0\n",
      "autoencoder training epoch 6: decode layer difference: 379.0\n",
      "autoencoder training epoch 7: decode layer difference: 307.0\n",
      "autoencoder training epoch 8: decode layer difference: 252.0\n",
      "autoencoder training epoch 9: decode layer difference: 237.0\n",
      "autoencoder training epoch 10: decode layer difference: 183.0\n",
      "autoencoder training epoch 11: decode layer difference: 143.0\n",
      "autoencoder training epoch 12: decode layer difference: 117.0\n",
      "autoencoder training epoch 13: decode layer difference: 85.0\n",
      "autoencoder training epoch 14: decode layer difference: 100.0\n",
      "autoencoder training epoch 15: decode layer difference: 95.0\n",
      "autoencoder training epoch 16: decode layer difference: 52.0\n",
      "autoencoder training epoch 17: decode layer difference: 32.0\n",
      "autoencoder training epoch 18: decode layer difference: 28.0\n",
      "autoencoder training epoch 19: decode layer difference: 22.0\n",
      "autoencoder training epoch 20: decode layer difference: 17.0\n",
      "autoencoder training epoch 21: decode layer difference: 23.0\n",
      "autoencoder training epoch 22: decode layer difference: 14.0\n",
      "autoencoder training epoch 23: decode layer difference: 10.0\n",
      "autoencoder training epoch 24: decode layer difference: 13.0\n",
      "autoencoder training epoch 25: decode layer difference: 8.0\n",
      "autoencoder training epoch 26: decode layer difference: 8.0\n",
      "autoencoder training epoch 27: decode layer difference: 5.0\n",
      "autoencoder training epoch 28: decode layer difference: 6.0\n",
      "autoencoder training epoch 29: decode layer difference: 10.0\n",
      "autoencoder training epoch 30: decode layer difference: 8.0\n",
      "autoencoder training epoch 31: decode layer difference: 7.0\n",
      "autoencoder training epoch 32: decode layer difference: 8.0\n",
      "autoencoder training epoch 33: decode layer difference: 10.0\n",
      "autoencoder training epoch 34: decode layer difference: 10.0\n",
      "autoencoder training epoch 35: decode layer difference: 13.0\n",
      "autoencoder training epoch 36: decode layer difference: 9.0\n",
      "autoencoder training epoch 37: decode layer difference: 7.0\n",
      "autoencoder training epoch 38: decode layer difference: 5.0\n",
      "autoencoder training epoch 39: decode layer difference: 6.0\n",
      "autoencoder training epoch 40: decode layer difference: 6.0\n",
      "autoencoder training epoch 41: decode layer difference: 5.0\n",
      "autoencoder training epoch 42: decode layer difference: 8.0\n",
      "autoencoder training epoch 43: decode layer difference: 4.0\n",
      "autoencoder training epoch 44: decode layer difference: 4.0\n",
      "autoencoder training epoch 45: decode layer difference: 4.0\n",
      "autoencoder training epoch 46: decode layer difference: 5.0\n",
      "autoencoder training epoch 47: decode layer difference: 3.0\n",
      "autoencoder training epoch 48: decode layer difference: 6.0\n",
      "autoencoder training epoch 49: decode layer difference: 1.0\n",
      "autoencoder training epoch 50: decode layer difference: 5.0\n",
      "autoencoder training epoch 51: decode layer difference: 2.0\n",
      "autoencoder training epoch 52: decode layer difference: 2.0\n",
      "autoencoder training epoch 53: decode layer difference: 3.0\n",
      "autoencoder training epoch 54: decode layer difference: 1.0\n",
      "autoencoder training epoch 55: decode layer difference: 1.0\n",
      "autoencoder training epoch 56: decode layer difference: 0.0\n",
      "training epoch 1: error: 76.177847147674 classification score: 0.9214285714285714\n",
      "training epoch 2: error: 30.46752397489869 classification score: 0.9714285714285714\n",
      "training epoch 3: error: 18.177315035795036 classification score: 0.9857142857142858\n",
      "training epoch 4: error: 17.98459305767569 classification score: 0.9821428571428571\n",
      "training epoch 5: error: 10.709854985835257 classification score: 0.9928571428571429\n",
      "training epoch 6: error: 12.673787533345955 classification score: 0.9892857142857143\n",
      "training epoch 7: error: 13.668496272004816 classification score: 0.9857142857142858\n",
      "training epoch 8: error: 7.561657714542121 classification score: 0.9964285714285714\n",
      "training epoch 9: error: 11.561890203304095 classification score: 0.9928571428571429\n",
      "training epoch 10: error: 7.959163253761977 classification score: 0.9964285714285714\n",
      "training epoch 11: error: 32.64572534114352 classification score: 0.9642857142857143\n",
      "training epoch 12: error: 8.66076872955689 classification score: 0.9964285714285714\n",
      "training epoch 13: error: 12.293411072243956 classification score: 0.9892857142857143\n",
      "training epoch 14: error: 3.080556193830315 classification score: 1.0\n",
      "\n",
      "fold 5: error: 19.272162797200902 classification score: 0.9411764705882353\n",
      "\n",
      "average error: 18.21133508459686 average classification score 0.9568067226890756\n"
     ]
    },
    {
     "data": {
      "text/plain": "18.21133508459686"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_layered(house_votes_data, 'class', train_autoencoder_network, (25, 60))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1: error: 242.29412467476527 classification score: 0.6151079136690647\n",
      "training epoch 2: error: 177.6087446274163 classification score: 0.6151079136690647\n",
      "training epoch 3: error: 159.01781518071243 classification score: 0.6151079136690647\n",
      "training epoch 4: error: 150.8304758272887 classification score: 0.6151079136690647\n",
      "training epoch 5: error: 145.52910040661266 classification score: 0.6151079136690647\n",
      "training epoch 6: error: 140.85841251551966 classification score: 0.6151079136690647\n",
      "training epoch 7: error: 137.9660638120817 classification score: 0.6151079136690647\n",
      "training epoch 8: error: 134.23888181729305 classification score: 0.6151079136690647\n",
      "training epoch 9: error: 118.61940203618533 classification score: 0.9496402877697842\n",
      "training epoch 10: error: 41.16177294659475 classification score: 0.9712230215827338\n",
      "training epoch 11: error: 36.91954809374689 classification score: 0.9676258992805755\n",
      "training epoch 12: error: 27.41383266763042 classification score: 0.9712230215827338\n",
      "training epoch 13: error: 24.882158434063232 classification score: 0.9748201438848921\n",
      "training epoch 14: error: 24.092357235914292 classification score: 0.9748201438848921\n",
      "training epoch 15: error: 35.30493740839702 classification score: 0.9784172661870504\n",
      "training epoch 16: error: 26.644088573498575 classification score: 0.9784172661870504\n",
      "training epoch 17: error: 19.00731781878801 classification score: 0.9784172661870504\n",
      "training epoch 18: error: 20.528005053291395 classification score: 0.9784172661870504\n",
      "training epoch 19: error: 25.86779372450198 classification score: 0.9784172661870504\n",
      "training epoch 20: error: 18.37264592644485 classification score: 0.9820143884892086\n",
      "\n",
      "fold 1: error: 11.859266399314535 classification score: 0.9714285714285714\n",
      "\n",
      "training epoch 1: error: 239.64269928051183 classification score: 0.6151079136690647\n",
      "training epoch 2: error: 178.98713234166843 classification score: 0.6151079136690647\n",
      "training epoch 3: error: 160.66078795791177 classification score: 0.6151079136690647\n",
      "training epoch 4: error: 151.4617835104145 classification score: 0.6151079136690647\n",
      "training epoch 5: error: 146.19174171063477 classification score: 0.6151079136690647\n",
      "training epoch 6: error: 142.36429946428396 classification score: 0.6151079136690647\n",
      "training epoch 7: error: 138.1823186114501 classification score: 0.6151079136690647\n",
      "training epoch 8: error: 135.5338808131151 classification score: 0.6151079136690647\n",
      "training epoch 9: error: 132.93576162752612 classification score: 0.6151079136690647\n",
      "training epoch 10: error: 83.835981284116 classification score: 0.9496402877697842\n",
      "training epoch 11: error: 42.603907337515075 classification score: 0.9568345323741008\n",
      "training epoch 12: error: 33.04733232589794 classification score: 0.9640287769784173\n",
      "training epoch 13: error: 29.339842897182532 classification score: 0.9712230215827338\n",
      "training epoch 14: error: 24.117092834169416 classification score: 0.9712230215827338\n",
      "training epoch 15: error: 21.330962084902666 classification score: 0.9892086330935251\n",
      "training epoch 16: error: 25.001173950064803 classification score: 0.9856115107913669\n",
      "training epoch 17: error: 28.954186142493455 classification score: 0.9676258992805755\n",
      "training epoch 18: error: 15.909904036154956 classification score: 0.9856115107913669\n",
      "training epoch 19: error: 20.96153685923263 classification score: 0.9820143884892086\n",
      "training epoch 20: error: 15.55426226270609 classification score: 0.9820143884892086\n",
      "\n",
      "fold 2: error: 16.41259112961201 classification score: 0.9428571428571428\n",
      "\n",
      "training epoch 1: error: 244.3721723637864 classification score: 0.6151079136690647\n",
      "training epoch 2: error: 186.86652976449997 classification score: 0.6151079136690647\n",
      "training epoch 3: error: 167.6139814965153 classification score: 0.6151079136690647\n",
      "training epoch 4: error: 157.86515108320506 classification score: 0.6151079136690647\n",
      "training epoch 5: error: 152.48323770863252 classification score: 0.6151079136690647\n",
      "training epoch 6: error: 148.57241461013234 classification score: 0.6151079136690647\n",
      "training epoch 7: error: 145.7515145655901 classification score: 0.6151079136690647\n",
      "training epoch 8: error: 142.1148165272992 classification score: 0.6151079136690647\n",
      "training epoch 9: error: 139.21934970603223 classification score: 0.6151079136690647\n",
      "training epoch 10: error: 136.91090181184043 classification score: 0.6151079136690647\n",
      "training epoch 11: error: 134.30782605978965 classification score: 0.6151079136690647\n",
      "training epoch 12: error: 131.61787227566285 classification score: 0.6151079136690647\n",
      "training epoch 13: error: 45.33635403114038 classification score: 0.960431654676259\n",
      "training epoch 14: error: 35.6850602835404 classification score: 0.9676258992805755\n",
      "training epoch 15: error: 32.3172052264348 classification score: 0.9712230215827338\n",
      "training epoch 16: error: 34.02343152657034 classification score: 0.9640287769784173\n",
      "training epoch 17: error: 27.076633791327833 classification score: 0.9712230215827338\n",
      "training epoch 18: error: 34.914911399611306 classification score: 0.9640287769784173\n",
      "training epoch 19: error: 28.816925056827557 classification score: 0.9676258992805755\n",
      "training epoch 20: error: 30.58643471408034 classification score: 0.960431654676259\n",
      "\n",
      "fold 3: error: 1.4900636107510965 classification score: 1.0\n",
      "\n",
      "training epoch 1: error: 244.03234237122865 classification score: 0.6151079136690647\n",
      "training epoch 2: error: 180.81681413827508 classification score: 0.6151079136690647\n",
      "training epoch 3: error: 159.99094506908722 classification score: 0.6151079136690647\n",
      "training epoch 4: error: 150.7990482188143 classification score: 0.6151079136690647\n",
      "training epoch 5: error: 145.24083663489225 classification score: 0.6151079136690647\n",
      "training epoch 6: error: 140.84393137291795 classification score: 0.6151079136690647\n",
      "training epoch 7: error: 137.2349987597075 classification score: 0.6151079136690647\n",
      "training epoch 8: error: 134.4071723133187 classification score: 0.6151079136690647\n",
      "training epoch 9: error: 132.0066344884632 classification score: 0.6151079136690647\n",
      "training epoch 10: error: 129.96550032011277 classification score: 0.6151079136690647\n",
      "training epoch 11: error: 127.8733079999243 classification score: 0.6151079136690647\n",
      "training epoch 12: error: 55.922710880825406 classification score: 0.9496402877697842\n",
      "training epoch 13: error: 30.458229220069263 classification score: 0.9676258992805755\n",
      "training epoch 14: error: 24.036868657890057 classification score: 0.9748201438848921\n",
      "training epoch 15: error: 21.955276516103755 classification score: 0.9748201438848921\n",
      "training epoch 16: error: 20.812608135027233 classification score: 0.9856115107913669\n",
      "training epoch 17: error: 19.404189350932537 classification score: 0.9856115107913669\n",
      "training epoch 18: error: 28.396551266114834 classification score: 0.9748201438848921\n",
      "training epoch 19: error: 42.83000951816821 classification score: 0.9568345323741008\n",
      "training epoch 20: error: 30.36054401705436 classification score: 0.9784172661870504\n",
      "\n",
      "fold 4: error: 24.59687314285685 classification score: 0.9428571428571428\n",
      "\n",
      "training epoch 1: error: 252.65454053649063 classification score: 0.6142857142857143\n",
      "training epoch 2: error: 185.82571332322087 classification score: 0.6142857142857143\n",
      "training epoch 3: error: 163.42061237069484 classification score: 0.6142857142857143\n",
      "training epoch 4: error: 155.1059140149108 classification score: 0.6142857142857143\n",
      "training epoch 5: error: 153.80809399239135 classification score: 0.6142857142857143\n",
      "training epoch 6: error: 110.55669179385659 classification score: 0.925\n",
      "training epoch 7: error: 56.11579747827639 classification score: 0.9357142857142857\n",
      "training epoch 8: error: 33.87963390720415 classification score: 0.9607142857142857\n",
      "training epoch 9: error: 36.983199754354104 classification score: 0.9607142857142857\n",
      "training epoch 10: error: 25.133628106889745 classification score: 0.9714285714285714\n",
      "training epoch 11: error: 20.644599653598494 classification score: 0.975\n",
      "training epoch 12: error: 26.897306987900283 classification score: 0.9785714285714285\n",
      "training epoch 13: error: 17.9993561940046 classification score: 0.9857142857142858\n",
      "training epoch 14: error: 15.534544848653747 classification score: 0.9892857142857143\n",
      "training epoch 15: error: 12.96510106379273 classification score: 0.9964285714285714\n",
      "training epoch 16: error: 24.253455449553194 classification score: 0.975\n",
      "training epoch 17: error: 12.45776464656219 classification score: 0.9928571428571429\n",
      "training epoch 18: error: 12.604508105217153 classification score: 0.9857142857142858\n",
      "training epoch 19: error: 12.62142768132071 classification score: 0.9892857142857143\n",
      "training epoch 20: error: 18.842966193360706 classification score: 0.9857142857142858\n",
      "\n",
      "fold 5: error: 22.194645645392853 classification score: 0.9411764705882353\n",
      "\n",
      "average error: 15.310687985585469 average classification score 0.9596638655462184\n"
     ]
    },
    {
     "data": {
      "text/plain": "15.310687985585469"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_layered(house_votes_data, 'class', train_layered_classification_network, (25, 60))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1: error: 28.60425237116041 classification score: 0.9712230215827338\n",
      "training epoch 2: error: 33.21168037542073 classification score: 0.9748201438848921\n",
      "training epoch 3: error: 19.435785753014017 classification score: 0.9856115107913669\n",
      "training epoch 4: error: 20.714818130797223 classification score: 0.9784172661870504\n",
      "training epoch 5: error: 11.671033561285565 classification score: 0.9892086330935251\n",
      "training epoch 6: error: 11.58465704792062 classification score: 0.9892086330935251\n",
      "training epoch 7: error: 10.489830216806322 classification score: 0.9892086330935251\n",
      "training epoch 8: error: 16.600191295145933 classification score: 0.9820143884892086\n",
      "training epoch 9: error: 21.008787529845545 classification score: 0.9820143884892086\n",
      "training epoch 10: error: 7.858500891861051 classification score: 0.9928057553956835\n",
      "training epoch 11: error: 14.265614253521983 classification score: 0.9856115107913669\n",
      "training epoch 12: error: 8.071801564542842 classification score: 0.9928057553956835\n",
      "training epoch 13: error: 7.0333316434063775 classification score: 0.9928057553956835\n",
      "training epoch 14: error: 7.727698672564327 classification score: 0.9928057553956835\n",
      "training epoch 15: error: 13.67326639582392 classification score: 0.9856115107913669\n",
      "training epoch 16: error: 15.475514043536926 classification score: 0.9892086330935251\n",
      "training epoch 17: error: 7.132553139813544 classification score: 0.9928057553956835\n",
      "training epoch 18: error: 6.739652356935689 classification score: 0.9928057553956835\n",
      "training epoch 19: error: 10.17474576050921 classification score: 0.9928057553956835\n",
      "training epoch 20: error: 6.6668149776955525 classification score: 0.9964028776978417\n",
      "\n",
      "fold 1: error: 8.258975255730313 classification score: 0.9857142857142858\n",
      "\n",
      "training epoch 1: error: 19.683319967639928 classification score: 0.9856115107913669\n",
      "training epoch 2: error: 18.43643596967025 classification score: 0.9856115107913669\n",
      "training epoch 3: error: 23.9380798568684 classification score: 0.9820143884892086\n",
      "training epoch 4: error: 13.146264092401998 classification score: 0.9928057553956835\n",
      "training epoch 5: error: 9.572811717017004 classification score: 0.9928057553956835\n",
      "training epoch 6: error: 8.806753072725204 classification score: 0.9964028776978417\n",
      "training epoch 7: error: 7.790278148769351 classification score: 0.9928057553956835\n",
      "training epoch 8: error: 7.302402475827413 classification score: 0.9928057553956835\n",
      "training epoch 9: error: 12.114168747259734 classification score: 0.9892086330935251\n",
      "training epoch 10: error: 7.983581956169603 classification score: 0.9928057553956835\n",
      "training epoch 11: error: 6.21633310506508 classification score: 0.9964028776978417\n",
      "training epoch 12: error: 9.724705971078103 classification score: 0.9892086330935251\n",
      "training epoch 13: error: 6.191227842572752 classification score: 0.9964028776978417\n",
      "training epoch 14: error: 6.052603721535162 classification score: 0.9964028776978417\n",
      "training epoch 15: error: 7.166192600955058 classification score: 0.9964028776978417\n",
      "training epoch 16: error: 5.374939848081195 classification score: 0.9964028776978417\n",
      "training epoch 17: error: 5.079884831320514 classification score: 0.9964028776978417\n",
      "training epoch 18: error: 8.367100029690858 classification score: 0.9892086330935251\n",
      "training epoch 19: error: 8.593879736169669 classification score: 0.9892086330935251\n",
      "training epoch 20: error: 7.663761513267665 classification score: 0.9928057553956835\n",
      "\n",
      "fold 2: error: 19.21421109202067 classification score: 0.9714285714285714\n",
      "\n",
      "training epoch 1: error: 23.529453154414682 classification score: 0.9784172661870504\n",
      "training epoch 2: error: 22.098210778589937 classification score: 0.9820143884892086\n",
      "training epoch 3: error: 16.89155303989923 classification score: 0.9856115107913669\n",
      "training epoch 4: error: 18.58890366046715 classification score: 0.9784172661870504\n",
      "training epoch 5: error: 11.080968943688466 classification score: 0.9892086330935251\n",
      "training epoch 6: error: 10.220427613622578 classification score: 0.9892086330935251\n",
      "training epoch 7: error: 10.970795371298749 classification score: 0.9964028776978417\n",
      "training epoch 8: error: 9.675945369587009 classification score: 0.9892086330935251\n",
      "training epoch 9: error: 7.251825749123438 classification score: 1.0\n",
      "\n",
      "fold 3: error: 12.122273125250311 classification score: 0.9714285714285714\n",
      "\n",
      "training epoch 1: error: 32.60730847977496 classification score: 0.9748201438848921\n",
      "training epoch 2: error: 19.308564680507466 classification score: 0.9820143884892086\n",
      "training epoch 3: error: 14.825881120800592 classification score: 0.9820143884892086\n",
      "training epoch 4: error: 33.02787198147607 classification score: 0.9784172661870504\n",
      "training epoch 5: error: 14.023708839918134 classification score: 0.9856115107913669\n",
      "training epoch 6: error: 11.170819723585904 classification score: 0.9964028776978417\n",
      "training epoch 7: error: 14.545888459291715 classification score: 0.9856115107913669\n",
      "training epoch 8: error: 8.114224475207527 classification score: 0.9856115107913669\n",
      "training epoch 9: error: 9.005133235537262 classification score: 0.9892086330935251\n",
      "training epoch 10: error: 7.426525205381696 classification score: 0.9964028776978417\n",
      "training epoch 11: error: 5.628635333916567 classification score: 0.9964028776978417\n",
      "training epoch 12: error: 5.848098475295099 classification score: 0.9964028776978417\n",
      "training epoch 13: error: 5.2282464475604655 classification score: 1.0\n",
      "\n",
      "fold 4: error: 6.547594283824317 classification score: 0.9857142857142858\n",
      "\n",
      "training epoch 1: error: 19.572416671236283 classification score: 0.9892857142857143\n",
      "training epoch 2: error: 17.54244368210641 classification score: 0.9892857142857143\n",
      "training epoch 3: error: 15.899890203665183 classification score: 0.9857142857142858\n",
      "training epoch 4: error: 11.834612540252241 classification score: 0.9892857142857143\n",
      "training epoch 5: error: 10.93586240253533 classification score: 0.9892857142857143\n",
      "training epoch 6: error: 7.504049318151078 classification score: 0.9928571428571429\n",
      "training epoch 7: error: 7.162116754342678 classification score: 0.9892857142857143\n",
      "training epoch 8: error: 9.523068698554068 classification score: 0.9928571428571429\n",
      "training epoch 9: error: 6.759016363008978 classification score: 0.9964285714285714\n",
      "training epoch 10: error: 8.281482805224401 classification score: 0.9892857142857143\n",
      "training epoch 11: error: 7.028245559382481 classification score: 0.9928571428571429\n",
      "training epoch 12: error: 3.49846693379273 classification score: 1.0\n",
      "\n",
      "fold 5: error: 33.24516919627045 classification score: 0.9264705882352942\n",
      "\n",
      "average error: 15.877644590619212 average classification score 0.9681512605042017\n"
     ]
    },
    {
     "data": {
      "text/plain": "15.877644590619212"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation(house_votes_data, 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_data = load_machine('datasets/machine.data')\n",
    "machine_data.name = 'machine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "         MYCT      MMIN      MMAX   CACH     CHMIN     CHMAX       PRP\n0    0.072825  0.006012  0.092843  1.000  0.307692  0.727273  0.167832\n1    0.008092  0.248497  0.499499  0.125  0.153846  0.181818  0.229895\n2    0.008092  0.248497  0.499499  0.125  0.153846  0.181818  0.187063\n3    0.008092  0.248497  0.499499  0.125  0.153846  0.181818  0.145105\n4    0.008092  0.248497  0.249249  0.125  0.153846  0.090909  0.110140\n..        ...       ...       ...    ...       ...       ...       ...\n204  0.072151  0.029309  0.124124  0.000  0.019231  0.045455  0.031469\n205  0.054619  0.029309  0.124124  0.125  0.038462  0.045455  0.034965\n206  0.072825  0.060621  0.124124  0.000  0.038462  0.079545  0.040210\n207  0.312205  0.014028  0.124124  0.125  0.000000  0.000000  0.053322\n208  0.312205  0.029309  0.061562  0.000  0.000000  0.000000  0.034091\n\n[209 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MYCT</th>\n      <th>MMIN</th>\n      <th>MMAX</th>\n      <th>CACH</th>\n      <th>CHMIN</th>\n      <th>CHMAX</th>\n      <th>PRP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.072825</td>\n      <td>0.006012</td>\n      <td>0.092843</td>\n      <td>1.000</td>\n      <td>0.307692</td>\n      <td>0.727273</td>\n      <td>0.167832</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.008092</td>\n      <td>0.248497</td>\n      <td>0.499499</td>\n      <td>0.125</td>\n      <td>0.153846</td>\n      <td>0.181818</td>\n      <td>0.229895</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.008092</td>\n      <td>0.248497</td>\n      <td>0.499499</td>\n      <td>0.125</td>\n      <td>0.153846</td>\n      <td>0.181818</td>\n      <td>0.187063</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.008092</td>\n      <td>0.248497</td>\n      <td>0.499499</td>\n      <td>0.125</td>\n      <td>0.153846</td>\n      <td>0.181818</td>\n      <td>0.145105</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.008092</td>\n      <td>0.248497</td>\n      <td>0.249249</td>\n      <td>0.125</td>\n      <td>0.153846</td>\n      <td>0.090909</td>\n      <td>0.110140</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>0.072151</td>\n      <td>0.029309</td>\n      <td>0.124124</td>\n      <td>0.000</td>\n      <td>0.019231</td>\n      <td>0.045455</td>\n      <td>0.031469</td>\n    </tr>\n    <tr>\n      <th>205</th>\n      <td>0.054619</td>\n      <td>0.029309</td>\n      <td>0.124124</td>\n      <td>0.125</td>\n      <td>0.038462</td>\n      <td>0.045455</td>\n      <td>0.034965</td>\n    </tr>\n    <tr>\n      <th>206</th>\n      <td>0.072825</td>\n      <td>0.060621</td>\n      <td>0.124124</td>\n      <td>0.000</td>\n      <td>0.038462</td>\n      <td>0.079545</td>\n      <td>0.040210</td>\n    </tr>\n    <tr>\n      <th>207</th>\n      <td>0.312205</td>\n      <td>0.014028</td>\n      <td>0.124124</td>\n      <td>0.125</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.053322</td>\n    </tr>\n    <tr>\n      <th>208</th>\n      <td>0.312205</td>\n      <td>0.029309</td>\n      <td>0.061562</td>\n      <td>0.000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.034091</td>\n    </tr>\n  </tbody>\n</table>\n<p>209 rows  7 columns</p>\n</div>"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoencoder training epoch 1: decode layer difference: 109.66912667871297\n",
      "autoencoder training epoch 2: decode layer difference: 109.66912667871297\n",
      "autoencoder training epoch 3: decode layer difference: 109.66912667871297\n",
      "autoencoder training epoch 4: decode layer difference: 109.66912667871297\n",
      "autoencoder training epoch 5: decode layer difference: 109.66912667871297\n",
      "autoencoder training epoch 6: decode layer difference: 109.66912667871297\n",
      "autoencoder training epoch 7: decode layer difference: 109.66912667871297\n",
      "autoencoder training epoch 8: decode layer difference: 109.66912667871297\n",
      "autoencoder training epoch 9: decode layer difference: 109.66912667871297\n",
      "autoencoder training epoch 10: decode layer difference: 109.66912667871297\n",
      "autoencoder training epoch 11: decode layer difference: 109.66912667871297\n",
      "autoencoder training epoch 12: decode layer difference: 109.66912667871297\n",
      "autoencoder training epoch 13: decode layer difference: 109.66912667871297\n",
      "autoencoder training epoch 14: decode layer difference: 109.66912667871297\n",
      "autoencoder training epoch 15: decode layer difference: 109.66912667871297\n",
      "autoencoder training epoch 16: decode layer difference: 109.66912667871297\n",
      "autoencoder training epoch 17: decode layer difference: 109.66912667871297\n",
      "autoencoder training epoch 18: decode layer difference: 109.66912667871297\n",
      "autoencoder training epoch 19: decode layer difference: 109.66912667871297\n",
      "autoencoder training epoch 20: decode layer difference: 109.66912667871297\n",
      "autoencoder training epoch 21: decode layer difference: 109.66912667871297\n",
      "autoencoder training epoch 22: decode layer difference: 108.66912667871297\n",
      "autoencoder training epoch 23: decode layer difference: 105.67212968171597\n",
      "autoencoder training epoch 24: decode layer difference: 105.67212968171597\n",
      "autoencoder training epoch 25: decode layer difference: 105.56834113254644\n",
      "autoencoder training epoch 26: decode layer difference: 105.56934213354745\n",
      "autoencoder training epoch 27: decode layer difference: 105.3785127336823\n",
      "autoencoder training epoch 28: decode layer difference: 105.18868433481816\n",
      "autoencoder training epoch 29: decode layer difference: 105.13271670164218\n",
      "autoencoder training epoch 30: decode layer difference: 105.13271670164218\n",
      "autoencoder training epoch 31: decode layer difference: 105.38396795289341\n",
      "autoencoder training epoch 32: decode layer difference: 105.10412978701345\n",
      "autoencoder training epoch 33: decode layer difference: 105.35412978701345\n",
      "autoencoder training epoch 34: decode layer difference: 105.35412978701345\n",
      "autoencoder training epoch 35: decode layer difference: 105.35412978701345\n",
      "autoencoder training epoch 36: decode layer difference: 105.35412978701345\n",
      "autoencoder training epoch 37: decode layer difference: 105.10287853576219\n",
      "autoencoder training epoch 38: decode layer difference: 105.35412978701345\n",
      "autoencoder training epoch 39: decode layer difference: 105.35412978701345\n",
      "autoencoder training epoch 40: decode layer difference: 105.10287853576219\n",
      "autoencoder training epoch 41: decode layer difference: 104.22787853576219\n",
      "autoencoder training epoch 42: decode layer difference: 104.35412978701345\n",
      "autoencoder training epoch 43: decode layer difference: 105.10287853576219\n",
      "autoencoder training epoch 44: decode layer difference: 105.10287853576219\n",
      "autoencoder training epoch 45: decode layer difference: 105.10287853576219\n",
      "autoencoder training epoch 46: decode layer difference: 105.10287853576219\n",
      "autoencoder training epoch 47: decode layer difference: 105.10287853576219\n",
      "autoencoder training epoch 48: decode layer difference: 105.10287853576219\n",
      "autoencoder training epoch 49: decode layer difference: 105.1038795367632\n",
      "autoencoder training epoch 50: decode layer difference: 105.10287853576219\n",
      "autoencoder training epoch 51: decode layer difference: 105.10287853576219\n",
      "autoencoder training epoch 52: decode layer difference: 105.10287853576219\n",
      "autoencoder training epoch 53: decode layer difference: 104.60588454778625\n",
      "autoencoder training epoch 54: decode layer difference: 103.79157023347193\n",
      "autoencoder training epoch 55: decode layer difference: 103.79257123447293\n",
      "autoencoder training epoch 56: decode layer difference: 104.29457624549597\n",
      "autoencoder training epoch 57: decode layer difference: 103.79157023347193\n",
      "autoencoder training epoch 58: decode layer difference: 104.29457624549597\n",
      "autoencoder training epoch 59: decode layer difference: 104.29557724649698\n",
      "autoencoder training epoch 60: decode layer difference: 103.29557724649698\n",
      "autoencoder training epoch 61: decode layer difference: 103.29457624549597\n",
      "autoencoder training epoch 62: decode layer difference: 103.29557724649698\n",
      "autoencoder training epoch 63: decode layer difference: 102.04357524449497\n",
      "autoencoder training epoch 64: decode layer difference: 103.04357524449497\n",
      "autoencoder training epoch 65: decode layer difference: 102.04357524449497\n",
      "autoencoder training epoch 66: decode layer difference: 100.04457624549597\n",
      "autoencoder training epoch 67: decode layer difference: 100.04757924849898\n",
      "autoencoder training epoch 68: decode layer difference: 100.050582251502\n",
      "autoencoder training epoch 69: decode layer difference: 100.04757924849898\n",
      "autoencoder training epoch 70: decode layer difference: 100.04757924849898\n",
      "autoencoder training epoch 71: decode layer difference: 100.04757924849898\n",
      "autoencoder training epoch 72: decode layer difference: 99.79757924849898\n",
      "autoencoder training epoch 73: decode layer difference: 99.79757924849898\n",
      "autoencoder training epoch 74: decode layer difference: 99.79757924849898\n",
      "autoencoder training epoch 75: decode layer difference: 100.07030652122624\n",
      "autoencoder training epoch 76: decode layer difference: 99.800582251502\n",
      "autoencoder training epoch 77: decode layer difference: 100.07030652122624\n",
      "autoencoder training epoch 78: decode layer difference: 99.79757924849898\n",
      "autoencoder training epoch 79: decode layer difference: 100.07431052523026\n",
      "autoencoder training epoch 80: decode layer difference: 100.02643452422924\n",
      "autoencoder training epoch 81: decode layer difference: 100.02743552523026\n",
      "autoencoder training epoch 82: decode layer difference: 100.56589706369179\n",
      "autoencoder training epoch 83: decode layer difference: 100.02743552523026\n",
      "autoencoder training epoch 84: decode layer difference: 100.02343152122624\n",
      "autoencoder training epoch 85: decode layer difference: 100.02343152122624\n",
      "autoencoder training epoch 86: decode layer difference: 100.02343152122624\n",
      "autoencoder training epoch 87: decode layer difference: 100.02343152122624\n",
      "autoencoder training epoch 88: decode layer difference: 100.02743552523026\n",
      "autoencoder training epoch 89: decode layer difference: 100.02343152122624\n",
      "autoencoder training epoch 90: decode layer difference: 100.02343152122624\n",
      "autoencoder training epoch 91: decode layer difference: 100.02343152122624\n",
      "autoencoder training epoch 92: decode layer difference: 100.02343152122624\n",
      "autoencoder training epoch 93: decode layer difference: 100.02343152122624\n",
      "autoencoder training epoch 94: decode layer difference: 100.02743552523026\n",
      "autoencoder training epoch 95: decode layer difference: 100.02743552523026\n",
      "autoencoder training epoch 96: decode layer difference: 100.02343152122624\n",
      "autoencoder training epoch 97: decode layer difference: 100.02343152122624\n",
      "autoencoder training epoch 98: decode layer difference: 100.02343152122624\n",
      "autoencoder training epoch 99: decode layer difference: 100.02343152122624\n",
      "autoencoder training epoch 100: decode layer difference: 100.02743552523026\n",
      "autoencoder training epoch 101: decode layer difference: 100.02743552523026\n",
      "autoencoder training epoch 102: decode layer difference: 100.02343152122624\n",
      "autoencoder training epoch 103: decode layer difference: 100.02643452422924\n",
      "autoencoder training epoch 104: decode layer difference: 100.02343152122624\n",
      "autoencoder training epoch 105: decode layer difference: 100.02343152122624\n",
      "autoencoder training epoch 106: decode layer difference: 100.02743552523026\n",
      "autoencoder training epoch 107: decode layer difference: 100.02343152122624\n",
      "autoencoder training epoch 108: decode layer difference: 100.02343152122624\n",
      "autoencoder training epoch 109: decode layer difference: 100.02743552523026\n",
      "autoencoder training epoch 110: decode layer difference: 100.02743552523026\n",
      "autoencoder training epoch 111: decode layer difference: 100.02343152122624\n",
      "autoencoder training epoch 112: decode layer difference: 100.02343152122624\n",
      "autoencoder training epoch 113: decode layer difference: 100.02743552523026\n",
      "autoencoder training epoch 114: decode layer difference: 100.02443252222724\n",
      "autoencoder training epoch 115: decode layer difference: 100.02343152122624\n",
      "autoencoder training epoch 116: decode layer difference: 100.02743552523026\n",
      "autoencoder training epoch 117: decode layer difference: 100.02743552523026\n",
      "autoencoder training epoch 118: decode layer difference: 100.02343152122624\n",
      "autoencoder training epoch 119: decode layer difference: 100.02743552523026\n",
      "autoencoder training epoch 120: decode layer difference: 100.02443252222724\n",
      "autoencoder training epoch 121: decode layer difference: 100.02743552523026\n",
      "autoencoder training epoch 122: decode layer difference: 100.02443252222724\n",
      "autoencoder training epoch 123: decode layer difference: 100.02743552523026\n",
      "autoencoder training epoch 124: decode layer difference: 100.02743552523026\n",
      "autoencoder training epoch 125: decode layer difference: 100.02443252222724\n",
      "autoencoder training epoch 126: decode layer difference: 100.02743552523026\n",
      "autoencoder training epoch 127: decode layer difference: 100.02743552523026\n",
      "autoencoder training epoch 128: decode layer difference: 99.84561734341207\n",
      "autoencoder training epoch 129: decode layer difference: 99.66379916159389\n",
      "autoencoder training epoch 130: decode layer difference: 99.43302993082466\n",
      "autoencoder training epoch 131: decode layer difference: 99.16079314956984\n",
      "autoencoder training epoch 132: decode layer difference: 99.16079314956984\n",
      "autoencoder training epoch 133: decode layer difference: 98.93002391880061\n",
      "autoencoder training epoch 134: decode layer difference: 99.46848545726215\n",
      "autoencoder training epoch 135: decode layer difference: 97.97247245223112\n",
      "autoencoder training epoch 136: decode layer difference: 97.97247245223112\n",
      "autoencoder training epoch 137: decode layer difference: 98.51293799870868\n",
      "autoencoder training epoch 138: decode layer difference: 98.51293799870868\n",
      "autoencoder training epoch 139: decode layer difference: 98.51293799870868\n",
      "autoencoder training epoch 140: decode layer difference: 98.51093399069265\n",
      "autoencoder training epoch 141: decode layer difference: 99.05139953717023\n",
      "autoencoder training epoch 142: decode layer difference: 99.05039853616921\n",
      "autoencoder training epoch 143: decode layer difference: 99.05139953717023\n",
      "autoencoder training epoch 144: decode layer difference: 99.05139953717023\n",
      "autoencoder training epoch 145: decode layer difference: 99.05039853616921\n",
      "autoencoder training epoch 146: decode layer difference: 99.05139953717023\n",
      "autoencoder training epoch 147: decode layer difference: 99.05039853616921\n",
      "autoencoder training epoch 148: decode layer difference: 99.05240254418526\n",
      "autoencoder training epoch 149: decode layer difference: 99.05039853616921\n",
      "autoencoder training epoch 150: decode layer difference: 98.77667026244094\n",
      "autoencoder training epoch 151: decode layer difference: 99.04939753516823\n",
      "autoencoder training epoch 152: decode layer difference: 99.05240254418526\n",
      "autoencoder training epoch 153: decode layer difference: 98.77767126344196\n",
      "autoencoder training epoch 154: decode layer difference: 98.77867427045699\n",
      "autoencoder training epoch 155: decode layer difference: 98.77667026244094\n",
      "autoencoder training epoch 156: decode layer difference: 99.46149345327618\n",
      "autoencoder training epoch 157: decode layer difference: 98.77767126344196\n",
      "autoencoder training epoch 158: decode layer difference: 98.77867427045699\n",
      "autoencoder training epoch 159: decode layer difference: 98.45949145127418\n",
      "autoencoder training epoch 160: decode layer difference: 97.77967527145799\n",
      "autoencoder training epoch 161: decode layer difference: 98.1887661805489\n",
      "autoencoder training epoch 162: decode layer difference: 98.1877651795479\n",
      "autoencoder training epoch 163: decode layer difference: 98.1887661805489\n",
      "autoencoder training epoch 164: decode layer difference: 97.18476017053085\n",
      "autoencoder training epoch 165: decode layer difference: 97.18776517954788\n",
      "autoencoder training epoch 166: decode layer difference: 97.1887661805489\n",
      "autoencoder training epoch 167: decode layer difference: 98.18676217253287\n",
      "autoencoder training epoch 168: decode layer difference: 98.18676217253287\n",
      "autoencoder training epoch 169: decode layer difference: 97.18676217253285\n",
      "autoencoder training epoch 170: decode layer difference: 97.1887661805489\n",
      "autoencoder training epoch 171: decode layer difference: 97.1887661805489\n",
      "autoencoder training epoch 172: decode layer difference: 97.18776517954788\n",
      "autoencoder training epoch 173: decode layer difference: 97.1887661805489\n",
      "autoencoder training epoch 174: decode layer difference: 97.7322167179874\n",
      "autoencoder training epoch 175: decode layer difference: 97.18576117153185\n",
      "autoencoder training epoch 176: decode layer difference: 97.18776517954788\n",
      "autoencoder training epoch 177: decode layer difference: 97.18576117153185\n",
      "autoencoder training epoch 178: decode layer difference: 96.73422072600343\n",
      "autoencoder training epoch 179: decode layer difference: 96.73321972500243\n",
      "autoencoder training epoch 180: decode layer difference: 96.73321972500243\n",
      "autoencoder training epoch 181: decode layer difference: 97.7312157169864\n",
      "autoencoder training epoch 182: decode layer difference: 96.73422072600343\n",
      "autoencoder training epoch 183: decode layer difference: 96.73422072600343\n",
      "autoencoder training epoch 184: decode layer difference: 96.73321972500243\n",
      "autoencoder training epoch 185: decode layer difference: 96.73422072600343\n",
      "autoencoder training epoch 186: decode layer difference: 96.73422072600343\n",
      "autoencoder training epoch 187: decode layer difference: 96.7312157169864\n",
      "autoencoder training epoch 188: decode layer difference: 96.7322167179874\n",
      "autoencoder training epoch 189: decode layer difference: 96.7312157169864\n",
      "autoencoder training epoch 190: decode layer difference: 96.7312157169864\n",
      "autoencoder training epoch 191: decode layer difference: 96.7312157169864\n",
      "autoencoder training epoch 192: decode layer difference: 96.73321972500243\n",
      "autoencoder training epoch 193: decode layer difference: 96.73321972500243\n",
      "autoencoder training epoch 194: decode layer difference: 96.73321972500243\n",
      "autoencoder training epoch 195: decode layer difference: 97.73522172700444\n",
      "autoencoder training epoch 196: decode layer difference: 96.73522172700443\n",
      "autoencoder training epoch 197: decode layer difference: 96.73422072600343\n",
      "autoencoder training epoch 198: decode layer difference: 96.7332177189884\n",
      "autoencoder training epoch 199: decode layer difference: 97.73321972500244\n",
      "autoencoder training epoch 200: decode layer difference: 96.73522172700443\n",
      "training epoch 1: error: 3.0676733411207966\n",
      "training epoch 2: error: 1.844919217761551\n",
      "training epoch 3: error: 2.050146525634215\n",
      "training epoch 4: error: 2.0670982816207415\n",
      "training epoch 5: error: 1.9407378611713175\n",
      "training epoch 6: error: 1.8365381563915792\n",
      "training epoch 7: error: 10.476979237865834\n",
      "training epoch 8: error: 1.904055596276486\n",
      "training epoch 9: error: 2.1944196345761444\n",
      "training epoch 10: error: 3.481948112774589\n",
      "training epoch 11: error: 2.424617106820299\n",
      "training epoch 12: error: 2.1319420093470813\n",
      "training epoch 13: error: 1.895868565906782\n",
      "training epoch 14: error: 1.806594365471997\n",
      "training epoch 15: error: 1.6426515835357038\n",
      "training epoch 16: error: 1.7699419378674304\n",
      "training epoch 17: error: 1.502622110718952\n",
      "training epoch 18: error: 1.3086161196110901\n",
      "training epoch 19: error: 5.516830347983842\n",
      "training epoch 20: error: 1.1672371644612425\n",
      "\n",
      "fold 1: mse: 0.07190055457919438\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 109.25596204649719\n",
      "autoencoder training epoch 2: decode layer difference: 109.25596204649719\n",
      "autoencoder training epoch 3: decode layer difference: 109.25596204649719\n",
      "autoencoder training epoch 4: decode layer difference: 109.25596204649719\n",
      "autoencoder training epoch 5: decode layer difference: 109.25596204649719\n",
      "autoencoder training epoch 6: decode layer difference: 109.25596204649719\n",
      "autoencoder training epoch 7: decode layer difference: 109.25596204649719\n",
      "autoencoder training epoch 8: decode layer difference: 109.25596204649719\n",
      "autoencoder training epoch 9: decode layer difference: 109.25596204649719\n",
      "autoencoder training epoch 10: decode layer difference: 109.25596204649719\n",
      "autoencoder training epoch 11: decode layer difference: 109.25596204649719\n",
      "autoencoder training epoch 12: decode layer difference: 109.25596204649719\n",
      "autoencoder training epoch 13: decode layer difference: 109.25596204649719\n",
      "autoencoder training epoch 14: decode layer difference: 109.25596204649719\n",
      "autoencoder training epoch 15: decode layer difference: 109.25596204649719\n",
      "autoencoder training epoch 16: decode layer difference: 109.25596204649719\n",
      "autoencoder training epoch 17: decode layer difference: 109.25596204649719\n",
      "autoencoder training epoch 18: decode layer difference: 109.25596204649719\n",
      "autoencoder training epoch 19: decode layer difference: 109.25596204649719\n",
      "autoencoder training epoch 20: decode layer difference: 109.25596204649719\n",
      "autoencoder training epoch 21: decode layer difference: 109.25596204649719\n",
      "autoencoder training epoch 22: decode layer difference: 107.25596204649719\n",
      "autoencoder training epoch 23: decode layer difference: 103.33485618001033\n",
      "autoencoder training epoch 24: decode layer difference: 102.33685818201234\n",
      "autoencoder training epoch 25: decode layer difference: 102.77051469759738\n",
      "autoencoder training epoch 26: decode layer difference: 102.64609425557747\n",
      "autoencoder training epoch 27: decode layer difference: 102.59312962540447\n",
      "autoencoder training epoch 28: decode layer difference: 102.59312962540447\n",
      "autoencoder training epoch 29: decode layer difference: 102.59413062640547\n",
      "autoencoder training epoch 30: decode layer difference: 102.67302475991863\n",
      "autoencoder training epoch 31: decode layer difference: 102.92527701217088\n",
      "autoencoder training epoch 32: decode layer difference: 102.92402576091962\n",
      "autoencoder training epoch 33: decode layer difference: 102.92302475991863\n",
      "autoencoder training epoch 34: decode layer difference: 102.17527701217088\n",
      "autoencoder training epoch 35: decode layer difference: 102.17527701217088\n",
      "autoencoder training epoch 36: decode layer difference: 101.92402576091962\n",
      "autoencoder training epoch 37: decode layer difference: 102.17527701217088\n",
      "autoencoder training epoch 38: decode layer difference: 102.30027701217088\n",
      "autoencoder training epoch 39: decode layer difference: 102.80027701217088\n",
      "autoencoder training epoch 40: decode layer difference: 102.17527701217088\n",
      "autoencoder training epoch 41: decode layer difference: 102.67527701217088\n",
      "autoencoder training epoch 42: decode layer difference: 102.54902576091962\n",
      "autoencoder training epoch 43: decode layer difference: 102.17527701217088\n",
      "autoencoder training epoch 44: decode layer difference: 102.67627801317188\n",
      "autoencoder training epoch 45: decode layer difference: 102.42302475991863\n",
      "autoencoder training epoch 46: decode layer difference: 102.17527701217088\n",
      "autoencoder training epoch 47: decode layer difference: 102.67527701217088\n",
      "autoencoder training epoch 48: decode layer difference: 102.17727901417288\n",
      "autoencoder training epoch 49: decode layer difference: 102.67527701217088\n",
      "autoencoder training epoch 50: decode layer difference: 102.67527701217088\n",
      "autoencoder training epoch 51: decode layer difference: 102.42402576091962\n",
      "autoencoder training epoch 52: decode layer difference: 102.67527701217088\n",
      "autoencoder training epoch 53: decode layer difference: 100.60871044560432\n",
      "autoencoder training epoch 54: decode layer difference: 101.86096269785656\n",
      "autoencoder training epoch 55: decode layer difference: 101.36196369885755\n",
      "autoencoder training epoch 56: decode layer difference: 101.60871044560432\n",
      "autoencoder training epoch 57: decode layer difference: 100.60871044560432\n",
      "autoencoder training epoch 58: decode layer difference: 101.61471946063136\n",
      "autoencoder training epoch 59: decode layer difference: 102.6157234706534\n",
      "autoencoder training epoch 60: decode layer difference: 102.6147224696524\n",
      "autoencoder training epoch 61: decode layer difference: 102.1147224696524\n",
      "autoencoder training epoch 62: decode layer difference: 102.1157234706534\n",
      "autoencoder training epoch 63: decode layer difference: 102.11772547265541\n",
      "autoencoder training epoch 64: decode layer difference: 102.1157234706534\n",
      "autoencoder training epoch 65: decode layer difference: 102.11772547265541\n",
      "autoencoder training epoch 66: decode layer difference: 102.11772547265541\n",
      "autoencoder training epoch 67: decode layer difference: 102.11972747465741\n",
      "autoencoder training epoch 68: decode layer difference: 101.11972747465741\n",
      "autoencoder training epoch 69: decode layer difference: 101.11972747465741\n",
      "autoencoder training epoch 70: decode layer difference: 101.1147224696524\n",
      "autoencoder training epoch 71: decode layer difference: 99.87073048167244\n",
      "autoencoder training epoch 72: decode layer difference: 99.87073048167244\n",
      "autoencoder training epoch 73: decode layer difference: 99.86972948067144\n",
      "autoencoder training epoch 74: decode layer difference: 99.87073048167244\n",
      "autoencoder training epoch 75: decode layer difference: 99.99573048167244\n",
      "autoencoder training epoch 76: decode layer difference: 99.99573048167244\n",
      "autoencoder training epoch 77: decode layer difference: 99.99573048167244\n",
      "autoencoder training epoch 78: decode layer difference: 99.99573048167244\n",
      "autoencoder training epoch 79: decode layer difference: 99.88635548167244\n",
      "autoencoder training epoch 80: decode layer difference: 99.63635548167244\n",
      "autoencoder training epoch 81: decode layer difference: 99.63635548167244\n",
      "autoencoder training epoch 82: decode layer difference: 99.63635548167244\n",
      "autoencoder training epoch 83: decode layer difference: 99.58948048167244\n",
      "autoencoder training epoch 84: decode layer difference: 99.58948048167244\n",
      "autoencoder training epoch 85: decode layer difference: 99.58847948067144\n",
      "autoencoder training epoch 86: decode layer difference: 99.58948048167244\n",
      "autoencoder training epoch 87: decode layer difference: 99.58948048167244\n",
      "autoencoder training epoch 88: decode layer difference: 99.58948048167244\n",
      "autoencoder training epoch 89: decode layer difference: 99.58847948067144\n",
      "autoencoder training epoch 90: decode layer difference: 99.58847948067144\n",
      "autoencoder training epoch 91: decode layer difference: 99.58847948067144\n",
      "autoencoder training epoch 92: decode layer difference: 99.58847948067144\n",
      "autoencoder training epoch 93: decode layer difference: 99.58847948067144\n",
      "autoencoder training epoch 94: decode layer difference: 99.58847948067144\n",
      "autoencoder training epoch 95: decode layer difference: 99.58847948067144\n",
      "autoencoder training epoch 96: decode layer difference: 99.58948048167244\n",
      "autoencoder training epoch 97: decode layer difference: 99.58948048167244\n",
      "autoencoder training epoch 98: decode layer difference: 99.58948048167244\n",
      "autoencoder training epoch 99: decode layer difference: 99.58948048167244\n",
      "autoencoder training epoch 100: decode layer difference: 99.58948048167244\n",
      "autoencoder training epoch 101: decode layer difference: 99.58847948067144\n",
      "autoencoder training epoch 102: decode layer difference: 99.58948048167244\n",
      "autoencoder training epoch 103: decode layer difference: 99.58847948067144\n",
      "autoencoder training epoch 104: decode layer difference: 99.58948048167244\n",
      "autoencoder training epoch 105: decode layer difference: 99.58948048167244\n",
      "autoencoder training epoch 106: decode layer difference: 99.58948048167244\n",
      "autoencoder training epoch 107: decode layer difference: 99.58948048167244\n",
      "autoencoder training epoch 108: decode layer difference: 99.58948048167244\n",
      "autoencoder training epoch 109: decode layer difference: 99.58948048167244\n",
      "autoencoder training epoch 110: decode layer difference: 99.58948048167244\n",
      "autoencoder training epoch 111: decode layer difference: 99.58948048167244\n",
      "autoencoder training epoch 112: decode layer difference: 99.58948048167244\n",
      "autoencoder training epoch 113: decode layer difference: 99.58948048167244\n",
      "autoencoder training epoch 114: decode layer difference: 99.46448048167244\n",
      "autoencoder training epoch 115: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 116: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 117: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 118: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 119: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 120: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 121: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 122: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 123: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 124: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 125: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 126: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 127: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 128: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 129: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 130: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 131: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 132: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 133: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 134: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 135: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 136: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 137: decode layer difference: 99.46347948067144\n",
      "autoencoder training epoch 138: decode layer difference: 99.28166129885327\n",
      "autoencoder training epoch 139: decode layer difference: 99.28166129885327\n",
      "autoencoder training epoch 140: decode layer difference: 98.7786552868292\n",
      "autoencoder training epoch 141: decode layer difference: 99.28166129885327\n",
      "autoencoder training epoch 142: decode layer difference: 98.82711584430781\n",
      "autoencoder training epoch 143: decode layer difference: 97.32410983228375\n",
      "autoencoder training epoch 144: decode layer difference: 95.82110382025971\n",
      "autoencoder training epoch 145: decode layer difference: 95.82110382025971\n",
      "autoencoder training epoch 146: decode layer difference: 95.82110382025971\n",
      "autoencoder training epoch 147: decode layer difference: 96.32410983228375\n",
      "autoencoder training epoch 148: decode layer difference: 95.82110382025971\n",
      "autoencoder training epoch 149: decode layer difference: 95.35956535872124\n",
      "autoencoder training epoch 150: decode layer difference: 95.89802689718279\n",
      "autoencoder training epoch 151: decode layer difference: 95.89802689718279\n",
      "autoencoder training epoch 152: decode layer difference: 95.89902789818379\n",
      "autoencoder training epoch 153: decode layer difference: 95.89802689718279\n",
      "autoencoder training epoch 154: decode layer difference: 95.89902789818379\n",
      "autoencoder training epoch 155: decode layer difference: 95.66725766641356\n",
      "autoencoder training epoch 156: decode layer difference: 95.66725766641356\n",
      "autoencoder training epoch 157: decode layer difference: 95.89902789818379\n",
      "autoencoder training epoch 158: decode layer difference: 95.89902789818379\n",
      "autoencoder training epoch 159: decode layer difference: 95.89802689718279\n",
      "autoencoder training epoch 160: decode layer difference: 95.89902789818379\n",
      "autoencoder training epoch 161: decode layer difference: 95.89902789818379\n",
      "autoencoder training epoch 162: decode layer difference: 95.89902789818379\n",
      "autoencoder training epoch 163: decode layer difference: 95.89902789818379\n",
      "autoencoder training epoch 164: decode layer difference: 95.66825866741456\n",
      "autoencoder training epoch 165: decode layer difference: 95.66825866741456\n",
      "autoencoder training epoch 166: decode layer difference: 96.3081188072747\n",
      "autoencoder training epoch 167: decode layer difference: 96.07734957650547\n",
      "autoencoder training epoch 168: decode layer difference: 95.66825866741456\n",
      "autoencoder training epoch 169: decode layer difference: 96.3081188072747\n",
      "autoencoder training epoch 170: decode layer difference: 95.66825866741456\n",
      "autoencoder training epoch 171: decode layer difference: 96.3081188072747\n",
      "autoencoder training epoch 172: decode layer difference: 96.07734957650547\n",
      "autoencoder training epoch 173: decode layer difference: 96.3081188072747\n",
      "autoencoder training epoch 174: decode layer difference: 96.3081188072747\n",
      "autoencoder training epoch 175: decode layer difference: 96.3081188072747\n",
      "autoencoder training epoch 176: decode layer difference: 96.3081188072747\n",
      "autoencoder training epoch 177: decode layer difference: 96.3081188072747\n",
      "autoencoder training epoch 178: decode layer difference: 96.07734957650547\n",
      "autoencoder training epoch 179: decode layer difference: 96.07734957650547\n",
      "autoencoder training epoch 180: decode layer difference: 96.07734957650547\n",
      "autoencoder training epoch 181: decode layer difference: 96.07734957650547\n",
      "autoencoder training epoch 182: decode layer difference: 96.07734957650547\n",
      "autoencoder training epoch 183: decode layer difference: 96.3081188072747\n",
      "autoencoder training epoch 184: decode layer difference: 96.30911980827571\n",
      "autoencoder training epoch 185: decode layer difference: 96.3081188072747\n",
      "autoencoder training epoch 186: decode layer difference: 96.3081188072747\n",
      "autoencoder training epoch 187: decode layer difference: 96.3081188072747\n",
      "autoencoder training epoch 188: decode layer difference: 96.3081188072747\n",
      "autoencoder training epoch 189: decode layer difference: 96.07734957650547\n",
      "autoencoder training epoch 190: decode layer difference: 95.85357335272924\n",
      "autoencoder training epoch 191: decode layer difference: 95.85357335272924\n",
      "autoencoder training epoch 192: decode layer difference: 95.85357335272924\n",
      "autoencoder training epoch 193: decode layer difference: 95.85357335272924\n",
      "autoencoder training epoch 194: decode layer difference: 95.85357335272924\n",
      "autoencoder training epoch 195: decode layer difference: 95.85357335272924\n",
      "autoencoder training epoch 196: decode layer difference: 95.62280412196\n",
      "autoencoder training epoch 197: decode layer difference: 95.85357335272924\n",
      "autoencoder training epoch 198: decode layer difference: 95.85357335272924\n",
      "autoencoder training epoch 199: decode layer difference: 95.85357335272924\n",
      "autoencoder training epoch 200: decode layer difference: 95.85357335272924\n",
      "training epoch 1: error: 2.1883061712858227\n",
      "training epoch 2: error: 1.8537276443331185\n",
      "training epoch 3: error: 1.909585835770804\n",
      "training epoch 4: error: 2.271564122608046\n",
      "training epoch 5: error: 1.953213720463706\n",
      "training epoch 6: error: 1.914973766690419\n",
      "training epoch 7: error: 2.075189594078918\n",
      "training epoch 8: error: 1.8520935880368024\n",
      "training epoch 9: error: 2.174583746926611\n",
      "training epoch 10: error: 1.9407750998312556\n",
      "training epoch 11: error: 1.8209400648470349\n",
      "training epoch 12: error: 4.370655804765249\n",
      "training epoch 13: error: 1.8489288728087945\n",
      "training epoch 14: error: 2.3249906707196315\n",
      "training epoch 15: error: 1.7719102180043382\n",
      "training epoch 16: error: 1.7934379518889325\n",
      "training epoch 17: error: 2.264508160979221\n",
      "training epoch 18: error: 1.6486902461085222\n",
      "training epoch 19: error: 1.602921055781295\n",
      "training epoch 20: error: 1.6276390866572203\n",
      "\n",
      "fold 2: mse: 0.11371347098427252\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 104.96895117076991\n",
      "autoencoder training epoch 2: decode layer difference: 104.96895117076991\n",
      "autoencoder training epoch 3: decode layer difference: 104.96895117076991\n",
      "autoencoder training epoch 4: decode layer difference: 104.96895117076991\n",
      "autoencoder training epoch 5: decode layer difference: 104.96895117076991\n",
      "autoencoder training epoch 6: decode layer difference: 104.96895117076991\n",
      "autoencoder training epoch 7: decode layer difference: 104.96895117076991\n",
      "autoencoder training epoch 8: decode layer difference: 104.96895117076991\n",
      "autoencoder training epoch 9: decode layer difference: 104.96895117076991\n",
      "autoencoder training epoch 10: decode layer difference: 104.96895117076991\n",
      "autoencoder training epoch 11: decode layer difference: 104.96895117076991\n",
      "autoencoder training epoch 12: decode layer difference: 104.96895117076991\n",
      "autoencoder training epoch 13: decode layer difference: 104.96895117076991\n",
      "autoencoder training epoch 14: decode layer difference: 104.96895117076991\n",
      "autoencoder training epoch 15: decode layer difference: 104.96895117076991\n",
      "autoencoder training epoch 16: decode layer difference: 104.96895117076991\n",
      "autoencoder training epoch 17: decode layer difference: 104.96895117076991\n",
      "autoencoder training epoch 18: decode layer difference: 104.96895117076991\n",
      "autoencoder training epoch 19: decode layer difference: 104.96895117076991\n",
      "autoencoder training epoch 20: decode layer difference: 104.96895117076991\n",
      "autoencoder training epoch 21: decode layer difference: 103.96895117076991\n",
      "autoencoder training epoch 22: decode layer difference: 99.96995217177093\n",
      "autoencoder training epoch 23: decode layer difference: 99.5093992385275\n",
      "autoencoder training epoch 24: decode layer difference: 99.32057184066437\n",
      "autoencoder training epoch 25: decode layer difference: 99.13074344180022\n",
      "autoencoder training epoch 26: decode layer difference: 99.06229063295632\n",
      "autoencoder training epoch 27: decode layer difference: 99.0632916339573\n",
      "autoencoder training epoch 28: decode layer difference: 98.83842010025234\n",
      "autoencoder training epoch 29: decode layer difference: 98.83942110125332\n",
      "autoencoder training epoch 30: decode layer difference: 98.91831523476648\n",
      "autoencoder training epoch 31: decode layer difference: 98.91831523476648\n",
      "autoencoder training epoch 32: decode layer difference: 98.91831523476648\n",
      "autoencoder training epoch 33: decode layer difference: 98.91831523476648\n",
      "autoencoder training epoch 34: decode layer difference: 98.92031723676848\n",
      "autoencoder training epoch 35: decode layer difference: 98.9213182377695\n",
      "autoencoder training epoch 36: decode layer difference: 98.9233202397715\n",
      "autoencoder training epoch 37: decode layer difference: 98.92031723676848\n",
      "autoencoder training epoch 38: decode layer difference: 98.9233202397715\n",
      "autoencoder training epoch 39: decode layer difference: 98.92031723676848\n",
      "autoencoder training epoch 40: decode layer difference: 98.92031723676848\n",
      "autoencoder training epoch 41: decode layer difference: 98.92031723676848\n",
      "autoencoder training epoch 42: decode layer difference: 97.92031723676848\n",
      "autoencoder training epoch 43: decode layer difference: 97.92231923877048\n",
      "autoencoder training epoch 44: decode layer difference: 97.92031723676848\n",
      "autoencoder training epoch 45: decode layer difference: 97.92332023977148\n",
      "autoencoder training epoch 46: decode layer difference: 97.92332023977148\n",
      "autoencoder training epoch 47: decode layer difference: 97.92332023977148\n",
      "autoencoder training epoch 48: decode layer difference: 97.92332023977148\n",
      "autoencoder training epoch 49: decode layer difference: 97.92332023977148\n",
      "autoencoder training epoch 50: decode layer difference: 97.92332023977148\n",
      "autoencoder training epoch 51: decode layer difference: 98.42632625179553\n",
      "autoencoder training epoch 52: decode layer difference: 98.42632625179553\n",
      "autoencoder training epoch 53: decode layer difference: 98.42632625179553\n",
      "autoencoder training epoch 54: decode layer difference: 98.42632625179553\n",
      "autoencoder training epoch 55: decode layer difference: 98.42632625179553\n",
      "autoencoder training epoch 56: decode layer difference: 98.42632625179553\n",
      "autoencoder training epoch 57: decode layer difference: 98.42632625179553\n",
      "autoencoder training epoch 58: decode layer difference: 98.43133326281458\n",
      "autoencoder training epoch 59: decode layer difference: 98.43133326281458\n",
      "autoencoder training epoch 60: decode layer difference: 98.43133326281458\n",
      "autoencoder training epoch 61: decode layer difference: 98.43133326281458\n",
      "autoencoder training epoch 62: decode layer difference: 98.43133326281458\n",
      "autoencoder training epoch 63: decode layer difference: 98.43033226181356\n",
      "autoencoder training epoch 64: decode layer difference: 98.43133326281458\n",
      "autoencoder training epoch 65: decode layer difference: 98.43133326281458\n",
      "autoencoder training epoch 66: decode layer difference: 98.42933126081257\n",
      "autoencoder training epoch 67: decode layer difference: 98.43133326281458\n",
      "autoencoder training epoch 68: decode layer difference: 98.42933126081257\n",
      "autoencoder training epoch 69: decode layer difference: 98.43033226181356\n",
      "autoencoder training epoch 70: decode layer difference: 98.43033226181356\n",
      "autoencoder training epoch 71: decode layer difference: 98.43033226181356\n",
      "autoencoder training epoch 72: decode layer difference: 98.43033226181356\n",
      "autoencoder training epoch 73: decode layer difference: 98.43033226181356\n",
      "autoencoder training epoch 74: decode layer difference: 98.43033226181356\n",
      "autoencoder training epoch 75: decode layer difference: 98.43033226181356\n",
      "autoencoder training epoch 76: decode layer difference: 98.42933126081257\n",
      "autoencoder training epoch 77: decode layer difference: 98.43033226181356\n",
      "autoencoder training epoch 78: decode layer difference: 98.43033226181356\n",
      "autoencoder training epoch 79: decode layer difference: 98.42933126081257\n",
      "autoencoder training epoch 80: decode layer difference: 98.42933126081257\n",
      "autoencoder training epoch 81: decode layer difference: 98.42933126081257\n",
      "autoencoder training epoch 82: decode layer difference: 98.55433126081257\n",
      "autoencoder training epoch 83: decode layer difference: 98.55433126081257\n",
      "autoencoder training epoch 84: decode layer difference: 98.55433126081257\n",
      "autoencoder training epoch 85: decode layer difference: 98.55433126081257\n",
      "autoencoder training epoch 86: decode layer difference: 98.55433126081257\n",
      "autoencoder training epoch 87: decode layer difference: 98.55433126081257\n",
      "autoencoder training epoch 88: decode layer difference: 98.44495626081257\n",
      "autoencoder training epoch 89: decode layer difference: 98.44495626081257\n",
      "autoencoder training epoch 90: decode layer difference: 98.44495626081257\n",
      "autoencoder training epoch 91: decode layer difference: 98.44495626081257\n",
      "autoencoder training epoch 92: decode layer difference: 98.44495626081257\n",
      "autoencoder training epoch 93: decode layer difference: 98.44495626081257\n",
      "autoencoder training epoch 94: decode layer difference: 98.44495626081257\n",
      "autoencoder training epoch 95: decode layer difference: 98.39808126081257\n",
      "autoencoder training epoch 96: decode layer difference: 98.39808126081257\n",
      "autoencoder training epoch 97: decode layer difference: 98.39808126081257\n",
      "autoencoder training epoch 98: decode layer difference: 98.39808126081257\n",
      "autoencoder training epoch 99: decode layer difference: 98.39808126081257\n",
      "autoencoder training epoch 100: decode layer difference: 98.39808126081257\n",
      "autoencoder training epoch 101: decode layer difference: 98.39808126081257\n",
      "autoencoder training epoch 102: decode layer difference: 98.39808126081257\n",
      "autoencoder training epoch 103: decode layer difference: 98.39808126081257\n",
      "autoencoder training epoch 104: decode layer difference: 98.39808126081257\n",
      "autoencoder training epoch 105: decode layer difference: 98.39808126081257\n",
      "autoencoder training epoch 106: decode layer difference: 98.39808126081257\n",
      "autoencoder training epoch 107: decode layer difference: 98.39808126081257\n",
      "autoencoder training epoch 108: decode layer difference: 98.39808126081257\n",
      "autoencoder training epoch 109: decode layer difference: 98.39808126081257\n",
      "autoencoder training epoch 110: decode layer difference: 97.89507524878852\n",
      "autoencoder training epoch 111: decode layer difference: 98.39808126081257\n",
      "autoencoder training epoch 112: decode layer difference: 97.77007524878852\n",
      "autoencoder training epoch 113: decode layer difference: 97.77007524878852\n",
      "autoencoder training epoch 114: decode layer difference: 97.77107624978953\n",
      "autoencoder training epoch 115: decode layer difference: 97.77007524878852\n",
      "autoencoder training epoch 116: decode layer difference: 97.77107624978953\n",
      "autoencoder training epoch 117: decode layer difference: 97.77007524878852\n",
      "autoencoder training epoch 118: decode layer difference: 97.77007524878852\n",
      "autoencoder training epoch 119: decode layer difference: 97.77007524878852\n",
      "autoencoder training epoch 120: decode layer difference: 97.77007524878852\n",
      "autoencoder training epoch 121: decode layer difference: 97.77007524878852\n",
      "autoencoder training epoch 122: decode layer difference: 97.77007524878852\n",
      "autoencoder training epoch 123: decode layer difference: 97.31653079524406\n",
      "autoencoder training epoch 124: decode layer difference: 96.31653079524406\n",
      "autoencoder training epoch 125: decode layer difference: 96.31653079524406\n",
      "autoencoder training epoch 126: decode layer difference: 96.13471261342589\n",
      "autoencoder training epoch 127: decode layer difference: 96.13471261342589\n",
      "autoencoder training epoch 128: decode layer difference: 96.13371161242489\n",
      "autoencoder training epoch 129: decode layer difference: 96.13471261342589\n",
      "autoencoder training epoch 130: decode layer difference: 96.13471261342589\n",
      "autoencoder training epoch 131: decode layer difference: 96.13571361442689\n",
      "autoencoder training epoch 132: decode layer difference: 96.13571361442689\n",
      "autoencoder training epoch 133: decode layer difference: 96.13571361442689\n",
      "autoencoder training epoch 134: decode layer difference: 96.13571361442689\n",
      "autoencoder training epoch 135: decode layer difference: 96.13571361442689\n",
      "autoencoder training epoch 136: decode layer difference: 96.13571361442689\n",
      "autoencoder training epoch 137: decode layer difference: 96.13571361442689\n",
      "autoencoder training epoch 138: decode layer difference: 96.13571361442689\n",
      "autoencoder training epoch 139: decode layer difference: 96.13571361442689\n",
      "autoencoder training epoch 140: decode layer difference: 96.13571361442689\n",
      "autoencoder training epoch 141: decode layer difference: 96.13571361442689\n",
      "autoencoder training epoch 142: decode layer difference: 96.13571361442689\n",
      "autoencoder training epoch 143: decode layer difference: 96.13571361442689\n",
      "autoencoder training epoch 144: decode layer difference: 96.13571361442689\n",
      "autoencoder training epoch 145: decode layer difference: 96.13571361442689\n",
      "autoencoder training epoch 146: decode layer difference: 96.13571361442689\n",
      "autoencoder training epoch 147: decode layer difference: 96.13571361442689\n",
      "autoencoder training epoch 148: decode layer difference: 95.90494438365766\n",
      "autoencoder training epoch 149: decode layer difference: 95.90494438365766\n",
      "autoencoder training epoch 150: decode layer difference: 95.90494438365766\n",
      "autoencoder training epoch 151: decode layer difference: 96.13571361442689\n",
      "autoencoder training epoch 152: decode layer difference: 96.13571361442689\n",
      "autoencoder training epoch 153: decode layer difference: 95.90494438365766\n",
      "autoencoder training epoch 154: decode layer difference: 95.90494438365766\n",
      "autoencoder training epoch 155: decode layer difference: 95.90494438365766\n",
      "autoencoder training epoch 156: decode layer difference: 95.90494438365766\n",
      "autoencoder training epoch 157: decode layer difference: 95.90494438365766\n",
      "autoencoder training epoch 158: decode layer difference: 95.90494438365766\n",
      "autoencoder training epoch 159: decode layer difference: 95.90494438365766\n",
      "autoencoder training epoch 160: decode layer difference: 95.90494438365766\n",
      "autoencoder training epoch 161: decode layer difference: 95.90494438365766\n",
      "autoencoder training epoch 162: decode layer difference: 95.90494438365766\n",
      "autoencoder training epoch 163: decode layer difference: 95.90494438365766\n",
      "autoencoder training epoch 164: decode layer difference: 96.13571361442689\n",
      "autoencoder training epoch 165: decode layer difference: 96.13571361442689\n",
      "autoencoder training epoch 166: decode layer difference: 96.4434059221192\n",
      "autoencoder training epoch 167: decode layer difference: 96.4434059221192\n",
      "autoencoder training epoch 168: decode layer difference: 96.4434059221192\n",
      "autoencoder training epoch 169: decode layer difference: 96.4434059221192\n",
      "autoencoder training epoch 170: decode layer difference: 96.4434059221192\n",
      "autoencoder training epoch 171: decode layer difference: 96.4444069231202\n",
      "autoencoder training epoch 172: decode layer difference: 96.4434059221192\n",
      "autoencoder training epoch 173: decode layer difference: 96.67417515288842\n",
      "autoencoder training epoch 174: decode layer difference: 96.4434059221192\n",
      "autoencoder training epoch 175: decode layer difference: 96.4434059221192\n",
      "autoencoder training epoch 176: decode layer difference: 96.4434059221192\n",
      "autoencoder training epoch 177: decode layer difference: 96.4434059221192\n",
      "autoencoder training epoch 178: decode layer difference: 96.4434059221192\n",
      "autoencoder training epoch 179: decode layer difference: 96.4434059221192\n",
      "autoencoder training epoch 180: decode layer difference: 96.44540993013523\n",
      "autoencoder training epoch 181: decode layer difference: 96.4434059221192\n",
      "autoencoder training epoch 182: decode layer difference: 96.4434059221192\n",
      "autoencoder training epoch 183: decode layer difference: 96.67417515288842\n",
      "autoencoder training epoch 184: decode layer difference: 96.4434059221192\n",
      "autoencoder training epoch 185: decode layer difference: 96.67317415188742\n",
      "autoencoder training epoch 186: decode layer difference: 95.5527809221192\n",
      "autoencoder training epoch 187: decode layer difference: 95.55478493013523\n",
      "autoencoder training epoch 188: decode layer difference: 95.5527809221192\n",
      "autoencoder training epoch 189: decode layer difference: 95.5527809221192\n",
      "autoencoder training epoch 190: decode layer difference: 95.4434059221192\n",
      "autoencoder training epoch 191: decode layer difference: 95.5527809221192\n",
      "autoencoder training epoch 192: decode layer difference: 95.4434059221192\n",
      "autoencoder training epoch 193: decode layer difference: 95.4434059221192\n",
      "autoencoder training epoch 194: decode layer difference: 95.78355015288842\n",
      "autoencoder training epoch 195: decode layer difference: 95.5527809221192\n",
      "autoencoder training epoch 196: decode layer difference: 95.44540993013523\n",
      "autoencoder training epoch 197: decode layer difference: 95.5527809221192\n",
      "autoencoder training epoch 198: decode layer difference: 95.5527809221192\n",
      "autoencoder training epoch 199: decode layer difference: 95.55478493013523\n",
      "autoencoder training epoch 200: decode layer difference: 95.5527809221192\n",
      "training epoch 1: error: 1.569258961226168\n",
      "training epoch 2: error: 1.6118970058553388\n",
      "training epoch 3: error: 1.5244288074575048\n",
      "training epoch 4: error: 2.1519244251381373\n",
      "training epoch 5: error: 1.5343163142658645\n",
      "training epoch 6: error: 1.7449926080360478\n",
      "training epoch 7: error: 1.8233923964056111\n",
      "training epoch 8: error: 1.8703362478072243\n",
      "training epoch 9: error: 1.5599320802770262\n",
      "training epoch 10: error: 1.8480175793979763\n",
      "training epoch 11: error: 1.7523531565987822\n",
      "training epoch 12: error: 1.523319488741042\n",
      "training epoch 13: error: 1.5029605527042815\n",
      "training epoch 14: error: 1.5113087810305208\n",
      "training epoch 15: error: 1.7147122627288798\n",
      "training epoch 16: error: 1.520117886257465\n",
      "training epoch 17: error: 1.5331994671730353\n",
      "training epoch 18: error: 1.429025561996729\n",
      "training epoch 19: error: 1.5516348277574996\n",
      "training epoch 20: error: 1.5627199979979425\n",
      "\n",
      "fold 3: mse: 0.487473189996478\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 105.69914653472688\n",
      "autoencoder training epoch 2: decode layer difference: 105.69914653472688\n",
      "autoencoder training epoch 3: decode layer difference: 105.69914653472688\n",
      "autoencoder training epoch 4: decode layer difference: 105.69914653472688\n",
      "autoencoder training epoch 5: decode layer difference: 105.69914653472688\n",
      "autoencoder training epoch 6: decode layer difference: 105.69914653472688\n",
      "autoencoder training epoch 7: decode layer difference: 105.69914653472688\n",
      "autoencoder training epoch 8: decode layer difference: 105.69914653472688\n",
      "autoencoder training epoch 9: decode layer difference: 105.69914653472688\n",
      "autoencoder training epoch 10: decode layer difference: 105.69914653472688\n",
      "autoencoder training epoch 11: decode layer difference: 105.69914653472688\n",
      "autoencoder training epoch 12: decode layer difference: 105.69914653472688\n",
      "autoencoder training epoch 13: decode layer difference: 105.69914653472688\n",
      "autoencoder training epoch 14: decode layer difference: 105.69914653472688\n",
      "autoencoder training epoch 15: decode layer difference: 105.69914653472688\n",
      "autoencoder training epoch 16: decode layer difference: 105.69914653472688\n",
      "autoencoder training epoch 17: decode layer difference: 105.69914653472688\n",
      "autoencoder training epoch 18: decode layer difference: 105.69914653472688\n",
      "autoencoder training epoch 19: decode layer difference: 105.69914653472688\n",
      "autoencoder training epoch 20: decode layer difference: 105.69914653472688\n",
      "autoencoder training epoch 21: decode layer difference: 105.69914653472688\n",
      "autoencoder training epoch 22: decode layer difference: 102.70214953772988\n",
      "autoencoder training epoch 23: decode layer difference: 103.51646385204418\n",
      "autoencoder training epoch 24: decode layer difference: 101.5194668550472\n",
      "autoencoder training epoch 25: decode layer difference: 101.05891392180376\n",
      "autoencoder training epoch 26: decode layer difference: 100.86908552293963\n",
      "autoencoder training epoch 27: decode layer difference: 100.67825612307449\n",
      "autoencoder training epoch 28: decode layer difference: 100.60880231322957\n",
      "autoencoder training epoch 29: decode layer difference: 100.55283468005359\n",
      "autoencoder training epoch 30: decode layer difference: 101.05408593130484\n",
      "autoencoder training epoch 31: decode layer difference: 100.52299651417361\n",
      "autoencoder training epoch 32: decode layer difference: 100.77424776542486\n",
      "autoencoder training epoch 33: decode layer difference: 100.77424776542486\n",
      "autoencoder training epoch 34: decode layer difference: 100.77524876642586\n",
      "autoencoder training epoch 35: decode layer difference: 99.854142899939\n",
      "autoencoder training epoch 36: decode layer difference: 100.85314189893802\n",
      "autoencoder training epoch 37: decode layer difference: 99.85314189893802\n",
      "autoencoder training epoch 38: decode layer difference: 99.85514390094002\n",
      "autoencoder training epoch 39: decode layer difference: 100.35189064768676\n",
      "autoencoder training epoch 40: decode layer difference: 100.60314189893802\n",
      "autoencoder training epoch 41: decode layer difference: 100.47814189893802\n",
      "autoencoder training epoch 42: decode layer difference: 100.60314189893802\n",
      "autoencoder training epoch 43: decode layer difference: 100.35189064768676\n",
      "autoencoder training epoch 44: decode layer difference: 100.35189064768676\n",
      "autoencoder training epoch 45: decode layer difference: 100.47814189893802\n",
      "autoencoder training epoch 46: decode layer difference: 100.60314189893802\n",
      "autoencoder training epoch 47: decode layer difference: 100.60314189893802\n",
      "autoencoder training epoch 48: decode layer difference: 100.60314189893802\n",
      "autoencoder training epoch 49: decode layer difference: 100.60314189893802\n",
      "autoencoder training epoch 50: decode layer difference: 100.60314189893802\n",
      "autoencoder training epoch 51: decode layer difference: 100.47814189893802\n",
      "autoencoder training epoch 52: decode layer difference: 100.47814189893802\n",
      "autoencoder training epoch 53: decode layer difference: 100.47814189893802\n",
      "autoencoder training epoch 54: decode layer difference: 100.47814189893802\n",
      "autoencoder training epoch 55: decode layer difference: 100.47814189893802\n",
      "autoencoder training epoch 56: decode layer difference: 100.604142899939\n",
      "autoencoder training epoch 57: decode layer difference: 100.47814189893802\n",
      "autoencoder training epoch 58: decode layer difference: 100.60314189893802\n",
      "autoencoder training epoch 59: decode layer difference: 100.354142899939\n",
      "autoencoder training epoch 60: decode layer difference: 100.35314189893802\n",
      "autoencoder training epoch 61: decode layer difference: 100.479142899939\n",
      "autoencoder training epoch 62: decode layer difference: 100.604142899939\n",
      "autoencoder training epoch 63: decode layer difference: 100.604142899939\n",
      "autoencoder training epoch 64: decode layer difference: 100.479142899939\n",
      "autoencoder training epoch 65: decode layer difference: 100.47814189893802\n",
      "autoencoder training epoch 66: decode layer difference: 100.47814189893802\n",
      "autoencoder training epoch 67: decode layer difference: 100.479142899939\n",
      "autoencoder training epoch 68: decode layer difference: 100.47814189893802\n",
      "autoencoder training epoch 69: decode layer difference: 100.479142899939\n",
      "autoencoder training epoch 70: decode layer difference: 100.479142899939\n",
      "autoencoder training epoch 71: decode layer difference: 100.479142899939\n",
      "autoencoder training epoch 72: decode layer difference: 100.60314189893802\n",
      "autoencoder training epoch 73: decode layer difference: 100.47814189893802\n",
      "autoencoder training epoch 74: decode layer difference: 100.60314189893802\n",
      "autoencoder training epoch 75: decode layer difference: 100.479142899939\n",
      "autoencoder training epoch 76: decode layer difference: 100.604142899939\n",
      "autoencoder training epoch 77: decode layer difference: 100.604142899939\n",
      "autoencoder training epoch 78: decode layer difference: 100.47814189893802\n",
      "autoencoder training epoch 79: decode layer difference: 100.604142899939\n",
      "autoencoder training epoch 80: decode layer difference: 100.479142899939\n",
      "autoencoder training epoch 81: decode layer difference: 100.479142899939\n",
      "autoencoder training epoch 82: decode layer difference: 100.479142899939\n",
      "autoencoder training epoch 83: decode layer difference: 100.48014390094002\n",
      "autoencoder training epoch 84: decode layer difference: 100.48014390094002\n",
      "autoencoder training epoch 85: decode layer difference: 100.35314189893802\n",
      "autoencoder training epoch 86: decode layer difference: 100.479142899939\n",
      "autoencoder training epoch 87: decode layer difference: 100.479142899939\n",
      "autoencoder training epoch 88: decode layer difference: 100.10189064768676\n",
      "autoencoder training epoch 89: decode layer difference: 100.35714590294202\n",
      "autoencoder training epoch 90: decode layer difference: 100.35514390094002\n",
      "autoencoder training epoch 91: decode layer difference: 100.35714590294202\n",
      "autoencoder training epoch 92: decode layer difference: 100.354142899939\n",
      "autoencoder training epoch 93: decode layer difference: 100.354142899939\n",
      "autoencoder training epoch 94: decode layer difference: 100.22789164868777\n",
      "autoencoder training epoch 95: decode layer difference: 100.35514390094002\n",
      "autoencoder training epoch 96: decode layer difference: 100.22889264968876\n",
      "autoencoder training epoch 97: decode layer difference: 99.28857733437346\n",
      "autoencoder training epoch 98: decode layer difference: 99.29258133837746\n",
      "autoencoder training epoch 99: decode layer difference: 99.29258133837746\n",
      "autoencoder training epoch 100: decode layer difference: 99.28857733437346\n",
      "autoencoder training epoch 101: decode layer difference: 99.28857733437346\n",
      "autoencoder training epoch 102: decode layer difference: 99.29258133837746\n",
      "autoencoder training epoch 103: decode layer difference: 99.28857733437346\n",
      "autoencoder training epoch 104: decode layer difference: 99.28857733437346\n",
      "autoencoder training epoch 105: decode layer difference: 99.28857733437346\n",
      "autoencoder training epoch 106: decode layer difference: 99.29258133837746\n",
      "autoencoder training epoch 107: decode layer difference: 99.29258133837746\n",
      "autoencoder training epoch 108: decode layer difference: 99.29258133837746\n",
      "autoencoder training epoch 109: decode layer difference: 99.29258133837746\n",
      "autoencoder training epoch 110: decode layer difference: 99.28957833537444\n",
      "autoencoder training epoch 111: decode layer difference: 99.29258133837746\n",
      "autoencoder training epoch 112: decode layer difference: 99.29258133837746\n",
      "autoencoder training epoch 113: decode layer difference: 99.29458334037946\n",
      "autoencoder training epoch 114: decode layer difference: 98.79458334037946\n",
      "autoencoder training epoch 115: decode layer difference: 98.79458334037946\n",
      "autoencoder training epoch 116: decode layer difference: 98.79458334037946\n",
      "autoencoder training epoch 117: decode layer difference: 98.91958334037946\n",
      "autoencoder training epoch 118: decode layer difference: 98.91958334037946\n",
      "autoencoder training epoch 119: decode layer difference: 99.04558434138045\n",
      "autoencoder training epoch 120: decode layer difference: 99.04558434138045\n",
      "autoencoder training epoch 121: decode layer difference: 99.04558434138045\n",
      "autoencoder training epoch 122: decode layer difference: 99.04658534238146\n",
      "autoencoder training epoch 123: decode layer difference: 99.04658534238146\n",
      "autoencoder training epoch 124: decode layer difference: 99.04758634338245\n",
      "autoencoder training epoch 125: decode layer difference: 99.04658534238146\n",
      "autoencoder training epoch 126: decode layer difference: 98.04758634338245\n",
      "autoencoder training epoch 127: decode layer difference: 98.04758634338245\n",
      "autoencoder training epoch 128: decode layer difference: 98.55059235540651\n",
      "autoencoder training epoch 129: decode layer difference: 98.04758634338245\n",
      "autoencoder training epoch 130: decode layer difference: 98.04858734438346\n",
      "autoencoder training epoch 131: decode layer difference: 98.55059235540651\n",
      "autoencoder training epoch 132: decode layer difference: 98.55059235540651\n",
      "autoencoder training epoch 133: decode layer difference: 99.05359836743055\n",
      "autoencoder training epoch 134: decode layer difference: 99.05359836743055\n",
      "autoencoder training epoch 135: decode layer difference: 98.05359836743055\n",
      "autoencoder training epoch 136: decode layer difference: 97.05359836743055\n",
      "autoencoder training epoch 137: decode layer difference: 97.94422336743055\n",
      "autoencoder training epoch 138: decode layer difference: 97.94422336743055\n",
      "autoencoder training epoch 139: decode layer difference: 97.94422336743055\n",
      "autoencoder training epoch 140: decode layer difference: 96.94422336743055\n",
      "autoencoder training epoch 141: decode layer difference: 96.94422336743055\n",
      "autoencoder training epoch 142: decode layer difference: 96.94422336743055\n",
      "autoencoder training epoch 143: decode layer difference: 96.69422336743055\n",
      "autoencoder training epoch 144: decode layer difference: 96.69422336743055\n",
      "autoencoder training epoch 145: decode layer difference: 96.69422336743055\n",
      "autoencoder training epoch 146: decode layer difference: 96.19121735540651\n",
      "autoencoder training epoch 147: decode layer difference: 96.69422336743055\n",
      "autoencoder training epoch 148: decode layer difference: 96.69422336743055\n",
      "autoencoder training epoch 149: decode layer difference: 96.69422336743055\n",
      "autoencoder training epoch 150: decode layer difference: 96.69422336743055\n",
      "autoencoder training epoch 151: decode layer difference: 96.19121735540651\n",
      "autoencoder training epoch 152: decode layer difference: 95.82758099177013\n",
      "autoencoder training epoch 153: decode layer difference: 96.3305870037942\n",
      "autoencoder training epoch 154: decode layer difference: 95.3245749797461\n",
      "autoencoder training epoch 155: decode layer difference: 95.3245749797461\n",
      "autoencoder training epoch 156: decode layer difference: 95.3245749797461\n",
      "autoencoder training epoch 157: decode layer difference: 95.3245749797461\n",
      "autoencoder training epoch 158: decode layer difference: 95.3245749797461\n",
      "autoencoder training epoch 159: decode layer difference: 95.3245749797461\n",
      "autoencoder training epoch 160: decode layer difference: 95.32357397874509\n",
      "autoencoder training epoch 161: decode layer difference: 95.07357397874509\n",
      "autoencoder training epoch 162: decode layer difference: 95.07357397874509\n",
      "autoencoder training epoch 163: decode layer difference: 95.0745749797461\n",
      "autoencoder training epoch 164: decode layer difference: 95.07357397874509\n",
      "autoencoder training epoch 165: decode layer difference: 95.07357397874509\n",
      "autoencoder training epoch 166: decode layer difference: 95.0725729777441\n",
      "autoencoder training epoch 167: decode layer difference: 95.0725729777441\n",
      "autoencoder training epoch 168: decode layer difference: 95.61103451620563\n",
      "autoencoder training epoch 169: decode layer difference: 95.15648906166018\n",
      "autoencoder training epoch 170: decode layer difference: 95.15648906166018\n",
      "autoencoder training epoch 171: decode layer difference: 95.15648906166018\n",
      "autoencoder training epoch 172: decode layer difference: 95.15648906166018\n",
      "autoencoder training epoch 173: decode layer difference: 95.15648906166018\n",
      "autoencoder training epoch 174: decode layer difference: 92.90849306967621\n",
      "autoencoder training epoch 175: decode layer difference: 92.90849306967621\n",
      "autoencoder training epoch 176: decode layer difference: 93.44695460813776\n",
      "autoencoder training epoch 177: decode layer difference: 93.44695460813776\n",
      "autoencoder training epoch 178: decode layer difference: 93.44695460813776\n",
      "autoencoder training epoch 179: decode layer difference: 93.44495060012173\n",
      "autoencoder training epoch 180: decode layer difference: 92.90849306967621\n",
      "autoencoder training epoch 181: decode layer difference: 93.44695460813776\n",
      "autoencoder training epoch 182: decode layer difference: 93.44695460813776\n",
      "autoencoder training epoch 183: decode layer difference: 93.44695460813776\n",
      "autoencoder training epoch 184: decode layer difference: 93.44695460813776\n",
      "autoencoder training epoch 185: decode layer difference: 92.90849306967621\n",
      "autoencoder training epoch 186: decode layer difference: 92.90849306967621\n",
      "autoencoder training epoch 187: decode layer difference: 92.90849306967621\n",
      "autoencoder training epoch 188: decode layer difference: 92.90849306967621\n",
      "autoencoder training epoch 189: decode layer difference: 92.90849306967621\n",
      "autoencoder training epoch 190: decode layer difference: 92.90849306967621\n",
      "autoencoder training epoch 191: decode layer difference: 92.90849306967621\n",
      "autoencoder training epoch 192: decode layer difference: 92.90849306967621\n",
      "autoencoder training epoch 193: decode layer difference: 92.37003153121466\n",
      "autoencoder training epoch 194: decode layer difference: 92.29113739770153\n",
      "autoencoder training epoch 195: decode layer difference: 92.37003153121466\n",
      "autoencoder training epoch 196: decode layer difference: 92.37003153121466\n",
      "autoencoder training epoch 197: decode layer difference: 92.29113739770153\n",
      "autoencoder training epoch 198: decode layer difference: 92.37003153121466\n",
      "autoencoder training epoch 199: decode layer difference: 92.37003153121466\n",
      "autoencoder training epoch 200: decode layer difference: 92.37003153121466\n",
      "training epoch 1: error: 1.6957805340635033\n",
      "training epoch 2: error: 1.4690331942382189\n",
      "training epoch 3: error: 1.7309406004772958\n",
      "training epoch 4: error: 1.559159584835271\n",
      "training epoch 5: error: 1.4723225048343391\n",
      "training epoch 6: error: 1.4776322931840202\n",
      "training epoch 7: error: 1.699784615451593\n",
      "training epoch 8: error: 2.9858083138729037\n",
      "training epoch 9: error: 1.742392175188824\n",
      "training epoch 10: error: 1.4050805341626549\n",
      "training epoch 11: error: 1.7335761440316357\n",
      "training epoch 12: error: 1.420161640071492\n",
      "training epoch 13: error: 1.5150976985417848\n",
      "training epoch 14: error: 1.397724889338365\n",
      "training epoch 15: error: 1.2791352681047625\n",
      "training epoch 16: error: 1.2829699723422678\n",
      "training epoch 17: error: 2.19932238997089\n",
      "training epoch 18: error: 1.2476419741826836\n",
      "training epoch 19: error: 0.9986372634751808\n",
      "training epoch 20: error: 0.8843063820166712\n",
      "\n",
      "fold 4: mse: 0.44129239335684756\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 108.79936141015759\n",
      "autoencoder training epoch 2: decode layer difference: 108.79936141015759\n",
      "autoencoder training epoch 3: decode layer difference: 108.79936141015759\n",
      "autoencoder training epoch 4: decode layer difference: 108.79936141015759\n",
      "autoencoder training epoch 5: decode layer difference: 108.79936141015759\n",
      "autoencoder training epoch 6: decode layer difference: 108.79936141015759\n",
      "autoencoder training epoch 7: decode layer difference: 108.79936141015759\n",
      "autoencoder training epoch 8: decode layer difference: 108.79936141015759\n",
      "autoencoder training epoch 9: decode layer difference: 108.79936141015759\n",
      "autoencoder training epoch 10: decode layer difference: 108.79936141015759\n",
      "autoencoder training epoch 11: decode layer difference: 108.79936141015759\n",
      "autoencoder training epoch 12: decode layer difference: 108.79936141015759\n",
      "autoencoder training epoch 13: decode layer difference: 108.79936141015759\n",
      "autoencoder training epoch 14: decode layer difference: 108.79936141015759\n",
      "autoencoder training epoch 15: decode layer difference: 108.79936141015759\n",
      "autoencoder training epoch 16: decode layer difference: 108.79936141015759\n",
      "autoencoder training epoch 17: decode layer difference: 108.79936141015759\n",
      "autoencoder training epoch 18: decode layer difference: 108.79936141015759\n",
      "autoencoder training epoch 19: decode layer difference: 108.79936141015759\n",
      "autoencoder training epoch 20: decode layer difference: 108.79936141015759\n",
      "autoencoder training epoch 21: decode layer difference: 108.79936141015759\n",
      "autoencoder training epoch 22: decode layer difference: 104.79936141015759\n",
      "autoencoder training epoch 23: decode layer difference: 103.88125854667373\n",
      "autoencoder training epoch 24: decode layer difference: 104.69757486299005\n",
      "autoencoder training epoch 25: decode layer difference: 104.69757486299005\n",
      "autoencoder training epoch 26: decode layer difference: 104.57415542197114\n",
      "autoencoder training epoch 27: decode layer difference: 104.57515642297216\n",
      "autoencoder training epoch 28: decode layer difference: 104.29531825709218\n",
      "autoencoder training epoch 29: decode layer difference: 104.29531825709218\n",
      "autoencoder training epoch 30: decode layer difference: 104.54531825709218\n",
      "autoencoder training epoch 31: decode layer difference: 104.54531825709218\n",
      "autoencoder training epoch 32: decode layer difference: 103.62421239060532\n",
      "autoencoder training epoch 33: decode layer difference: 103.75021339160634\n",
      "autoencoder training epoch 34: decode layer difference: 103.74921239060532\n",
      "autoencoder training epoch 35: decode layer difference: 103.87421239060532\n",
      "autoencoder training epoch 36: decode layer difference: 103.87421239060532\n",
      "autoencoder training epoch 37: decode layer difference: 103.87521339160634\n",
      "autoencoder training epoch 38: decode layer difference: 103.87421239060532\n",
      "autoencoder training epoch 39: decode layer difference: 103.87421239060532\n",
      "autoencoder training epoch 40: decode layer difference: 103.87521339160634\n",
      "autoencoder training epoch 41: decode layer difference: 103.87421239060532\n",
      "autoencoder training epoch 42: decode layer difference: 103.87521339160634\n",
      "autoencoder training epoch 43: decode layer difference: 103.87521339160634\n",
      "autoencoder training epoch 44: decode layer difference: 103.87521339160634\n",
      "autoencoder training epoch 45: decode layer difference: 103.87421239060532\n",
      "autoencoder training epoch 46: decode layer difference: 103.87521339160634\n",
      "autoencoder training epoch 47: decode layer difference: 103.87521339160634\n",
      "autoencoder training epoch 48: decode layer difference: 103.87521339160634\n",
      "autoencoder training epoch 49: decode layer difference: 103.87521339160634\n",
      "autoencoder training epoch 50: decode layer difference: 102.87421239060532\n",
      "autoencoder training epoch 51: decode layer difference: 102.87521339160634\n",
      "autoencoder training epoch 52: decode layer difference: 102.87521339160634\n",
      "autoencoder training epoch 53: decode layer difference: 102.87421239060532\n",
      "autoencoder training epoch 54: decode layer difference: 102.059898076291\n",
      "autoencoder training epoch 55: decode layer difference: 102.059898076291\n",
      "autoencoder training epoch 56: decode layer difference: 102.06089907729202\n",
      "autoencoder training epoch 57: decode layer difference: 102.06089907729202\n",
      "autoencoder training epoch 58: decode layer difference: 102.06089907729202\n",
      "autoencoder training epoch 59: decode layer difference: 101.93589907729202\n",
      "autoencoder training epoch 60: decode layer difference: 101.93589907729202\n",
      "autoencoder training epoch 61: decode layer difference: 102.18715032854327\n",
      "autoencoder training epoch 62: decode layer difference: 101.93589907729202\n",
      "autoencoder training epoch 63: decode layer difference: 101.93589907729202\n",
      "autoencoder training epoch 64: decode layer difference: 102.06215032854327\n",
      "autoencoder training epoch 65: decode layer difference: 101.81290308530805\n",
      "autoencoder training epoch 66: decode layer difference: 101.81290308530805\n",
      "autoencoder training epoch 67: decode layer difference: 102.0641543365593\n",
      "autoencoder training epoch 68: decode layer difference: 101.81290308530805\n",
      "autoencoder training epoch 69: decode layer difference: 101.81290308530805\n",
      "autoencoder training epoch 70: decode layer difference: 101.81290308530805\n",
      "autoencoder training epoch 71: decode layer difference: 102.0641543365593\n",
      "autoencoder training epoch 72: decode layer difference: 101.81290308530805\n",
      "autoencoder training epoch 73: decode layer difference: 102.0641543365593\n",
      "autoencoder training epoch 74: decode layer difference: 102.0641543365593\n",
      "autoencoder training epoch 75: decode layer difference: 102.0641543365593\n",
      "autoencoder training epoch 76: decode layer difference: 102.0631533355583\n",
      "autoencoder training epoch 77: decode layer difference: 101.8131533355583\n",
      "autoencoder training epoch 78: decode layer difference: 101.8131533355583\n",
      "autoencoder training epoch 79: decode layer difference: 101.8131533355583\n",
      "autoencoder training epoch 80: decode layer difference: 101.8121523345573\n",
      "autoencoder training epoch 81: decode layer difference: 101.8121523345573\n",
      "autoencoder training epoch 82: decode layer difference: 101.8121523345573\n",
      "autoencoder training epoch 83: decode layer difference: 101.8121523345573\n",
      "autoencoder training epoch 84: decode layer difference: 101.8121523345573\n",
      "autoencoder training epoch 85: decode layer difference: 101.8121523345573\n",
      "autoencoder training epoch 86: decode layer difference: 101.8121523345573\n",
      "autoencoder training epoch 87: decode layer difference: 101.8121523345573\n",
      "autoencoder training epoch 88: decode layer difference: 101.8121523345573\n",
      "autoencoder training epoch 89: decode layer difference: 101.8121523345573\n",
      "autoencoder training epoch 90: decode layer difference: 102.8121523345573\n",
      "autoencoder training epoch 91: decode layer difference: 102.8121523345573\n",
      "autoencoder training epoch 92: decode layer difference: 102.8121523345573\n",
      "autoencoder training epoch 93: decode layer difference: 102.8121523345573\n",
      "autoencoder training epoch 94: decode layer difference: 102.8121523345573\n",
      "autoencoder training epoch 95: decode layer difference: 102.8121523345573\n",
      "autoencoder training epoch 96: decode layer difference: 102.8121523345573\n",
      "autoencoder training epoch 97: decode layer difference: 102.35760688001184\n",
      "autoencoder training epoch 98: decode layer difference: 102.35860788101284\n",
      "autoencoder training epoch 99: decode layer difference: 102.35860788101284\n",
      "autoencoder training epoch 100: decode layer difference: 101.90406242646739\n",
      "autoencoder training epoch 101: decode layer difference: 101.90406242646739\n",
      "autoencoder training epoch 102: decode layer difference: 101.90406242646739\n",
      "autoencoder training epoch 103: decode layer difference: 101.90406242646739\n",
      "autoencoder training epoch 104: decode layer difference: 101.90406242646739\n",
      "autoencoder training epoch 105: decode layer difference: 101.90406242646739\n",
      "autoencoder training epoch 106: decode layer difference: 101.7222442446492\n",
      "autoencoder training epoch 107: decode layer difference: 101.7222442446492\n",
      "autoencoder training epoch 108: decode layer difference: 101.54042606283102\n",
      "autoencoder training epoch 109: decode layer difference: 101.54042606283102\n",
      "autoencoder training epoch 110: decode layer difference: 101.54042606283102\n",
      "autoencoder training epoch 111: decode layer difference: 101.54042606283102\n",
      "autoencoder training epoch 112: decode layer difference: 101.54042606283102\n",
      "autoencoder training epoch 113: decode layer difference: 101.28917481157976\n",
      "autoencoder training epoch 114: decode layer difference: 101.28917481157976\n",
      "autoencoder training epoch 115: decode layer difference: 101.66417481157976\n",
      "autoencoder training epoch 116: decode layer difference: 101.66417481157976\n",
      "autoencoder training epoch 117: decode layer difference: 101.41417481157976\n",
      "autoencoder training epoch 118: decode layer difference: 100.29217781458277\n",
      "autoencoder training epoch 119: decode layer difference: 100.53917481157976\n",
      "autoencoder training epoch 120: decode layer difference: 100.53917481157976\n",
      "autoencoder training epoch 121: decode layer difference: 100.53917481157976\n",
      "autoencoder training epoch 122: decode layer difference: 100.54217781458277\n",
      "autoencoder training epoch 123: decode layer difference: 100.79217781458277\n",
      "autoencoder training epoch 124: decode layer difference: 100.53917481157976\n",
      "autoencoder training epoch 125: decode layer difference: 97.53917481157976\n",
      "autoencoder training epoch 126: decode layer difference: 97.54217781458277\n",
      "autoencoder training epoch 127: decode layer difference: 97.53917481157976\n",
      "autoencoder training epoch 128: decode layer difference: 97.53917481157976\n",
      "autoencoder training epoch 129: decode layer difference: 97.53917481157976\n",
      "autoencoder training epoch 130: decode layer difference: 97.54217781458277\n",
      "autoencoder training epoch 131: decode layer difference: 97.54217781458277\n",
      "autoencoder training epoch 132: decode layer difference: 97.54217781458277\n",
      "autoencoder training epoch 133: decode layer difference: 97.29217781458277\n",
      "autoencoder training epoch 134: decode layer difference: 96.13392380656673\n",
      "autoencoder training epoch 135: decode layer difference: 96.13292280556573\n",
      "autoencoder training epoch 136: decode layer difference: 96.67138434402727\n",
      "autoencoder training epoch 137: decode layer difference: 96.13492480756773\n",
      "autoencoder training epoch 138: decode layer difference: 96.67138434402727\n",
      "autoencoder training epoch 139: decode layer difference: 96.67138434402727\n",
      "autoencoder training epoch 140: decode layer difference: 96.67138434402727\n",
      "autoencoder training epoch 141: decode layer difference: 96.6753903540453\n",
      "autoencoder training epoch 142: decode layer difference: 96.67338634602928\n",
      "autoencoder training epoch 143: decode layer difference: 96.6753903540453\n",
      "autoencoder training epoch 144: decode layer difference: 96.67338634602928\n",
      "autoencoder training epoch 145: decode layer difference: 96.67338634602928\n",
      "autoencoder training epoch 146: decode layer difference: 96.67138434402727\n",
      "autoencoder training epoch 147: decode layer difference: 96.6753903540453\n",
      "autoencoder training epoch 148: decode layer difference: 96.44261711526005\n",
      "autoencoder training epoch 149: decode layer difference: 96.6753903540453\n",
      "autoencoder training epoch 150: decode layer difference: 96.31962112327608\n",
      "autoencoder training epoch 151: decode layer difference: 96.42338634602928\n",
      "autoencoder training epoch 152: decode layer difference: 96.67338634602928\n",
      "autoencoder training epoch 153: decode layer difference: 96.4273923560473\n",
      "autoencoder training epoch 154: decode layer difference: 96.19462112327608\n",
      "autoencoder training epoch 155: decode layer difference: 96.4273923560473\n",
      "autoencoder training epoch 156: decode layer difference: 96.19662312527808\n",
      "autoencoder training epoch 157: decode layer difference: 96.4273923560473\n",
      "autoencoder training epoch 158: decode layer difference: 96.43039736506434\n",
      "autoencoder training epoch 159: decode layer difference: 96.19662312527808\n",
      "autoencoder training epoch 160: decode layer difference: 96.19762412627908\n",
      "autoencoder training epoch 161: decode layer difference: 96.19762412627908\n",
      "autoencoder training epoch 162: decode layer difference: 96.19762412627908\n",
      "autoencoder training epoch 163: decode layer difference: 96.19962813429511\n",
      "autoencoder training epoch 164: decode layer difference: 96.4263913550463\n",
      "autoencoder training epoch 165: decode layer difference: 96.19762412627908\n",
      "autoencoder training epoch 166: decode layer difference: 96.4273923560473\n",
      "autoencoder training epoch 167: decode layer difference: 96.43039736506434\n",
      "autoencoder training epoch 168: decode layer difference: 96.43039736506434\n",
      "autoencoder training epoch 169: decode layer difference: 96.42638934903228\n",
      "autoencoder training epoch 170: decode layer difference: 96.19962813429511\n",
      "autoencoder training epoch 171: decode layer difference: 96.19962813429511\n",
      "autoencoder training epoch 172: decode layer difference: 96.19762412627908\n",
      "autoencoder training epoch 173: decode layer difference: 96.32462813429511\n",
      "autoencoder training epoch 174: decode layer difference: 96.19762412627908\n",
      "autoencoder training epoch 175: decode layer difference: 96.19962813429511\n",
      "autoencoder training epoch 176: decode layer difference: 96.19962813429511\n",
      "autoencoder training epoch 177: decode layer difference: 96.19962813429511\n",
      "autoencoder training epoch 178: decode layer difference: 96.20163214231114\n",
      "autoencoder training epoch 179: decode layer difference: 96.42839335704831\n",
      "autoencoder training epoch 180: decode layer difference: 96.42638934903228\n",
      "autoencoder training epoch 181: decode layer difference: 96.43039736506434\n",
      "autoencoder training epoch 182: decode layer difference: 96.19962813429511\n",
      "autoencoder training epoch 183: decode layer difference: 96.43039736506434\n",
      "autoencoder training epoch 184: decode layer difference: 96.43039736506434\n",
      "autoencoder training epoch 185: decode layer difference: 96.19962813429511\n",
      "autoencoder training epoch 186: decode layer difference: 96.19962813429511\n",
      "autoencoder training epoch 187: decode layer difference: 96.19962813429511\n",
      "autoencoder training epoch 188: decode layer difference: 96.19962813429511\n",
      "autoencoder training epoch 189: decode layer difference: 96.19962813429511\n",
      "autoencoder training epoch 190: decode layer difference: 96.43039736506434\n",
      "autoencoder training epoch 191: decode layer difference: 96.19962813429511\n",
      "autoencoder training epoch 192: decode layer difference: 96.20363615032718\n",
      "autoencoder training epoch 193: decode layer difference: 96.20163214231115\n",
      "autoencoder training epoch 194: decode layer difference: 96.20764416635924\n",
      "autoencoder training epoch 195: decode layer difference: 96.19962813429511\n",
      "autoencoder training epoch 196: decode layer difference: 96.20163214231115\n",
      "autoencoder training epoch 197: decode layer difference: 96.20764416635924\n",
      "autoencoder training epoch 198: decode layer difference: 96.20764416635924\n",
      "autoencoder training epoch 199: decode layer difference: 96.20764416635924\n",
      "autoencoder training epoch 200: decode layer difference: 96.20764416635924\n",
      "training epoch 1: error: 1.6620313155150888\n",
      "training epoch 2: error: 1.6579441513103799\n",
      "training epoch 3: error: 2.441734879269783\n",
      "training epoch 4: error: 1.4947908393547467\n",
      "training epoch 5: error: 1.411201966594775\n",
      "training epoch 6: error: 1.4796113823636736\n",
      "training epoch 7: error: 1.706083203250745\n",
      "training epoch 8: error: 1.5555103505259789\n",
      "training epoch 9: error: 1.628595202853193\n",
      "training epoch 10: error: 3.507510333530025\n",
      "training epoch 11: error: 1.4322263267769801\n",
      "training epoch 12: error: 3.441414328628995\n",
      "training epoch 13: error: 1.4955722044734265\n",
      "training epoch 14: error: 1.6660348883010667\n",
      "training epoch 15: error: 1.475552822527757\n",
      "training epoch 16: error: 7.2963211308252856\n",
      "training epoch 17: error: 1.2624811787778687\n",
      "training epoch 18: error: 2.358158808868054\n",
      "training epoch 19: error: 1.0819616053882033\n",
      "training epoch 20: error: 1.0980818671997783\n",
      "\n",
      "fold 5: mse: 0.52706929127166\n",
      "\n",
      "average error: 0.32828978003769055\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.32828978003769055"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_layered(machine_data, 'PRP', train_autoencoder_network_regression, (5,10), z_norm=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1: error: 1.714037596688974\n",
      "training epoch 2: error: 1.7576812420134116\n",
      "training epoch 3: error: 1.8215859370876284\n",
      "training epoch 4: error: 1.9837295244263453\n",
      "training epoch 5: error: 3.474980996136825\n",
      "training epoch 6: error: 2.4502974017584203\n",
      "training epoch 7: error: 1.7712054051085566\n",
      "training epoch 8: error: 1.7463779785680777\n",
      "training epoch 9: error: 1.6688157123452234\n",
      "training epoch 10: error: 1.8123432318520887\n",
      "training epoch 11: error: 1.527382969579361\n",
      "training epoch 12: error: 1.6419640479766455\n",
      "training epoch 13: error: 1.6571342144804482\n",
      "training epoch 14: error: 1.516998452378821\n",
      "training epoch 15: error: 1.656411732827283\n",
      "training epoch 16: error: 1.542024524997142\n",
      "training epoch 17: error: 1.6907106780982368\n",
      "training epoch 18: error: 1.6667437454797058\n",
      "training epoch 19: error: 1.5090140718462923\n",
      "training epoch 20: error: 1.5866090834600075\n",
      "\n",
      "fold 1: mse: 0.48635591727493066\n",
      "\n",
      "training epoch 1: error: 1.9034274263923323\n",
      "training epoch 2: error: 1.676506288329774\n",
      "training epoch 3: error: 1.4706583179343715\n",
      "training epoch 4: error: 1.9597366129582687\n",
      "training epoch 5: error: 1.5937729741796145\n",
      "training epoch 6: error: 1.7134578501191307\n",
      "training epoch 7: error: 1.595970310945078\n",
      "training epoch 8: error: 1.4249975290453576\n",
      "training epoch 9: error: 1.5615923033793522\n",
      "training epoch 10: error: 1.7928110951341771\n",
      "training epoch 11: error: 1.4796568497072005\n",
      "training epoch 12: error: 1.5274826373404116\n",
      "training epoch 13: error: 1.4543150192045653\n",
      "training epoch 14: error: 1.7471526950604108\n",
      "training epoch 15: error: 1.5131392214231956\n",
      "training epoch 16: error: 1.420083383290144\n",
      "training epoch 17: error: 1.4719180344902498\n",
      "training epoch 18: error: 3.402312260962187\n",
      "training epoch 19: error: 1.6228619619131264\n",
      "training epoch 20: error: 1.5500114163351641\n",
      "\n",
      "fold 2: mse: 0.6565841978383057\n",
      "\n",
      "training epoch 1: error: 2.2600828270163227\n",
      "training epoch 2: error: 2.1072186564311295\n",
      "training epoch 3: error: 2.215827616455438\n",
      "training epoch 4: error: 2.2265999620924424\n",
      "training epoch 5: error: 1.923227563297878\n",
      "training epoch 6: error: 1.8635829124737349\n",
      "training epoch 7: error: 1.8822540842421365\n",
      "training epoch 8: error: 1.8567713954477556\n",
      "training epoch 9: error: 2.6954900471163574\n",
      "training epoch 10: error: 2.072242454779023\n",
      "training epoch 11: error: 2.0700258293114993\n",
      "training epoch 12: error: 2.3978758230579444\n",
      "training epoch 13: error: 1.8792230959245133\n",
      "training epoch 14: error: 2.283066291690681\n",
      "training epoch 15: error: 2.0173924174405764\n",
      "training epoch 16: error: 1.86405959759739\n",
      "training epoch 17: error: 2.1344160212804835\n",
      "training epoch 18: error: 2.0944855376535214\n",
      "training epoch 19: error: 2.1686161309164538\n",
      "training epoch 20: error: 5.713189715253964\n",
      "\n",
      "fold 3: mse: 1.1977464464237368\n",
      "\n",
      "training epoch 1: error: 1.6971548560069034\n",
      "training epoch 2: error: 17.158568124429102\n",
      "training epoch 3: error: 1.6927652750872997\n",
      "training epoch 4: error: 1.7871629390255013\n",
      "training epoch 5: error: 1.5979803586867927\n",
      "training epoch 6: error: 1.725004523066194\n",
      "training epoch 7: error: 1.845352528094014\n",
      "training epoch 8: error: 1.6276628254651482\n",
      "training epoch 9: error: 1.5921820015905417\n",
      "training epoch 10: error: 1.5972838007219012\n",
      "training epoch 11: error: 1.7563466013798827\n",
      "training epoch 12: error: 1.647849004907219\n",
      "training epoch 13: error: 1.7577801007635228\n",
      "training epoch 14: error: 1.6385667034172577\n",
      "training epoch 15: error: 1.6909762971402005\n",
      "training epoch 16: error: 1.7370179331654039\n",
      "training epoch 17: error: 1.6345574478139424\n",
      "training epoch 18: error: 2.607053907646371\n",
      "training epoch 19: error: 1.5983168168395014\n",
      "training epoch 20: error: 1.8578608278007678\n",
      "\n",
      "fold 4: mse: 0.5218108586888857\n",
      "\n",
      "training epoch 1: error: 1.9556441226396575\n",
      "training epoch 2: error: 1.962801218118325\n",
      "training epoch 3: error: 1.7623944770293056\n",
      "training epoch 4: error: 1.8966716110661412\n",
      "training epoch 5: error: 1.6630218888570605\n",
      "training epoch 6: error: 1.8937133791531842\n",
      "training epoch 7: error: 1.67736326672279\n",
      "training epoch 8: error: 1.7715434438434032\n",
      "training epoch 9: error: 1.829771219937299\n",
      "training epoch 10: error: 1.9033664363620977\n",
      "training epoch 11: error: 2.2031990848639973\n",
      "training epoch 12: error: 1.7021662653269771\n",
      "training epoch 13: error: 1.6629226882293535\n",
      "training epoch 14: error: 1.87570325618691\n",
      "training epoch 15: error: 1.7679857050165715\n",
      "training epoch 16: error: 1.6694112402855938\n",
      "training epoch 17: error: 2.016776575330535\n",
      "training epoch 18: error: 1.702926086485002\n",
      "training epoch 19: error: 1.8500295170279064\n",
      "training epoch 20: error: 1.6797740989005534\n",
      "\n",
      "fold 5: mse: 0.34663518611786104\n",
      "\n",
      "average error: 0.641826521268744\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.641826521268744"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_layered(machine_data, 'PRP', train_layered_regression_network, (5,10), z_norm=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1: mse: 2.373676146223765\n",
      "training epoch 2: mse: 2.173380401770946\n",
      "training epoch 3: mse: 1.9929555702321546\n",
      "training epoch 4: mse: 1.8310052883117551\n",
      "training epoch 5: mse: 1.6852107185349587\n",
      "training epoch 6: mse: 1.5536622903118562\n",
      "training epoch 7: mse: 1.435610134808471\n",
      "training epoch 8: mse: 1.3292847097706997\n",
      "training epoch 9: mse: 1.233478901963772\n",
      "training epoch 10: mse: 1.1474386697776158\n",
      "training epoch 11: mse: 1.0696218812074374\n",
      "training epoch 12: mse: 0.9997363241938064\n",
      "training epoch 13: mse: 0.9369413766419477\n",
      "training epoch 14: mse: 0.880428880708636\n",
      "training epoch 15: mse: 0.8292010613195842\n",
      "training epoch 16: mse: 0.7836177088961525\n",
      "training epoch 17: mse: 0.7423278890576037\n",
      "training epoch 18: mse: 0.704929985455976\n",
      "training epoch 19: mse: 0.6714020173123809\n",
      "training epoch 20: mse: 0.6409792637920231\n",
      "\n",
      "fold 1: mse: 0.052045326329703524\n",
      "\n",
      "training epoch 1: mse: 2.4021994047345396\n",
      "training epoch 2: mse: 2.2178377074292976\n",
      "training epoch 3: mse: 2.050431191269114\n",
      "training epoch 4: mse: 1.8979653934891227\n",
      "training epoch 5: mse: 1.7592062175833592\n",
      "training epoch 6: mse: 1.6326769042851899\n",
      "training epoch 7: mse: 1.5181722432351525\n",
      "training epoch 8: mse: 1.4137397570822878\n",
      "training epoch 9: mse: 1.318417126956202\n",
      "training epoch 10: mse: 1.231815089444618\n",
      "training epoch 11: mse: 1.1528268199891536\n",
      "training epoch 12: mse: 1.0807562751320263\n",
      "training epoch 13: mse: 1.0150590682938498\n",
      "training epoch 14: mse: 0.9552339689862427\n",
      "training epoch 15: mse: 0.9011931571158194\n",
      "training epoch 16: mse: 0.8518003944230128\n",
      "training epoch 17: mse: 0.8067438082512853\n",
      "training epoch 18: mse: 0.7656830483497422\n",
      "training epoch 19: mse: 0.7282550836132456\n",
      "training epoch 20: mse: 0.6941872465761783\n",
      "\n",
      "fold 2: mse: 0.040700956013152476\n",
      "\n",
      "training epoch 1: mse: 1.9244052194622001\n",
      "training epoch 2: mse: 1.787753068426286\n",
      "training epoch 3: mse: 1.6631104554649117\n",
      "training epoch 4: mse: 1.5492050364644865\n",
      "training epoch 5: mse: 1.4453713585966446\n",
      "training epoch 6: mse: 1.3507897521235102\n",
      "training epoch 7: mse: 1.2641245447115517\n",
      "training epoch 8: mse: 1.185133782978176\n",
      "training epoch 9: mse: 1.113211637853248\n",
      "training epoch 10: mse: 1.0474864494266147\n",
      "training epoch 11: mse: 0.9874924375936662\n",
      "training epoch 12: mse: 0.9326230022650265\n",
      "training epoch 13: mse: 0.8824402217960061\n",
      "training epoch 14: mse: 0.836925373264253\n",
      "training epoch 15: mse: 0.7952065288001028\n",
      "training epoch 16: mse: 0.757038602052909\n",
      "training epoch 17: mse: 0.7223805523948028\n",
      "training epoch 18: mse: 0.6905797713653945\n",
      "training epoch 19: mse: 0.6614873796111408\n",
      "training epoch 20: mse: 0.6351065999418168\n",
      "\n",
      "fold 3: mse: 0.17779648978153745\n",
      "\n",
      "training epoch 1: mse: 2.299840410972551\n",
      "training epoch 2: mse: 2.1156690953997384\n",
      "training epoch 3: mse: 1.9486888988897537\n",
      "training epoch 4: mse: 1.7982010921075013\n",
      "training epoch 5: mse: 1.661995370317996\n",
      "training epoch 6: mse: 1.5392524060885968\n",
      "training epoch 7: mse: 1.4283414701785135\n",
      "training epoch 8: mse: 1.3278992878909226\n",
      "training epoch 9: mse: 1.2373157576354838\n",
      "training epoch 10: mse: 1.155331290236714\n",
      "training epoch 11: mse: 1.0815192171180983\n",
      "training epoch 12: mse: 1.014533512241944\n",
      "training epoch 13: mse: 0.9540295201456817\n",
      "training epoch 14: mse: 0.8994338186864157\n",
      "training epoch 15: mse: 0.8501554867248071\n",
      "training epoch 16: mse: 0.8054900671312257\n",
      "training epoch 17: mse: 0.7649888734453356\n",
      "training epoch 18: mse: 0.7283393515592738\n",
      "training epoch 19: mse: 0.6950625202354723\n",
      "training epoch 20: mse: 0.6650833189122722\n",
      "\n",
      "fold 4: mse: 0.06011275407777686\n",
      "\n",
      "training epoch 1: mse: 1.3637621686535721\n",
      "training epoch 2: mse: 1.2762440833195625\n",
      "training epoch 3: mse: 1.195318836605599\n",
      "training epoch 4: mse: 1.1205889012536345\n",
      "training epoch 5: mse: 1.0516782539964318\n",
      "training epoch 6: mse: 0.987865089331881\n",
      "training epoch 7: mse: 0.929060396833773\n",
      "training epoch 8: mse: 0.874558498247927\n",
      "training epoch 9: mse: 0.8242978887990469\n",
      "training epoch 10: mse: 0.7777758233927277\n",
      "training epoch 11: mse: 0.7348330233633119\n",
      "training epoch 12: mse: 0.6951899404653011\n",
      "training epoch 13: mse: 0.6585292917043633\n",
      "training epoch 14: mse: 0.6245706143796277\n",
      "training epoch 15: mse: 0.5932582536267967\n",
      "training epoch 16: mse: 0.5642639664017205\n",
      "training epoch 17: mse: 0.5375396150947626\n",
      "training epoch 18: mse: 0.5127493805569564\n",
      "training epoch 19: mse: 0.4898780335171357\n",
      "training epoch 20: mse: 0.46870564293457045\n",
      "\n",
      "fold 5: mse: 0.6014112177234429\n",
      "\n",
      "average error: 0.18641334878512264\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.18641334878512264"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation(machine_data, 'PRP', z_norm=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_data = load_forest_fires('datasets/forestfires.data')\n",
    "forest_data.name = 'forest fires'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "         FFMC       DMC        DC       ISI      temp        RH      wind  \\\n0    0.870968  0.086492  0.101325  0.090909  0.192926  0.423529  0.700000   \n1    0.927742  0.118194  0.775419  0.119430  0.508039  0.211765  0.055556   \n2    0.927742  0.146795  0.796294  0.119430  0.398714  0.211765  0.100000   \n3    0.941935  0.110958  0.081623  0.160428  0.196141  0.964706  0.400000   \n4    0.910968  0.172984  0.110590  0.171123  0.295820  0.988235  0.155556   \n..        ...       ...       ...       ...       ...       ...       ...   \n512  0.811613  0.191592  0.771315  0.033868  0.823151  0.200000  0.255556   \n513  0.811613  0.191592  0.771315  0.033868  0.633441  0.658824  0.600000   \n514  0.811613  0.191592  0.771315  0.033868  0.610932  0.647059  0.700000   \n515  0.976774  0.499311  0.711622  0.201426  0.752412  0.317647  0.400000   \n516  0.784516  0.006547  0.115867  0.019608  0.308682  0.188235  0.455556   \n\n        rain      area  \n0    0.00000  0.000000  \n1    0.00000  0.000000  \n2    0.00000  0.000000  \n3    0.03125  0.000000  \n4    0.00000  0.000000  \n..       ...       ...  \n512  0.00000  0.005904  \n513  0.00000  0.049769  \n514  0.00000  0.010231  \n515  0.00000  0.000000  \n516  0.00000  0.000000  \n\n[517 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FFMC</th>\n      <th>DMC</th>\n      <th>DC</th>\n      <th>ISI</th>\n      <th>temp</th>\n      <th>RH</th>\n      <th>wind</th>\n      <th>rain</th>\n      <th>area</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.870968</td>\n      <td>0.086492</td>\n      <td>0.101325</td>\n      <td>0.090909</td>\n      <td>0.192926</td>\n      <td>0.423529</td>\n      <td>0.700000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.927742</td>\n      <td>0.118194</td>\n      <td>0.775419</td>\n      <td>0.119430</td>\n      <td>0.508039</td>\n      <td>0.211765</td>\n      <td>0.055556</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.927742</td>\n      <td>0.146795</td>\n      <td>0.796294</td>\n      <td>0.119430</td>\n      <td>0.398714</td>\n      <td>0.211765</td>\n      <td>0.100000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.941935</td>\n      <td>0.110958</td>\n      <td>0.081623</td>\n      <td>0.160428</td>\n      <td>0.196141</td>\n      <td>0.964706</td>\n      <td>0.400000</td>\n      <td>0.03125</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.910968</td>\n      <td>0.172984</td>\n      <td>0.110590</td>\n      <td>0.171123</td>\n      <td>0.295820</td>\n      <td>0.988235</td>\n      <td>0.155556</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>512</th>\n      <td>0.811613</td>\n      <td>0.191592</td>\n      <td>0.771315</td>\n      <td>0.033868</td>\n      <td>0.823151</td>\n      <td>0.200000</td>\n      <td>0.255556</td>\n      <td>0.00000</td>\n      <td>0.005904</td>\n    </tr>\n    <tr>\n      <th>513</th>\n      <td>0.811613</td>\n      <td>0.191592</td>\n      <td>0.771315</td>\n      <td>0.033868</td>\n      <td>0.633441</td>\n      <td>0.658824</td>\n      <td>0.600000</td>\n      <td>0.00000</td>\n      <td>0.049769</td>\n    </tr>\n    <tr>\n      <th>514</th>\n      <td>0.811613</td>\n      <td>0.191592</td>\n      <td>0.771315</td>\n      <td>0.033868</td>\n      <td>0.610932</td>\n      <td>0.647059</td>\n      <td>0.700000</td>\n      <td>0.00000</td>\n      <td>0.010231</td>\n    </tr>\n    <tr>\n      <th>515</th>\n      <td>0.976774</td>\n      <td>0.499311</td>\n      <td>0.711622</td>\n      <td>0.201426</td>\n      <td>0.752412</td>\n      <td>0.317647</td>\n      <td>0.400000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>516</th>\n      <td>0.784516</td>\n      <td>0.006547</td>\n      <td>0.115867</td>\n      <td>0.019608</td>\n      <td>0.308682</td>\n      <td>0.188235</td>\n      <td>0.455556</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>517 rows  9 columns</p>\n</div>"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoencoder training epoch 1: decode layer difference: 818.909644207064\n",
      "autoencoder training epoch 2: decode layer difference: 818.909644207064\n",
      "autoencoder training epoch 3: decode layer difference: 818.909644207064\n",
      "autoencoder training epoch 4: decode layer difference: 716.456156604484\n",
      "autoencoder training epoch 5: decode layer difference: 704.1571392045396\n",
      "autoencoder training epoch 6: decode layer difference: 700.0435062251194\n",
      "autoencoder training epoch 7: decode layer difference: 689.1460730313595\n",
      "autoencoder training epoch 8: decode layer difference: 668.8834138501643\n",
      "autoencoder training epoch 9: decode layer difference: 663.5668066701393\n",
      "autoencoder training epoch 10: decode layer difference: 658.7843057500233\n",
      "autoencoder training epoch 11: decode layer difference: 648.6072978898484\n",
      "autoencoder training epoch 12: decode layer difference: 639.9898785951032\n",
      "autoencoder training epoch 13: decode layer difference: 636.3074554644556\n",
      "autoencoder training epoch 14: decode layer difference: 635.618689780214\n",
      "autoencoder training epoch 15: decode layer difference: 635.989165410695\n",
      "autoencoder training epoch 16: decode layer difference: 635.1374394384242\n",
      "autoencoder training epoch 17: decode layer difference: 635.2354982694252\n",
      "autoencoder training epoch 18: decode layer difference: 635.1755817627356\n",
      "autoencoder training epoch 19: decode layer difference: 634.7750902528088\n",
      "autoencoder training epoch 20: decode layer difference: 634.4243881680973\n",
      "autoencoder training epoch 21: decode layer difference: 634.7812711055254\n",
      "autoencoder training epoch 22: decode layer difference: 634.9854810505318\n",
      "autoencoder training epoch 23: decode layer difference: 634.7978781996752\n",
      "autoencoder training epoch 24: decode layer difference: 634.8773754307676\n",
      "autoencoder training epoch 25: decode layer difference: 635.2632503662994\n",
      "autoencoder training epoch 26: decode layer difference: 635.4518149227457\n",
      "autoencoder training epoch 27: decode layer difference: 634.0445514109654\n",
      "autoencoder training epoch 28: decode layer difference: 634.418772188496\n",
      "autoencoder training epoch 29: decode layer difference: 634.5631026320661\n",
      "autoencoder training epoch 30: decode layer difference: 634.4738789425753\n",
      "autoencoder training epoch 31: decode layer difference: 634.6303373334724\n",
      "autoencoder training epoch 32: decode layer difference: 634.5619012894822\n",
      "autoencoder training epoch 33: decode layer difference: 634.5171666458946\n",
      "autoencoder training epoch 34: decode layer difference: 635.6660314544044\n",
      "autoencoder training epoch 35: decode layer difference: 634.7766547444087\n",
      "autoencoder training epoch 36: decode layer difference: 634.7011920377474\n",
      "autoencoder training epoch 37: decode layer difference: 634.9096893650554\n",
      "autoencoder training epoch 38: decode layer difference: 634.7816940726191\n",
      "autoencoder training epoch 39: decode layer difference: 634.5779784599004\n",
      "autoencoder training epoch 40: decode layer difference: 634.4646571616872\n",
      "autoencoder training epoch 41: decode layer difference: 634.7462924016038\n",
      "autoencoder training epoch 42: decode layer difference: 634.6027088461369\n",
      "autoencoder training epoch 43: decode layer difference: 635.9459883019829\n",
      "autoencoder training epoch 44: decode layer difference: 634.829063752004\n",
      "autoencoder training epoch 45: decode layer difference: 635.2695674919016\n",
      "autoencoder training epoch 46: decode layer difference: 634.7148557891157\n",
      "autoencoder training epoch 47: decode layer difference: 634.4861114472401\n",
      "autoencoder training epoch 48: decode layer difference: 634.6283711202213\n",
      "autoencoder training epoch 49: decode layer difference: 634.9706918720028\n",
      "autoencoder training epoch 50: decode layer difference: 635.8632038746433\n",
      "autoencoder training epoch 51: decode layer difference: 635.8004179063821\n",
      "autoencoder training epoch 52: decode layer difference: 635.2593494996258\n",
      "autoencoder training epoch 53: decode layer difference: 635.4933650035166\n",
      "autoencoder training epoch 54: decode layer difference: 634.633884559229\n",
      "autoencoder training epoch 55: decode layer difference: 635.5698768747604\n",
      "autoencoder training epoch 56: decode layer difference: 634.471774394091\n",
      "autoencoder training epoch 57: decode layer difference: 635.4517155861931\n",
      "autoencoder training epoch 58: decode layer difference: 635.1900656752566\n",
      "autoencoder training epoch 59: decode layer difference: 635.6582382754884\n",
      "autoencoder training epoch 60: decode layer difference: 635.2900458302863\n",
      "autoencoder training epoch 61: decode layer difference: 635.0092778188222\n",
      "autoencoder training epoch 62: decode layer difference: 635.5425641673955\n",
      "autoencoder training epoch 63: decode layer difference: 635.1992506526003\n",
      "autoencoder training epoch 64: decode layer difference: 634.7328376889448\n",
      "autoencoder training epoch 65: decode layer difference: 635.5538199155915\n",
      "autoencoder training epoch 66: decode layer difference: 635.0804978454868\n",
      "autoencoder training epoch 67: decode layer difference: 635.5161193854929\n",
      "autoencoder training epoch 68: decode layer difference: 635.2141592693106\n",
      "autoencoder training epoch 69: decode layer difference: 635.7288824952199\n",
      "autoencoder training epoch 70: decode layer difference: 634.9982739977838\n",
      "autoencoder training epoch 71: decode layer difference: 635.4167894005559\n",
      "autoencoder training epoch 72: decode layer difference: 634.9890488725259\n",
      "autoencoder training epoch 73: decode layer difference: 636.445617406292\n",
      "autoencoder training epoch 74: decode layer difference: 635.9271089935627\n",
      "autoencoder training epoch 75: decode layer difference: 635.5554953561675\n",
      "autoencoder training epoch 76: decode layer difference: 635.0333015354713\n",
      "autoencoder training epoch 77: decode layer difference: 634.6062037967919\n",
      "autoencoder training epoch 78: decode layer difference: 635.1155834338953\n",
      "autoencoder training epoch 79: decode layer difference: 634.8935211263067\n",
      "autoencoder training epoch 80: decode layer difference: 635.4501844692003\n",
      "autoencoder training epoch 81: decode layer difference: 635.5740048898645\n",
      "autoencoder training epoch 82: decode layer difference: 635.3758973003978\n",
      "autoencoder training epoch 83: decode layer difference: 635.4814821154763\n",
      "autoencoder training epoch 84: decode layer difference: 635.7500745464808\n",
      "autoencoder training epoch 85: decode layer difference: 635.6926569374775\n",
      "autoencoder training epoch 86: decode layer difference: 635.0498978636624\n",
      "autoencoder training epoch 87: decode layer difference: 635.3055496300312\n",
      "autoencoder training epoch 88: decode layer difference: 635.0019070152515\n",
      "autoencoder training epoch 89: decode layer difference: 634.79211095881\n",
      "autoencoder training epoch 90: decode layer difference: 635.4281128184333\n",
      "autoencoder training epoch 91: decode layer difference: 635.3292161075731\n",
      "autoencoder training epoch 92: decode layer difference: 634.7836063616568\n",
      "autoencoder training epoch 93: decode layer difference: 635.5410443366063\n",
      "autoencoder training epoch 94: decode layer difference: 635.0050963258229\n",
      "autoencoder training epoch 95: decode layer difference: 634.8066749837769\n",
      "autoencoder training epoch 96: decode layer difference: 634.6907680422222\n",
      "autoencoder training epoch 97: decode layer difference: 634.9145868684682\n",
      "autoencoder training epoch 98: decode layer difference: 635.4056631524865\n",
      "autoencoder training epoch 99: decode layer difference: 635.7091863279657\n",
      "autoencoder training epoch 100: decode layer difference: 635.5227729494957\n",
      "autoencoder training epoch 101: decode layer difference: 634.9186897876671\n",
      "autoencoder training epoch 102: decode layer difference: 635.1628315943312\n",
      "autoencoder training epoch 103: decode layer difference: 634.8895468890014\n",
      "autoencoder training epoch 104: decode layer difference: 634.7935012093767\n",
      "autoencoder training epoch 105: decode layer difference: 634.9951956636351\n",
      "autoencoder training epoch 106: decode layer difference: 635.201850151089\n",
      "autoencoder training epoch 107: decode layer difference: 635.0727606087421\n",
      "autoencoder training epoch 108: decode layer difference: 635.1914960583474\n",
      "autoencoder training epoch 109: decode layer difference: 635.7050371550364\n",
      "autoencoder training epoch 110: decode layer difference: 634.6570172754139\n",
      "autoencoder training epoch 111: decode layer difference: 634.9678455596063\n",
      "autoencoder training epoch 112: decode layer difference: 635.1516083254703\n",
      "autoencoder training epoch 113: decode layer difference: 635.4457574553828\n",
      "autoencoder training epoch 114: decode layer difference: 635.438850732576\n",
      "autoencoder training epoch 115: decode layer difference: 635.0342108460627\n",
      "autoencoder training epoch 116: decode layer difference: 634.6618537462434\n",
      "autoencoder training epoch 117: decode layer difference: 634.9711507813008\n",
      "autoencoder training epoch 118: decode layer difference: 634.8639129583557\n",
      "autoencoder training epoch 119: decode layer difference: 636.1637427294926\n",
      "autoencoder training epoch 120: decode layer difference: 635.9094883313583\n",
      "autoencoder training epoch 121: decode layer difference: 635.2742117090779\n",
      "autoencoder training epoch 122: decode layer difference: 634.4655374773503\n",
      "autoencoder training epoch 123: decode layer difference: 634.970156678113\n",
      "autoencoder training epoch 124: decode layer difference: 635.2761559565756\n",
      "autoencoder training epoch 125: decode layer difference: 635.0001978806098\n",
      "autoencoder training epoch 126: decode layer difference: 635.4227238674476\n",
      "autoencoder training epoch 127: decode layer difference: 635.3720912038555\n",
      "autoencoder training epoch 128: decode layer difference: 635.0792802751622\n",
      "autoencoder training epoch 129: decode layer difference: 635.3076051702901\n",
      "autoencoder training epoch 130: decode layer difference: 634.9679463695368\n",
      "autoencoder training epoch 131: decode layer difference: 634.5333218731103\n",
      "autoencoder training epoch 132: decode layer difference: 635.0076162676589\n",
      "autoencoder training epoch 133: decode layer difference: 634.9617816258267\n",
      "autoencoder training epoch 134: decode layer difference: 635.0863351869227\n",
      "autoencoder training epoch 135: decode layer difference: 635.1652164936504\n",
      "autoencoder training epoch 136: decode layer difference: 635.8049224983858\n",
      "autoencoder training epoch 137: decode layer difference: 634.5130333777427\n",
      "autoencoder training epoch 138: decode layer difference: 634.5130333777427\n",
      "autoencoder training epoch 139: decode layer difference: 634.2189157306839\n",
      "autoencoder training epoch 140: decode layer difference: 635.2296201532702\n",
      "autoencoder training epoch 141: decode layer difference: 634.9457725746036\n",
      "autoencoder training epoch 142: decode layer difference: 635.2055791941292\n",
      "autoencoder training epoch 143: decode layer difference: 634.8570962958113\n",
      "autoencoder training epoch 144: decode layer difference: 634.7582088774567\n",
      "autoencoder training epoch 145: decode layer difference: 634.5886045498906\n",
      "autoencoder training epoch 146: decode layer difference: 634.8676417245729\n",
      "autoencoder training epoch 147: decode layer difference: 634.7404023378714\n",
      "autoencoder training epoch 148: decode layer difference: 635.5267349862265\n",
      "autoencoder training epoch 149: decode layer difference: 634.646823305236\n",
      "autoencoder training epoch 150: decode layer difference: 634.8299774237239\n",
      "autoencoder training epoch 151: decode layer difference: 634.5110096105216\n",
      "autoencoder training epoch 152: decode layer difference: 634.3284953309594\n",
      "autoencoder training epoch 153: decode layer difference: 634.6844162162517\n",
      "autoencoder training epoch 154: decode layer difference: 635.0489414725096\n",
      "autoencoder training epoch 155: decode layer difference: 634.4620117045547\n",
      "autoencoder training epoch 156: decode layer difference: 634.3410059727075\n",
      "autoencoder training epoch 157: decode layer difference: 634.3174069584035\n",
      "autoencoder training epoch 158: decode layer difference: 634.5935564203621\n",
      "autoencoder training epoch 159: decode layer difference: 636.1722737729406\n",
      "autoencoder training epoch 160: decode layer difference: 634.8033872773451\n",
      "autoencoder training epoch 161: decode layer difference: 634.682383775994\n",
      "autoencoder training epoch 162: decode layer difference: 634.7429558290382\n",
      "autoencoder training epoch 163: decode layer difference: 634.7992826684556\n",
      "autoencoder training epoch 164: decode layer difference: 634.2230203395732\n",
      "autoencoder training epoch 165: decode layer difference: 636.3999808906085\n",
      "autoencoder training epoch 166: decode layer difference: 634.3667395981222\n",
      "autoencoder training epoch 167: decode layer difference: 634.4675533979162\n",
      "autoencoder training epoch 168: decode layer difference: 635.9207405334705\n",
      "autoencoder training epoch 169: decode layer difference: 634.9060903351483\n",
      "autoencoder training epoch 170: decode layer difference: 634.3245949926034\n",
      "autoencoder training epoch 171: decode layer difference: 634.8313613520191\n",
      "autoencoder training epoch 172: decode layer difference: 634.5381575112051\n",
      "autoencoder training epoch 173: decode layer difference: 635.9571679624713\n",
      "autoencoder training epoch 174: decode layer difference: 634.1209208502597\n",
      "autoencoder training epoch 175: decode layer difference: 634.4212915641442\n",
      "autoencoder training epoch 176: decode layer difference: 634.6388308595851\n",
      "autoencoder training epoch 177: decode layer difference: 635.594384061911\n",
      "autoencoder training epoch 178: decode layer difference: 635.1133171576915\n",
      "autoencoder training epoch 179: decode layer difference: 634.8430997282478\n",
      "autoencoder training epoch 180: decode layer difference: 634.9097514435371\n",
      "autoencoder training epoch 181: decode layer difference: 636.1710647802693\n",
      "autoencoder training epoch 182: decode layer difference: 634.3008178108944\n",
      "autoencoder training epoch 183: decode layer difference: 634.7806412587693\n",
      "autoencoder training epoch 184: decode layer difference: 634.3731052521515\n",
      "autoencoder training epoch 185: decode layer difference: 634.6496898752295\n",
      "autoencoder training epoch 186: decode layer difference: 634.6746129900562\n",
      "autoencoder training epoch 187: decode layer difference: 635.1700138477836\n",
      "autoencoder training epoch 188: decode layer difference: 634.7060707332096\n",
      "autoencoder training epoch 189: decode layer difference: 635.0090531664231\n",
      "autoencoder training epoch 190: decode layer difference: 635.8748513385103\n",
      "autoencoder training epoch 191: decode layer difference: 635.6553381448523\n",
      "autoencoder training epoch 192: decode layer difference: 636.4784694753082\n",
      "autoencoder training epoch 193: decode layer difference: 635.460926927348\n",
      "autoencoder training epoch 194: decode layer difference: 636.6294505622948\n",
      "autoencoder training epoch 195: decode layer difference: 634.8372717764291\n",
      "autoencoder training epoch 196: decode layer difference: 634.5352575585068\n",
      "autoencoder training epoch 197: decode layer difference: 635.594384061911\n",
      "autoencoder training epoch 198: decode layer difference: 634.7937409750942\n",
      "autoencoder training epoch 199: decode layer difference: 634.4763028217051\n",
      "autoencoder training epoch 200: decode layer difference: 635.5301121181756\n",
      "training epoch 1: error: 0.8657868966601241\n",
      "training epoch 2: error: 0.8655895735982322\n",
      "training epoch 3: error: 0.8821472865124262\n",
      "training epoch 4: error: 0.8955562372250137\n",
      "training epoch 5: error: 0.8987416603329663\n",
      "training epoch 6: error: 1.0630847341806684\n",
      "training epoch 7: error: 0.8599895638281407\n",
      "training epoch 8: error: 0.8913476252469901\n",
      "training epoch 9: error: 0.8694220573762266\n",
      "training epoch 10: error: 0.8985333234225367\n",
      "training epoch 11: error: 0.8829999996990923\n",
      "training epoch 12: error: 0.8947786117528352\n",
      "training epoch 13: error: 0.8789597077038321\n",
      "training epoch 14: error: 0.8767669064305699\n",
      "training epoch 15: error: 17.312785604683526\n",
      "training epoch 16: error: 0.8755337007800665\n",
      "training epoch 17: error: 0.8646785836972128\n",
      "training epoch 18: error: 0.8900965743987868\n",
      "training epoch 19: error: 0.926169106297966\n",
      "training epoch 20: error: 1.050586649256715\n",
      "\n",
      "fold 1: mse: 0.08976618814647458\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 814.4218790435121\n",
      "autoencoder training epoch 2: decode layer difference: 814.4218790435121\n",
      "autoencoder training epoch 3: decode layer difference: 814.4218790435121\n",
      "autoencoder training epoch 4: decode layer difference: 715.2963153076589\n",
      "autoencoder training epoch 5: decode layer difference: 700.7298254618744\n",
      "autoencoder training epoch 6: decode layer difference: 706.751071962791\n",
      "autoencoder training epoch 7: decode layer difference: 698.3114459926718\n",
      "autoencoder training epoch 8: decode layer difference: 687.4690900425514\n",
      "autoencoder training epoch 9: decode layer difference: 665.2141413117374\n",
      "autoencoder training epoch 10: decode layer difference: 655.1183041286092\n",
      "autoencoder training epoch 11: decode layer difference: 648.2424361711428\n",
      "autoencoder training epoch 12: decode layer difference: 640.1914339155246\n",
      "autoencoder training epoch 13: decode layer difference: 638.7002576209138\n",
      "autoencoder training epoch 14: decode layer difference: 637.7981956774846\n",
      "autoencoder training epoch 15: decode layer difference: 637.41014003294\n",
      "autoencoder training epoch 16: decode layer difference: 636.8336960471647\n",
      "autoencoder training epoch 17: decode layer difference: 636.2558172076489\n",
      "autoencoder training epoch 18: decode layer difference: 635.7327002290021\n",
      "autoencoder training epoch 19: decode layer difference: 635.0207956174179\n",
      "autoencoder training epoch 20: decode layer difference: 634.6923323089429\n",
      "autoencoder training epoch 21: decode layer difference: 635.1526230893628\n",
      "autoencoder training epoch 22: decode layer difference: 635.2000845071294\n",
      "autoencoder training epoch 23: decode layer difference: 635.4017724731343\n",
      "autoencoder training epoch 24: decode layer difference: 635.5100833934978\n",
      "autoencoder training epoch 25: decode layer difference: 635.2967496515362\n",
      "autoencoder training epoch 26: decode layer difference: 635.1095300141385\n",
      "autoencoder training epoch 27: decode layer difference: 635.5705087329045\n",
      "autoencoder training epoch 28: decode layer difference: 635.5191203974755\n",
      "autoencoder training epoch 29: decode layer difference: 635.7049695448782\n",
      "autoencoder training epoch 30: decode layer difference: 636.3495639337058\n",
      "autoencoder training epoch 31: decode layer difference: 635.271706241417\n",
      "autoencoder training epoch 32: decode layer difference: 635.8128984065877\n",
      "autoencoder training epoch 33: decode layer difference: 635.6381142646683\n",
      "autoencoder training epoch 34: decode layer difference: 635.8816234968152\n",
      "autoencoder training epoch 35: decode layer difference: 635.5343115626597\n",
      "autoencoder training epoch 36: decode layer difference: 636.2056124385944\n",
      "autoencoder training epoch 37: decode layer difference: 635.2896909106216\n",
      "autoencoder training epoch 38: decode layer difference: 636.0339810629723\n",
      "autoencoder training epoch 39: decode layer difference: 635.994801562411\n",
      "autoencoder training epoch 40: decode layer difference: 636.8700206061906\n",
      "autoencoder training epoch 41: decode layer difference: 635.0889767939295\n",
      "autoencoder training epoch 42: decode layer difference: 636.6101485333595\n",
      "autoencoder training epoch 43: decode layer difference: 635.6060575339229\n",
      "autoencoder training epoch 44: decode layer difference: 636.0552702884427\n",
      "autoencoder training epoch 45: decode layer difference: 634.9156237760328\n",
      "autoencoder training epoch 46: decode layer difference: 634.1732394616528\n",
      "autoencoder training epoch 47: decode layer difference: 633.9563728820999\n",
      "autoencoder training epoch 48: decode layer difference: 634.4502806858138\n",
      "autoencoder training epoch 49: decode layer difference: 634.4527924186222\n",
      "autoencoder training epoch 50: decode layer difference: 634.5601996595608\n",
      "autoencoder training epoch 51: decode layer difference: 635.1066988522748\n",
      "autoencoder training epoch 52: decode layer difference: 635.1703227703526\n",
      "autoencoder training epoch 53: decode layer difference: 634.4890357787782\n",
      "autoencoder training epoch 54: decode layer difference: 633.9400554653259\n",
      "autoencoder training epoch 55: decode layer difference: 633.7270922604321\n",
      "autoencoder training epoch 56: decode layer difference: 637.448057133751\n",
      "autoencoder training epoch 57: decode layer difference: 635.1170368098622\n",
      "autoencoder training epoch 58: decode layer difference: 633.6545449568107\n",
      "autoencoder training epoch 59: decode layer difference: 633.8308063885675\n",
      "autoencoder training epoch 60: decode layer difference: 634.6239119798378\n",
      "autoencoder training epoch 61: decode layer difference: 633.9778635855728\n",
      "autoencoder training epoch 62: decode layer difference: 634.5401246818677\n",
      "autoencoder training epoch 63: decode layer difference: 634.0479015571161\n",
      "autoencoder training epoch 64: decode layer difference: 634.13808612131\n",
      "autoencoder training epoch 65: decode layer difference: 634.1292288829732\n",
      "autoencoder training epoch 66: decode layer difference: 634.008371012017\n",
      "autoencoder training epoch 67: decode layer difference: 635.1180550779308\n",
      "autoencoder training epoch 68: decode layer difference: 635.1004591018154\n",
      "autoencoder training epoch 69: decode layer difference: 634.0550253365082\n",
      "autoencoder training epoch 70: decode layer difference: 635.3750923401354\n",
      "autoencoder training epoch 71: decode layer difference: 634.3851177774962\n",
      "autoencoder training epoch 72: decode layer difference: 635.1445149171726\n",
      "autoencoder training epoch 73: decode layer difference: 636.0854341756562\n",
      "autoencoder training epoch 74: decode layer difference: 634.2667687873106\n",
      "autoencoder training epoch 75: decode layer difference: 634.4886546158441\n",
      "autoencoder training epoch 76: decode layer difference: 635.420401558922\n",
      "autoencoder training epoch 77: decode layer difference: 635.2151054654048\n",
      "autoencoder training epoch 78: decode layer difference: 633.8100428685709\n",
      "autoencoder training epoch 79: decode layer difference: 634.2820149170348\n",
      "autoencoder training epoch 80: decode layer difference: 635.0782025228251\n",
      "autoencoder training epoch 81: decode layer difference: 636.0604403613884\n",
      "autoencoder training epoch 82: decode layer difference: 633.7578651912404\n",
      "autoencoder training epoch 83: decode layer difference: 634.8531199394706\n",
      "autoencoder training epoch 84: decode layer difference: 635.5190165941622\n",
      "autoencoder training epoch 85: decode layer difference: 634.2121998235297\n",
      "autoencoder training epoch 86: decode layer difference: 634.6476235101692\n",
      "autoencoder training epoch 87: decode layer difference: 634.2092699682757\n",
      "autoencoder training epoch 88: decode layer difference: 633.7423797176864\n",
      "autoencoder training epoch 89: decode layer difference: 634.3124564772158\n",
      "autoencoder training epoch 90: decode layer difference: 635.0036697256451\n",
      "autoencoder training epoch 91: decode layer difference: 634.4746349031624\n",
      "autoencoder training epoch 92: decode layer difference: 635.2351555603614\n",
      "autoencoder training epoch 93: decode layer difference: 633.9473283387575\n",
      "autoencoder training epoch 94: decode layer difference: 634.6533657861634\n",
      "autoencoder training epoch 95: decode layer difference: 633.3752793489211\n",
      "autoencoder training epoch 96: decode layer difference: 633.913318638978\n",
      "autoencoder training epoch 97: decode layer difference: 634.5432795229291\n",
      "autoencoder training epoch 98: decode layer difference: 635.4641116602875\n",
      "autoencoder training epoch 99: decode layer difference: 635.0192787178064\n",
      "autoencoder training epoch 100: decode layer difference: 634.1968903696286\n",
      "autoencoder training epoch 101: decode layer difference: 634.8185388219605\n",
      "autoencoder training epoch 102: decode layer difference: 634.0263718447301\n",
      "autoencoder training epoch 103: decode layer difference: 634.5382339346093\n",
      "autoencoder training epoch 104: decode layer difference: 634.4107451360559\n",
      "autoencoder training epoch 105: decode layer difference: 633.924268710048\n",
      "autoencoder training epoch 106: decode layer difference: 634.4392554296413\n",
      "autoencoder training epoch 107: decode layer difference: 633.4925874596803\n",
      "autoencoder training epoch 108: decode layer difference: 634.0402405149176\n",
      "autoencoder training epoch 109: decode layer difference: 633.8025552382692\n",
      "autoencoder training epoch 110: decode layer difference: 634.6680491484713\n",
      "autoencoder training epoch 111: decode layer difference: 633.7949498236505\n",
      "autoencoder training epoch 112: decode layer difference: 634.0259847886509\n",
      "autoencoder training epoch 113: decode layer difference: 633.1902338286463\n",
      "autoencoder training epoch 114: decode layer difference: 635.0339869351374\n",
      "autoencoder training epoch 115: decode layer difference: 633.3005334952288\n",
      "autoencoder training epoch 116: decode layer difference: 634.2976565639037\n",
      "autoencoder training epoch 117: decode layer difference: 633.8471038824295\n",
      "autoencoder training epoch 118: decode layer difference: 634.4922432841253\n",
      "autoencoder training epoch 119: decode layer difference: 633.1388547077737\n",
      "autoencoder training epoch 120: decode layer difference: 634.5338936400897\n",
      "autoencoder training epoch 121: decode layer difference: 641.0916008258855\n",
      "autoencoder training epoch 122: decode layer difference: 633.256767026375\n",
      "autoencoder training epoch 123: decode layer difference: 633.4735830153481\n",
      "autoencoder training epoch 124: decode layer difference: 633.7594776497338\n",
      "autoencoder training epoch 125: decode layer difference: 633.9355108132299\n",
      "autoencoder training epoch 126: decode layer difference: 634.7031925663198\n",
      "autoencoder training epoch 127: decode layer difference: 633.762418101293\n",
      "autoencoder training epoch 128: decode layer difference: 633.547728266594\n",
      "autoencoder training epoch 129: decode layer difference: 633.471500850038\n",
      "autoencoder training epoch 130: decode layer difference: 633.4691953106716\n",
      "autoencoder training epoch 131: decode layer difference: 634.2324273564482\n",
      "autoencoder training epoch 132: decode layer difference: 634.7872127866364\n",
      "autoencoder training epoch 133: decode layer difference: 633.8514387938677\n",
      "autoencoder training epoch 134: decode layer difference: 633.0000566895479\n",
      "autoencoder training epoch 135: decode layer difference: 633.9072897525749\n",
      "autoencoder training epoch 136: decode layer difference: 633.2627170198376\n",
      "autoencoder training epoch 137: decode layer difference: 634.3338475901453\n",
      "autoencoder training epoch 138: decode layer difference: 632.2934795683628\n",
      "autoencoder training epoch 139: decode layer difference: 633.3146480043882\n",
      "autoencoder training epoch 140: decode layer difference: 633.1143546304088\n",
      "autoencoder training epoch 141: decode layer difference: 634.2337807549197\n",
      "autoencoder training epoch 142: decode layer difference: 633.2751218620095\n",
      "autoencoder training epoch 143: decode layer difference: 632.1665421050205\n",
      "autoencoder training epoch 144: decode layer difference: 633.1129966956294\n",
      "autoencoder training epoch 145: decode layer difference: 633.289159694282\n",
      "autoencoder training epoch 146: decode layer difference: 632.8265575102178\n",
      "autoencoder training epoch 147: decode layer difference: 632.5098744168156\n",
      "autoencoder training epoch 148: decode layer difference: 633.1027679012509\n",
      "autoencoder training epoch 149: decode layer difference: 632.808080322987\n",
      "autoencoder training epoch 150: decode layer difference: 633.6228450682042\n",
      "autoencoder training epoch 151: decode layer difference: 634.059256563168\n",
      "autoencoder training epoch 152: decode layer difference: 633.8391722866472\n",
      "autoencoder training epoch 153: decode layer difference: 632.5987635183262\n",
      "autoencoder training epoch 154: decode layer difference: 632.9445119436411\n",
      "autoencoder training epoch 155: decode layer difference: 633.0316963431502\n",
      "autoencoder training epoch 156: decode layer difference: 633.2217712316126\n",
      "autoencoder training epoch 157: decode layer difference: 632.7912064569825\n",
      "autoencoder training epoch 158: decode layer difference: 633.0687069780995\n",
      "autoencoder training epoch 159: decode layer difference: 632.7468160484812\n",
      "autoencoder training epoch 160: decode layer difference: 633.0425922116121\n",
      "autoencoder training epoch 161: decode layer difference: 632.3988191608055\n",
      "autoencoder training epoch 162: decode layer difference: 632.5974267489275\n",
      "autoencoder training epoch 163: decode layer difference: 632.1850723751642\n",
      "autoencoder training epoch 164: decode layer difference: 633.0220121564437\n",
      "autoencoder training epoch 165: decode layer difference: 632.5248657295398\n",
      "autoencoder training epoch 166: decode layer difference: 632.2488777091583\n",
      "autoencoder training epoch 167: decode layer difference: 633.01597460779\n",
      "autoencoder training epoch 168: decode layer difference: 632.9944430054645\n",
      "autoencoder training epoch 169: decode layer difference: 632.8104541926199\n",
      "autoencoder training epoch 170: decode layer difference: 633.0735307174539\n",
      "autoencoder training epoch 171: decode layer difference: 632.5099899056368\n",
      "autoencoder training epoch 172: decode layer difference: 634.3167184285539\n",
      "autoencoder training epoch 173: decode layer difference: 633.4637775206561\n",
      "autoencoder training epoch 174: decode layer difference: 633.1888821959317\n",
      "autoencoder training epoch 175: decode layer difference: 633.7237282172334\n",
      "autoencoder training epoch 176: decode layer difference: 633.0191231794659\n",
      "autoencoder training epoch 177: decode layer difference: 632.9055327899325\n",
      "autoencoder training epoch 178: decode layer difference: 633.3695770427165\n",
      "autoencoder training epoch 179: decode layer difference: 632.9403498184731\n",
      "autoencoder training epoch 180: decode layer difference: 632.3232461245617\n",
      "autoencoder training epoch 181: decode layer difference: 633.1942624656258\n",
      "autoencoder training epoch 182: decode layer difference: 633.1262371122843\n",
      "autoencoder training epoch 183: decode layer difference: 632.580191829316\n",
      "autoencoder training epoch 184: decode layer difference: 633.6648883243538\n",
      "autoencoder training epoch 185: decode layer difference: 633.1446926012738\n",
      "autoencoder training epoch 186: decode layer difference: 632.4139478327006\n",
      "autoencoder training epoch 187: decode layer difference: 632.9233563453273\n",
      "autoencoder training epoch 188: decode layer difference: 634.3212861019774\n",
      "autoencoder training epoch 189: decode layer difference: 633.4354727050575\n",
      "autoencoder training epoch 190: decode layer difference: 632.6599603542425\n",
      "autoencoder training epoch 191: decode layer difference: 633.6981086304461\n",
      "autoencoder training epoch 192: decode layer difference: 632.9055092142836\n",
      "autoencoder training epoch 193: decode layer difference: 632.7603616462616\n",
      "autoencoder training epoch 194: decode layer difference: 633.8165726534041\n",
      "autoencoder training epoch 195: decode layer difference: 632.3075137117742\n",
      "autoencoder training epoch 196: decode layer difference: 633.3818341031168\n",
      "autoencoder training epoch 197: decode layer difference: 633.21499036383\n",
      "autoencoder training epoch 198: decode layer difference: 633.5813482546955\n",
      "autoencoder training epoch 199: decode layer difference: 633.417444432707\n",
      "autoencoder training epoch 200: decode layer difference: 632.8038277615692\n",
      "training epoch 1: error: 0.8516224752165813\n",
      "training epoch 2: error: 0.8871365852122084\n",
      "training epoch 3: error: 0.862079520915165\n",
      "training epoch 4: error: 0.8533402611327827\n",
      "training epoch 5: error: 0.8558491985542143\n",
      "training epoch 6: error: 0.8861668074716892\n",
      "training epoch 7: error: 1.1898744151678766\n",
      "training epoch 8: error: 0.8496784351839148\n",
      "training epoch 9: error: 0.8695967867140502\n",
      "training epoch 10: error: 0.8767255944172216\n",
      "training epoch 11: error: 0.8836001544548355\n",
      "training epoch 12: error: 0.8719463457690213\n",
      "training epoch 13: error: 0.868434081552506\n",
      "training epoch 14: error: 0.8655567722783032\n",
      "training epoch 15: error: 0.8729709314325012\n",
      "training epoch 16: error: 0.8547361451872524\n",
      "training epoch 17: error: 0.850205877923401\n",
      "training epoch 18: error: 0.8583474348161368\n",
      "training epoch 19: error: 0.8685393013147779\n",
      "training epoch 20: error: 0.8706542207770556\n",
      "\n",
      "fold 2: mse: 0.02381077010089021\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 831.2027715112511\n",
      "autoencoder training epoch 2: decode layer difference: 850.3346043086788\n",
      "autoencoder training epoch 3: decode layer difference: 767.9727859575135\n",
      "autoencoder training epoch 4: decode layer difference: 715.9901818366463\n",
      "autoencoder training epoch 5: decode layer difference: 711.9936450684988\n",
      "autoencoder training epoch 6: decode layer difference: 708.1802585181167\n",
      "autoencoder training epoch 7: decode layer difference: 703.2423085937605\n",
      "autoencoder training epoch 8: decode layer difference: 683.1427671897986\n",
      "autoencoder training epoch 9: decode layer difference: 671.4654719624416\n",
      "autoencoder training epoch 10: decode layer difference: 667.9932031854214\n",
      "autoencoder training epoch 11: decode layer difference: 669.8731716487727\n",
      "autoencoder training epoch 12: decode layer difference: 658.1700679805163\n",
      "autoencoder training epoch 13: decode layer difference: 647.0591164932687\n",
      "autoencoder training epoch 14: decode layer difference: 642.5448793030032\n",
      "autoencoder training epoch 15: decode layer difference: 638.0213900952249\n",
      "autoencoder training epoch 16: decode layer difference: 638.2467996747812\n",
      "autoencoder training epoch 17: decode layer difference: 637.2735396775786\n",
      "autoencoder training epoch 18: decode layer difference: 637.482302729403\n",
      "autoencoder training epoch 19: decode layer difference: 636.276122301943\n",
      "autoencoder training epoch 20: decode layer difference: 636.5333372885507\n",
      "autoencoder training epoch 21: decode layer difference: 636.0638308622397\n",
      "autoencoder training epoch 22: decode layer difference: 636.5834512700228\n",
      "autoencoder training epoch 23: decode layer difference: 636.4704967774564\n",
      "autoencoder training epoch 24: decode layer difference: 636.4054217568199\n",
      "autoencoder training epoch 25: decode layer difference: 636.2492192382869\n",
      "autoencoder training epoch 26: decode layer difference: 637.0489840813873\n",
      "autoencoder training epoch 27: decode layer difference: 636.3355638791286\n",
      "autoencoder training epoch 28: decode layer difference: 636.9029955873059\n",
      "autoencoder training epoch 29: decode layer difference: 636.011054921923\n",
      "autoencoder training epoch 30: decode layer difference: 635.9685565812608\n",
      "autoencoder training epoch 31: decode layer difference: 635.94295708942\n",
      "autoencoder training epoch 32: decode layer difference: 636.0886866168722\n",
      "autoencoder training epoch 33: decode layer difference: 636.2214942280436\n",
      "autoencoder training epoch 34: decode layer difference: 638.0592581061018\n",
      "autoencoder training epoch 35: decode layer difference: 636.2288983924155\n",
      "autoencoder training epoch 36: decode layer difference: 636.5537960862928\n",
      "autoencoder training epoch 37: decode layer difference: 636.6362852190347\n",
      "autoencoder training epoch 38: decode layer difference: 636.3333903334003\n",
      "autoencoder training epoch 39: decode layer difference: 635.9266403926772\n",
      "autoencoder training epoch 40: decode layer difference: 637.1389166888589\n",
      "autoencoder training epoch 41: decode layer difference: 636.0590574323761\n",
      "autoencoder training epoch 42: decode layer difference: 636.7801130070991\n",
      "autoencoder training epoch 43: decode layer difference: 636.7870799098521\n",
      "autoencoder training epoch 44: decode layer difference: 636.5880345490443\n",
      "autoencoder training epoch 45: decode layer difference: 637.2289604133932\n",
      "autoencoder training epoch 46: decode layer difference: 635.9253680182093\n",
      "autoencoder training epoch 47: decode layer difference: 635.7676204452575\n",
      "autoencoder training epoch 48: decode layer difference: 636.9091625160473\n",
      "autoencoder training epoch 49: decode layer difference: 636.9620672065246\n",
      "autoencoder training epoch 50: decode layer difference: 636.8866569769248\n",
      "autoencoder training epoch 51: decode layer difference: 636.6475580908702\n",
      "autoencoder training epoch 52: decode layer difference: 636.9176021815115\n",
      "autoencoder training epoch 53: decode layer difference: 636.1205990952296\n",
      "autoencoder training epoch 54: decode layer difference: 637.1764879625168\n",
      "autoencoder training epoch 55: decode layer difference: 636.6768716072027\n",
      "autoencoder training epoch 56: decode layer difference: 637.5667752806278\n",
      "autoencoder training epoch 57: decode layer difference: 636.0476659914173\n",
      "autoencoder training epoch 58: decode layer difference: 636.8071123521332\n",
      "autoencoder training epoch 59: decode layer difference: 636.890713108633\n",
      "autoencoder training epoch 60: decode layer difference: 636.7652933106924\n",
      "autoencoder training epoch 61: decode layer difference: 636.8296545219386\n",
      "autoencoder training epoch 62: decode layer difference: 636.0600535010847\n",
      "autoencoder training epoch 63: decode layer difference: 637.4434289557422\n",
      "autoencoder training epoch 64: decode layer difference: 636.9977564552721\n",
      "autoencoder training epoch 65: decode layer difference: 636.7690592902795\n",
      "autoencoder training epoch 66: decode layer difference: 636.4935609801161\n",
      "autoencoder training epoch 67: decode layer difference: 634.8863646863293\n",
      "autoencoder training epoch 68: decode layer difference: 635.6131410970268\n",
      "autoencoder training epoch 69: decode layer difference: 635.8074068819151\n",
      "autoencoder training epoch 70: decode layer difference: 635.513125722104\n",
      "autoencoder training epoch 71: decode layer difference: 637.6723340878741\n",
      "autoencoder training epoch 72: decode layer difference: 635.8936916056537\n",
      "autoencoder training epoch 73: decode layer difference: 635.185577198893\n",
      "autoencoder training epoch 74: decode layer difference: 635.1748376973744\n",
      "autoencoder training epoch 75: decode layer difference: 635.2039788181718\n",
      "autoencoder training epoch 76: decode layer difference: 635.8522645537316\n",
      "autoencoder training epoch 77: decode layer difference: 636.4166912243078\n",
      "autoencoder training epoch 78: decode layer difference: 635.5310273216039\n",
      "autoencoder training epoch 79: decode layer difference: 635.6493952521043\n",
      "autoencoder training epoch 80: decode layer difference: 635.4965640502527\n",
      "autoencoder training epoch 81: decode layer difference: 635.8514163888674\n",
      "autoencoder training epoch 82: decode layer difference: 636.0186551037971\n",
      "autoencoder training epoch 83: decode layer difference: 635.6698442574371\n",
      "autoencoder training epoch 84: decode layer difference: 635.9360984010726\n",
      "autoencoder training epoch 85: decode layer difference: 635.630528572514\n",
      "autoencoder training epoch 86: decode layer difference: 635.7425859681052\n",
      "autoencoder training epoch 87: decode layer difference: 636.0271697155711\n",
      "autoencoder training epoch 88: decode layer difference: 636.1662857820729\n",
      "autoencoder training epoch 89: decode layer difference: 635.7886955877974\n",
      "autoencoder training epoch 90: decode layer difference: 634.8245876294363\n",
      "autoencoder training epoch 91: decode layer difference: 635.818271799835\n",
      "autoencoder training epoch 92: decode layer difference: 636.0747068913147\n",
      "autoencoder training epoch 93: decode layer difference: 636.9653279337338\n",
      "autoencoder training epoch 94: decode layer difference: 635.9222331701443\n",
      "autoencoder training epoch 95: decode layer difference: 635.5138154233946\n",
      "autoencoder training epoch 96: decode layer difference: 634.8647767576552\n",
      "autoencoder training epoch 97: decode layer difference: 635.4353300550788\n",
      "autoencoder training epoch 98: decode layer difference: 635.9219242378749\n",
      "autoencoder training epoch 99: decode layer difference: 636.2369914916791\n",
      "autoencoder training epoch 100: decode layer difference: 636.1739896969132\n",
      "autoencoder training epoch 101: decode layer difference: 635.7098062136467\n",
      "autoencoder training epoch 102: decode layer difference: 635.1174577206727\n",
      "autoencoder training epoch 103: decode layer difference: 636.9826208857102\n",
      "autoencoder training epoch 104: decode layer difference: 635.4563258552732\n",
      "autoencoder training epoch 105: decode layer difference: 635.7850652324204\n",
      "autoencoder training epoch 106: decode layer difference: 636.3004824353536\n",
      "autoencoder training epoch 107: decode layer difference: 635.8863548594707\n",
      "autoencoder training epoch 108: decode layer difference: 636.2693631104383\n",
      "autoencoder training epoch 109: decode layer difference: 635.5792907282596\n",
      "autoencoder training epoch 110: decode layer difference: 636.7070595325762\n",
      "autoencoder training epoch 111: decode layer difference: 636.3493641322638\n",
      "autoencoder training epoch 112: decode layer difference: 636.3206622723096\n",
      "autoencoder training epoch 113: decode layer difference: 636.3713866738922\n",
      "autoencoder training epoch 114: decode layer difference: 635.1291937677647\n",
      "autoencoder training epoch 115: decode layer difference: 635.5946904605819\n",
      "autoencoder training epoch 116: decode layer difference: 636.0745621595576\n",
      "autoencoder training epoch 117: decode layer difference: 635.6304063857166\n",
      "autoencoder training epoch 118: decode layer difference: 635.66202455279\n",
      "autoencoder training epoch 119: decode layer difference: 637.7823220659914\n",
      "autoencoder training epoch 120: decode layer difference: 635.2771013676667\n",
      "autoencoder training epoch 121: decode layer difference: 636.9097440928792\n",
      "autoencoder training epoch 122: decode layer difference: 636.1681928862507\n",
      "autoencoder training epoch 123: decode layer difference: 636.3245093826188\n",
      "autoencoder training epoch 124: decode layer difference: 635.5407518793654\n",
      "autoencoder training epoch 125: decode layer difference: 635.5833328031217\n",
      "autoencoder training epoch 126: decode layer difference: 634.9998032580977\n",
      "autoencoder training epoch 127: decode layer difference: 635.5664055629657\n",
      "autoencoder training epoch 128: decode layer difference: 635.7489596123426\n",
      "autoencoder training epoch 129: decode layer difference: 635.227006188335\n",
      "autoencoder training epoch 130: decode layer difference: 635.9311695440665\n",
      "autoencoder training epoch 131: decode layer difference: 635.2303578345774\n",
      "autoencoder training epoch 132: decode layer difference: 635.4837433317416\n",
      "autoencoder training epoch 133: decode layer difference: 635.423374621872\n",
      "autoencoder training epoch 134: decode layer difference: 635.9006729444999\n",
      "autoencoder training epoch 135: decode layer difference: 635.9229022514226\n",
      "autoencoder training epoch 136: decode layer difference: 636.3373223882567\n",
      "autoencoder training epoch 137: decode layer difference: 635.1233865794009\n",
      "autoencoder training epoch 138: decode layer difference: 636.2690849067299\n",
      "autoencoder training epoch 139: decode layer difference: 635.2816196805942\n",
      "autoencoder training epoch 140: decode layer difference: 635.6625843217678\n",
      "autoencoder training epoch 141: decode layer difference: 635.4201195680182\n",
      "autoencoder training epoch 142: decode layer difference: 635.5065373042889\n",
      "autoencoder training epoch 143: decode layer difference: 635.127992376161\n",
      "autoencoder training epoch 144: decode layer difference: 635.4033474367579\n",
      "autoencoder training epoch 145: decode layer difference: 636.0523186402943\n",
      "autoencoder training epoch 146: decode layer difference: 635.1369461469982\n",
      "autoencoder training epoch 147: decode layer difference: 635.3207624241863\n",
      "autoencoder training epoch 148: decode layer difference: 635.5064879368998\n",
      "autoencoder training epoch 149: decode layer difference: 635.57378654274\n",
      "autoencoder training epoch 150: decode layer difference: 635.8367584279997\n",
      "autoencoder training epoch 151: decode layer difference: 635.8228924774482\n",
      "autoencoder training epoch 152: decode layer difference: 635.4127932604878\n",
      "autoencoder training epoch 153: decode layer difference: 635.964335376106\n",
      "autoencoder training epoch 154: decode layer difference: 635.2950915624724\n",
      "autoencoder training epoch 155: decode layer difference: 636.0562385201199\n",
      "autoencoder training epoch 156: decode layer difference: 635.6250211361488\n",
      "autoencoder training epoch 157: decode layer difference: 635.8785550254943\n",
      "autoencoder training epoch 158: decode layer difference: 635.9331609346996\n",
      "autoencoder training epoch 159: decode layer difference: 635.9341209646539\n",
      "autoencoder training epoch 160: decode layer difference: 635.708824172516\n",
      "autoencoder training epoch 161: decode layer difference: 635.2150454681844\n",
      "autoencoder training epoch 162: decode layer difference: 635.7802102636923\n",
      "autoencoder training epoch 163: decode layer difference: 635.3890042814194\n",
      "autoencoder training epoch 164: decode layer difference: 635.4264478478437\n",
      "autoencoder training epoch 165: decode layer difference: 636.0227724338812\n",
      "autoencoder training epoch 166: decode layer difference: 635.3732290390194\n",
      "autoencoder training epoch 167: decode layer difference: 635.5014866095951\n",
      "autoencoder training epoch 168: decode layer difference: 635.386361578476\n",
      "autoencoder training epoch 169: decode layer difference: 635.8796112214222\n",
      "autoencoder training epoch 170: decode layer difference: 634.8489680086266\n",
      "autoencoder training epoch 171: decode layer difference: 636.0463298613126\n",
      "autoencoder training epoch 172: decode layer difference: 635.6836846131487\n",
      "autoencoder training epoch 173: decode layer difference: 636.0223581345633\n",
      "autoencoder training epoch 174: decode layer difference: 635.5677987459844\n",
      "autoencoder training epoch 175: decode layer difference: 636.1084999001816\n",
      "autoencoder training epoch 176: decode layer difference: 636.0623300604375\n",
      "autoencoder training epoch 177: decode layer difference: 634.5491887294504\n",
      "autoencoder training epoch 178: decode layer difference: 635.2275269036154\n",
      "autoencoder training epoch 179: decode layer difference: 636.4821079046862\n",
      "autoencoder training epoch 180: decode layer difference: 634.7297890669246\n",
      "autoencoder training epoch 181: decode layer difference: 635.3784958321005\n",
      "autoencoder training epoch 182: decode layer difference: 634.4004454063679\n",
      "autoencoder training epoch 183: decode layer difference: 635.6322643214862\n",
      "autoencoder training epoch 184: decode layer difference: 635.6972965730828\n",
      "autoencoder training epoch 185: decode layer difference: 635.0200608283211\n",
      "autoencoder training epoch 186: decode layer difference: 636.026769985102\n",
      "autoencoder training epoch 187: decode layer difference: 635.2843853706172\n",
      "autoencoder training epoch 188: decode layer difference: 634.5570198712705\n",
      "autoencoder training epoch 189: decode layer difference: 635.2363189573472\n",
      "autoencoder training epoch 190: decode layer difference: 634.6636079023286\n",
      "autoencoder training epoch 191: decode layer difference: 635.1037410329405\n",
      "autoencoder training epoch 192: decode layer difference: 635.7206606371967\n",
      "autoencoder training epoch 193: decode layer difference: 635.5225784271747\n",
      "autoencoder training epoch 194: decode layer difference: 634.8362277925215\n",
      "autoencoder training epoch 195: decode layer difference: 634.8818656380374\n",
      "autoencoder training epoch 196: decode layer difference: 636.3444051571023\n",
      "autoencoder training epoch 197: decode layer difference: 634.6194148499842\n",
      "autoencoder training epoch 198: decode layer difference: 636.0594939831523\n",
      "autoencoder training epoch 199: decode layer difference: 635.0193011622464\n",
      "autoencoder training epoch 200: decode layer difference: 636.7547992487807\n",
      "training epoch 1: error: 0.5833829109231542\n",
      "training epoch 2: error: 0.601136044048435\n",
      "training epoch 3: error: 0.5846777368553921\n",
      "training epoch 4: error: 0.5908802720914147\n",
      "training epoch 5: error: 0.6003676877620827\n",
      "training epoch 6: error: 0.5878412824561166\n",
      "training epoch 7: error: 0.6011463888722065\n",
      "training epoch 8: error: 0.5899068447111055\n",
      "training epoch 9: error: 0.6015129113180313\n",
      "training epoch 10: error: 0.6007834250952455\n",
      "training epoch 11: error: 0.5984710806406869\n",
      "training epoch 12: error: 0.5840996508979588\n",
      "training epoch 13: error: 0.611300671820578\n",
      "training epoch 14: error: 0.5992664261200146\n",
      "training epoch 15: error: 0.5834285757607713\n",
      "training epoch 16: error: 0.6013631855029216\n",
      "training epoch 17: error: 0.5988582824819667\n",
      "training epoch 18: error: 0.5895276081711975\n",
      "training epoch 19: error: 0.6022015720895125\n",
      "training epoch 20: error: 0.5915841774696389\n",
      "\n",
      "fold 3: mse: 0.30368742624530753\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 808.8147050067865\n",
      "autoencoder training epoch 2: decode layer difference: 808.8147050067865\n",
      "autoencoder training epoch 3: decode layer difference: 805.4580995277653\n",
      "autoencoder training epoch 4: decode layer difference: 697.3185448051781\n",
      "autoencoder training epoch 5: decode layer difference: 696.81395421283\n",
      "autoencoder training epoch 6: decode layer difference: 695.242774383704\n",
      "autoencoder training epoch 7: decode layer difference: 669.5322272375697\n",
      "autoencoder training epoch 8: decode layer difference: 657.832090828243\n",
      "autoencoder training epoch 9: decode layer difference: 647.4938460677918\n",
      "autoencoder training epoch 10: decode layer difference: 636.9574200020352\n",
      "autoencoder training epoch 11: decode layer difference: 633.6142589112397\n",
      "autoencoder training epoch 12: decode layer difference: 631.7769369270134\n",
      "autoencoder training epoch 13: decode layer difference: 630.644132982144\n",
      "autoencoder training epoch 14: decode layer difference: 630.6505674982814\n",
      "autoencoder training epoch 15: decode layer difference: 630.082341463559\n",
      "autoencoder training epoch 16: decode layer difference: 630.5671626035728\n",
      "autoencoder training epoch 17: decode layer difference: 630.2002980869283\n",
      "autoencoder training epoch 18: decode layer difference: 630.3545609386156\n",
      "autoencoder training epoch 19: decode layer difference: 628.7857810163234\n",
      "autoencoder training epoch 20: decode layer difference: 628.6973261414414\n",
      "autoencoder training epoch 21: decode layer difference: 627.8874782234029\n",
      "autoencoder training epoch 22: decode layer difference: 627.3522232356886\n",
      "autoencoder training epoch 23: decode layer difference: 627.0143285337274\n",
      "autoencoder training epoch 24: decode layer difference: 627.7349095409104\n",
      "autoencoder training epoch 25: decode layer difference: 627.4608205518847\n",
      "autoencoder training epoch 26: decode layer difference: 627.815863107161\n",
      "autoencoder training epoch 27: decode layer difference: 628.020167165333\n",
      "autoencoder training epoch 28: decode layer difference: 628.8440460668855\n",
      "autoencoder training epoch 29: decode layer difference: 628.1441859503893\n",
      "autoencoder training epoch 30: decode layer difference: 629.3328728145614\n",
      "autoencoder training epoch 31: decode layer difference: 628.2612027915048\n",
      "autoencoder training epoch 32: decode layer difference: 628.1905275499817\n",
      "autoencoder training epoch 33: decode layer difference: 628.0915360537381\n",
      "autoencoder training epoch 34: decode layer difference: 629.1441992280241\n",
      "autoencoder training epoch 35: decode layer difference: 627.6230527241362\n",
      "autoencoder training epoch 36: decode layer difference: 628.299531254164\n",
      "autoencoder training epoch 37: decode layer difference: 628.2245121166177\n",
      "autoencoder training epoch 38: decode layer difference: 627.256533104013\n",
      "autoencoder training epoch 39: decode layer difference: 628.2251250867826\n",
      "autoencoder training epoch 40: decode layer difference: 628.2252828967931\n",
      "autoencoder training epoch 41: decode layer difference: 627.1869197548813\n",
      "autoencoder training epoch 42: decode layer difference: 627.8473277361009\n",
      "autoencoder training epoch 43: decode layer difference: 627.8816363904076\n",
      "autoencoder training epoch 44: decode layer difference: 628.9445148471987\n",
      "autoencoder training epoch 45: decode layer difference: 628.1177347504631\n",
      "autoencoder training epoch 46: decode layer difference: 627.4451075961442\n",
      "autoencoder training epoch 47: decode layer difference: 627.7301673509148\n",
      "autoencoder training epoch 48: decode layer difference: 628.1536111392182\n",
      "autoencoder training epoch 49: decode layer difference: 627.2903557082757\n",
      "autoencoder training epoch 50: decode layer difference: 627.7080376817903\n",
      "autoencoder training epoch 51: decode layer difference: 628.1411885049553\n",
      "autoencoder training epoch 52: decode layer difference: 627.4389528082538\n",
      "autoencoder training epoch 53: decode layer difference: 627.6587259052042\n",
      "autoencoder training epoch 54: decode layer difference: 627.7013462454336\n",
      "autoencoder training epoch 55: decode layer difference: 628.0220509306899\n",
      "autoencoder training epoch 56: decode layer difference: 629.4771092742819\n",
      "autoencoder training epoch 57: decode layer difference: 628.643580130901\n",
      "autoencoder training epoch 58: decode layer difference: 628.5110227010872\n",
      "autoencoder training epoch 59: decode layer difference: 628.0397078309614\n",
      "autoencoder training epoch 60: decode layer difference: 629.0552686878561\n",
      "autoencoder training epoch 61: decode layer difference: 627.4200376179363\n",
      "autoencoder training epoch 62: decode layer difference: 627.5075161165773\n",
      "autoencoder training epoch 63: decode layer difference: 629.5020806273199\n",
      "autoencoder training epoch 64: decode layer difference: 627.2753066529837\n",
      "autoencoder training epoch 65: decode layer difference: 627.848026637464\n",
      "autoencoder training epoch 66: decode layer difference: 627.3324883610214\n",
      "autoencoder training epoch 67: decode layer difference: 628.1529178711747\n",
      "autoencoder training epoch 68: decode layer difference: 628.154193591881\n",
      "autoencoder training epoch 69: decode layer difference: 628.0010861907678\n",
      "autoencoder training epoch 70: decode layer difference: 626.7076865681915\n",
      "autoencoder training epoch 71: decode layer difference: 627.7338254780202\n",
      "autoencoder training epoch 72: decode layer difference: 628.0374009619876\n",
      "autoencoder training epoch 73: decode layer difference: 627.2168084706461\n",
      "autoencoder training epoch 74: decode layer difference: 628.7317780649998\n",
      "autoencoder training epoch 75: decode layer difference: 627.5607841809571\n",
      "autoencoder training epoch 76: decode layer difference: 627.9335181822687\n",
      "autoencoder training epoch 77: decode layer difference: 628.1736310209958\n",
      "autoencoder training epoch 78: decode layer difference: 628.0117877424566\n",
      "autoencoder training epoch 79: decode layer difference: 627.9779935541451\n",
      "autoencoder training epoch 80: decode layer difference: 627.765478281289\n",
      "autoencoder training epoch 81: decode layer difference: 626.2103018658446\n",
      "autoencoder training epoch 82: decode layer difference: 628.0495710156556\n",
      "autoencoder training epoch 83: decode layer difference: 626.8703463547519\n",
      "autoencoder training epoch 84: decode layer difference: 628.7479838834386\n",
      "autoencoder training epoch 85: decode layer difference: 628.2294542920511\n",
      "autoencoder training epoch 86: decode layer difference: 627.7368537242277\n",
      "autoencoder training epoch 87: decode layer difference: 626.8671309206683\n",
      "autoencoder training epoch 88: decode layer difference: 627.2154596751075\n",
      "autoencoder training epoch 89: decode layer difference: 626.3540678903846\n",
      "autoencoder training epoch 90: decode layer difference: 626.3973749153723\n",
      "autoencoder training epoch 91: decode layer difference: 626.651118286549\n",
      "autoencoder training epoch 92: decode layer difference: 627.3265909591739\n",
      "autoencoder training epoch 93: decode layer difference: 627.0989076454555\n",
      "autoencoder training epoch 94: decode layer difference: 627.428492142891\n",
      "autoencoder training epoch 95: decode layer difference: 627.4381426222528\n",
      "autoencoder training epoch 96: decode layer difference: 627.1027929232474\n",
      "autoencoder training epoch 97: decode layer difference: 627.2382777092139\n",
      "autoencoder training epoch 98: decode layer difference: 626.7759003070256\n",
      "autoencoder training epoch 99: decode layer difference: 626.8335249718635\n",
      "autoencoder training epoch 100: decode layer difference: 626.6863219920372\n",
      "autoencoder training epoch 101: decode layer difference: 627.4626392353613\n",
      "autoencoder training epoch 102: decode layer difference: 627.0516590084308\n",
      "autoencoder training epoch 103: decode layer difference: 627.3751086245609\n",
      "autoencoder training epoch 104: decode layer difference: 626.4645820639261\n",
      "autoencoder training epoch 105: decode layer difference: 626.9776473463733\n",
      "autoencoder training epoch 106: decode layer difference: 626.3271107819544\n",
      "autoencoder training epoch 107: decode layer difference: 626.9871638551718\n",
      "autoencoder training epoch 108: decode layer difference: 626.4447722015611\n",
      "autoencoder training epoch 109: decode layer difference: 626.7666087132383\n",
      "autoencoder training epoch 110: decode layer difference: 626.7622584039506\n",
      "autoencoder training epoch 111: decode layer difference: 628.085363162805\n",
      "autoencoder training epoch 112: decode layer difference: 627.3618625082548\n",
      "autoencoder training epoch 113: decode layer difference: 626.8417752912276\n",
      "autoencoder training epoch 114: decode layer difference: 626.7705374265119\n",
      "autoencoder training epoch 115: decode layer difference: 626.4447722015611\n",
      "autoencoder training epoch 116: decode layer difference: 626.681796894588\n",
      "autoencoder training epoch 117: decode layer difference: 626.8060635334022\n",
      "autoencoder training epoch 118: decode layer difference: 628.0787029646126\n",
      "autoencoder training epoch 119: decode layer difference: 625.9553540640181\n",
      "autoencoder training epoch 120: decode layer difference: 626.0998336276941\n",
      "autoencoder training epoch 121: decode layer difference: 627.5767837345262\n",
      "autoencoder training epoch 122: decode layer difference: 627.3245322430414\n",
      "autoencoder training epoch 123: decode layer difference: 628.2468220919375\n",
      "autoencoder training epoch 124: decode layer difference: 626.450305127723\n",
      "autoencoder training epoch 125: decode layer difference: 627.2925303323082\n",
      "autoencoder training epoch 126: decode layer difference: 627.4951442736747\n",
      "autoencoder training epoch 127: decode layer difference: 630.1863321095666\n",
      "autoencoder training epoch 128: decode layer difference: 626.0727749342956\n",
      "autoencoder training epoch 129: decode layer difference: 627.4443959892702\n",
      "autoencoder training epoch 130: decode layer difference: 627.0766574970573\n",
      "autoencoder training epoch 131: decode layer difference: 627.4264841575118\n",
      "autoencoder training epoch 132: decode layer difference: 627.3204810408804\n",
      "autoencoder training epoch 133: decode layer difference: 627.0521295442834\n",
      "autoencoder training epoch 134: decode layer difference: 627.4266329081255\n",
      "autoencoder training epoch 135: decode layer difference: 630.9646299116024\n",
      "autoencoder training epoch 136: decode layer difference: 627.074470340254\n",
      "autoencoder training epoch 137: decode layer difference: 626.8002856210156\n",
      "autoencoder training epoch 138: decode layer difference: 626.5903720959832\n",
      "autoencoder training epoch 139: decode layer difference: 627.2877841775348\n",
      "autoencoder training epoch 140: decode layer difference: 626.2907970325259\n",
      "autoencoder training epoch 141: decode layer difference: 626.8057666173922\n",
      "autoencoder training epoch 142: decode layer difference: 627.4464913029208\n",
      "autoencoder training epoch 143: decode layer difference: 627.2315649088662\n",
      "autoencoder training epoch 144: decode layer difference: 625.862103061477\n",
      "autoencoder training epoch 145: decode layer difference: 626.568102863388\n",
      "autoencoder training epoch 146: decode layer difference: 626.5437232356304\n",
      "autoencoder training epoch 147: decode layer difference: 627.1503592664083\n",
      "autoencoder training epoch 148: decode layer difference: 626.940956568468\n",
      "autoencoder training epoch 149: decode layer difference: 626.7179915869634\n",
      "autoencoder training epoch 150: decode layer difference: 629.533767600989\n",
      "autoencoder training epoch 151: decode layer difference: 626.4179020077294\n",
      "autoencoder training epoch 152: decode layer difference: 626.8729220514524\n",
      "autoencoder training epoch 153: decode layer difference: 627.6729325733925\n",
      "autoencoder training epoch 154: decode layer difference: 627.2854780935126\n",
      "autoencoder training epoch 155: decode layer difference: 628.8115208453928\n",
      "autoencoder training epoch 156: decode layer difference: 627.2162484712613\n",
      "autoencoder training epoch 157: decode layer difference: 627.4342104243062\n",
      "autoencoder training epoch 158: decode layer difference: 626.5865916488897\n",
      "autoencoder training epoch 159: decode layer difference: 626.0919348115365\n",
      "autoencoder training epoch 160: decode layer difference: 627.4320334714826\n",
      "autoencoder training epoch 161: decode layer difference: 627.0967271079471\n",
      "autoencoder training epoch 162: decode layer difference: 626.6082601623149\n",
      "autoencoder training epoch 163: decode layer difference: 626.3436183238719\n",
      "autoencoder training epoch 164: decode layer difference: 626.9448238458815\n",
      "autoencoder training epoch 165: decode layer difference: 626.9218512553331\n",
      "autoencoder training epoch 166: decode layer difference: 626.4612564132128\n",
      "autoencoder training epoch 167: decode layer difference: 626.4390510037179\n",
      "autoencoder training epoch 168: decode layer difference: 626.671843857762\n",
      "autoencoder training epoch 169: decode layer difference: 628.6747787831181\n",
      "autoencoder training epoch 170: decode layer difference: 625.5401343508452\n",
      "autoencoder training epoch 171: decode layer difference: 631.0794782046203\n",
      "autoencoder training epoch 172: decode layer difference: 626.0476219171234\n",
      "autoencoder training epoch 173: decode layer difference: 628.334308491161\n",
      "autoencoder training epoch 174: decode layer difference: 626.1062646471385\n",
      "autoencoder training epoch 175: decode layer difference: 626.2695236609957\n",
      "autoencoder training epoch 176: decode layer difference: 626.4548332109913\n",
      "autoencoder training epoch 177: decode layer difference: 626.6270808393706\n",
      "autoencoder training epoch 178: decode layer difference: 627.6117683489515\n",
      "autoencoder training epoch 179: decode layer difference: 626.1409261928081\n",
      "autoencoder training epoch 180: decode layer difference: 626.0584880003287\n",
      "autoencoder training epoch 181: decode layer difference: 626.0201360077399\n",
      "autoencoder training epoch 182: decode layer difference: 626.9377444421139\n",
      "autoencoder training epoch 183: decode layer difference: 627.1416043074626\n",
      "autoencoder training epoch 184: decode layer difference: 627.99135779925\n",
      "autoencoder training epoch 185: decode layer difference: 625.5490292925117\n",
      "autoencoder training epoch 186: decode layer difference: 626.2327826605118\n",
      "autoencoder training epoch 187: decode layer difference: 625.9715200325086\n",
      "autoencoder training epoch 188: decode layer difference: 626.8944980873372\n",
      "autoencoder training epoch 189: decode layer difference: 627.0362420326679\n",
      "autoencoder training epoch 190: decode layer difference: 626.7449333566591\n",
      "autoencoder training epoch 191: decode layer difference: 627.6817130778114\n",
      "autoencoder training epoch 192: decode layer difference: 626.9190776949271\n",
      "autoencoder training epoch 193: decode layer difference: 627.1680501741234\n",
      "autoencoder training epoch 194: decode layer difference: 626.4337855643811\n",
      "autoencoder training epoch 195: decode layer difference: 626.594261220526\n",
      "autoencoder training epoch 196: decode layer difference: 626.9186424544461\n",
      "autoencoder training epoch 197: decode layer difference: 626.2416262459859\n",
      "autoencoder training epoch 198: decode layer difference: 625.4328784232433\n",
      "autoencoder training epoch 199: decode layer difference: 628.6938451921308\n",
      "autoencoder training epoch 200: decode layer difference: 626.1139468804618\n",
      "training epoch 1: error: 0.36617152523372126\n",
      "training epoch 2: error: 0.36811277242268814\n",
      "training epoch 3: error: 0.35057747834057085\n",
      "training epoch 4: error: 0.35074899709110785\n",
      "training epoch 5: error: 0.3528284099690874\n",
      "training epoch 6: error: 0.3514232346571668\n",
      "training epoch 7: error: 0.366942502022374\n",
      "training epoch 8: error: 0.35054442537679387\n",
      "training epoch 9: error: 0.3718349518339335\n",
      "training epoch 10: error: 0.35060052198273645\n",
      "training epoch 11: error: 0.3712045580848127\n",
      "training epoch 12: error: 0.3669481775775971\n",
      "training epoch 13: error: 0.3530571613419922\n",
      "training epoch 14: error: 0.36907708744658413\n",
      "training epoch 15: error: 0.3648881211454567\n",
      "training epoch 16: error: 0.37291288943365325\n",
      "training epoch 17: error: 0.5118982603813478\n",
      "training epoch 18: error: 0.3563647514805859\n",
      "training epoch 19: error: 0.37265111810543217\n",
      "training epoch 20: error: 0.3591805575758486\n",
      "\n",
      "fold 4: mse: 0.5333665225522801\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 819.4408842933688\n",
      "autoencoder training epoch 2: decode layer difference: 819.4408842933688\n",
      "autoencoder training epoch 3: decode layer difference: 792.5718284697989\n",
      "autoencoder training epoch 4: decode layer difference: 704.8094266405512\n",
      "autoencoder training epoch 5: decode layer difference: 706.1576042672659\n",
      "autoencoder training epoch 6: decode layer difference: 698.7013674587995\n",
      "autoencoder training epoch 7: decode layer difference: 689.8101329021266\n",
      "autoencoder training epoch 8: decode layer difference: 664.3756207971853\n",
      "autoencoder training epoch 9: decode layer difference: 654.3905578529259\n",
      "autoencoder training epoch 10: decode layer difference: 646.3934751218085\n",
      "autoencoder training epoch 11: decode layer difference: 640.5699400678085\n",
      "autoencoder training epoch 12: decode layer difference: 636.9948663633104\n",
      "autoencoder training epoch 13: decode layer difference: 637.5022940764406\n",
      "autoencoder training epoch 14: decode layer difference: 635.8495295550804\n",
      "autoencoder training epoch 15: decode layer difference: 637.1325186180313\n",
      "autoencoder training epoch 16: decode layer difference: 636.1557261507016\n",
      "autoencoder training epoch 17: decode layer difference: 636.0569728402072\n",
      "autoencoder training epoch 18: decode layer difference: 636.3865625812194\n",
      "autoencoder training epoch 19: decode layer difference: 640.6526547183221\n",
      "autoencoder training epoch 20: decode layer difference: 635.5721338864507\n",
      "autoencoder training epoch 21: decode layer difference: 636.8708920084441\n",
      "autoencoder training epoch 22: decode layer difference: 635.3898844552662\n",
      "autoencoder training epoch 23: decode layer difference: 635.2952138228144\n",
      "autoencoder training epoch 24: decode layer difference: 635.5440264005222\n",
      "autoencoder training epoch 25: decode layer difference: 637.3293710162868\n",
      "autoencoder training epoch 26: decode layer difference: 637.1524282696628\n",
      "autoencoder training epoch 27: decode layer difference: 636.7242756955516\n",
      "autoencoder training epoch 28: decode layer difference: 636.6882293822725\n",
      "autoencoder training epoch 29: decode layer difference: 637.4415507105048\n",
      "autoencoder training epoch 30: decode layer difference: 636.2742484668414\n",
      "autoencoder training epoch 31: decode layer difference: 634.7131094028332\n",
      "autoencoder training epoch 32: decode layer difference: 636.2651779889113\n",
      "autoencoder training epoch 33: decode layer difference: 635.9460727166565\n",
      "autoencoder training epoch 34: decode layer difference: 635.0884924561748\n",
      "autoencoder training epoch 35: decode layer difference: 636.3064180735411\n",
      "autoencoder training epoch 36: decode layer difference: 637.1757282055427\n",
      "autoencoder training epoch 37: decode layer difference: 634.4098979485045\n",
      "autoencoder training epoch 38: decode layer difference: 635.3017195941181\n",
      "autoencoder training epoch 39: decode layer difference: 635.1990604037851\n",
      "autoencoder training epoch 40: decode layer difference: 634.6073874341562\n",
      "autoencoder training epoch 41: decode layer difference: 636.8703792229089\n",
      "autoencoder training epoch 42: decode layer difference: 634.0224111680175\n",
      "autoencoder training epoch 43: decode layer difference: 635.8896186394845\n",
      "autoencoder training epoch 44: decode layer difference: 634.7932531399765\n",
      "autoencoder training epoch 45: decode layer difference: 635.5941024809773\n",
      "autoencoder training epoch 46: decode layer difference: 633.8398830303058\n",
      "autoencoder training epoch 47: decode layer difference: 635.0372916774734\n",
      "autoencoder training epoch 48: decode layer difference: 634.7433236156905\n",
      "autoencoder training epoch 49: decode layer difference: 634.4096397786178\n",
      "autoencoder training epoch 50: decode layer difference: 635.3373241778363\n",
      "autoencoder training epoch 51: decode layer difference: 634.8320069451842\n",
      "autoencoder training epoch 52: decode layer difference: 633.6586091370522\n",
      "autoencoder training epoch 53: decode layer difference: 633.2569234767191\n",
      "autoencoder training epoch 54: decode layer difference: 633.7343326131529\n",
      "autoencoder training epoch 55: decode layer difference: 634.3564575120452\n",
      "autoencoder training epoch 56: decode layer difference: 634.8004845582311\n",
      "autoencoder training epoch 57: decode layer difference: 634.8282048416055\n",
      "autoencoder training epoch 58: decode layer difference: 634.7724160094597\n",
      "autoencoder training epoch 59: decode layer difference: 634.928321983695\n",
      "autoencoder training epoch 60: decode layer difference: 634.1043308775606\n",
      "autoencoder training epoch 61: decode layer difference: 643.3342298642134\n",
      "autoencoder training epoch 62: decode layer difference: 637.937774277281\n",
      "autoencoder training epoch 63: decode layer difference: 633.311575003957\n",
      "autoencoder training epoch 64: decode layer difference: 633.7471627998318\n",
      "autoencoder training epoch 65: decode layer difference: 633.7201561254863\n",
      "autoencoder training epoch 66: decode layer difference: 634.4953430506048\n",
      "autoencoder training epoch 67: decode layer difference: 634.9208795102734\n",
      "autoencoder training epoch 68: decode layer difference: 634.7598808122713\n",
      "autoencoder training epoch 69: decode layer difference: 633.8414378971266\n",
      "autoencoder training epoch 70: decode layer difference: 632.6264207680964\n",
      "autoencoder training epoch 71: decode layer difference: 633.3793415995513\n",
      "autoencoder training epoch 72: decode layer difference: 633.5245272062092\n",
      "autoencoder training epoch 73: decode layer difference: 632.1044123302624\n",
      "autoencoder training epoch 74: decode layer difference: 633.1459581793551\n",
      "autoencoder training epoch 75: decode layer difference: 633.2622617350023\n",
      "autoencoder training epoch 76: decode layer difference: 634.2282624392602\n",
      "autoencoder training epoch 77: decode layer difference: 634.0139867289878\n",
      "autoencoder training epoch 78: decode layer difference: 633.1070470131241\n",
      "autoencoder training epoch 79: decode layer difference: 632.8873927220718\n",
      "autoencoder training epoch 80: decode layer difference: 632.0956355104918\n",
      "autoencoder training epoch 81: decode layer difference: 632.8996613694895\n",
      "autoencoder training epoch 82: decode layer difference: 633.370068709904\n",
      "autoencoder training epoch 83: decode layer difference: 632.9437155017007\n",
      "autoencoder training epoch 84: decode layer difference: 633.2329850209635\n",
      "autoencoder training epoch 85: decode layer difference: 633.274351810291\n",
      "autoencoder training epoch 86: decode layer difference: 632.0175863674909\n",
      "autoencoder training epoch 87: decode layer difference: 632.8951435808692\n",
      "autoencoder training epoch 88: decode layer difference: 632.6807587176643\n",
      "autoencoder training epoch 89: decode layer difference: 631.5740500697652\n",
      "autoencoder training epoch 90: decode layer difference: 632.6041856348345\n",
      "autoencoder training epoch 91: decode layer difference: 631.4001056799883\n",
      "autoencoder training epoch 92: decode layer difference: 632.9506285531851\n",
      "autoencoder training epoch 93: decode layer difference: 633.7367062188023\n",
      "autoencoder training epoch 94: decode layer difference: 632.8176112228508\n",
      "autoencoder training epoch 95: decode layer difference: 630.9774135881414\n",
      "autoencoder training epoch 96: decode layer difference: 631.7570831833787\n",
      "autoencoder training epoch 97: decode layer difference: 633.0683381044032\n",
      "autoencoder training epoch 98: decode layer difference: 632.313470358296\n",
      "autoencoder training epoch 99: decode layer difference: 638.915466762295\n",
      "autoencoder training epoch 100: decode layer difference: 632.4228561800155\n",
      "autoencoder training epoch 101: decode layer difference: 632.879144588273\n",
      "autoencoder training epoch 102: decode layer difference: 634.0021853461155\n",
      "autoencoder training epoch 103: decode layer difference: 632.6998935880295\n",
      "autoencoder training epoch 104: decode layer difference: 632.4690334477164\n",
      "autoencoder training epoch 105: decode layer difference: 632.2408114945102\n",
      "autoencoder training epoch 106: decode layer difference: 632.3531770312459\n",
      "autoencoder training epoch 107: decode layer difference: 631.4811185107887\n",
      "autoencoder training epoch 108: decode layer difference: 633.6999754993815\n",
      "autoencoder training epoch 109: decode layer difference: 631.7966007340019\n",
      "autoencoder training epoch 110: decode layer difference: 633.3634923491076\n",
      "autoencoder training epoch 111: decode layer difference: 631.4680264002421\n",
      "autoencoder training epoch 112: decode layer difference: 631.747650230823\n",
      "autoencoder training epoch 113: decode layer difference: 632.4863417448596\n",
      "autoencoder training epoch 114: decode layer difference: 632.8593272294227\n",
      "autoencoder training epoch 115: decode layer difference: 633.3712232754679\n",
      "autoencoder training epoch 116: decode layer difference: 631.9436953227256\n",
      "autoencoder training epoch 117: decode layer difference: 632.4805614099027\n",
      "autoencoder training epoch 118: decode layer difference: 633.0196439299034\n",
      "autoencoder training epoch 119: decode layer difference: 632.6872703422688\n",
      "autoencoder training epoch 120: decode layer difference: 631.9810012966012\n",
      "autoencoder training epoch 121: decode layer difference: 632.0977026395177\n",
      "autoencoder training epoch 122: decode layer difference: 632.0139752491996\n",
      "autoencoder training epoch 123: decode layer difference: 633.4662666566637\n",
      "autoencoder training epoch 124: decode layer difference: 633.3291490446311\n",
      "autoencoder training epoch 125: decode layer difference: 632.4649360940989\n",
      "autoencoder training epoch 126: decode layer difference: 632.7615994930212\n",
      "autoencoder training epoch 127: decode layer difference: 633.2782215364671\n",
      "autoencoder training epoch 128: decode layer difference: 633.3163275537767\n",
      "autoencoder training epoch 129: decode layer difference: 632.3679963044755\n",
      "autoencoder training epoch 130: decode layer difference: 635.2654811151906\n",
      "autoencoder training epoch 131: decode layer difference: 632.188938077039\n",
      "autoencoder training epoch 132: decode layer difference: 632.7754689050132\n",
      "autoencoder training epoch 133: decode layer difference: 633.3076504389542\n",
      "autoencoder training epoch 134: decode layer difference: 632.6353643507795\n",
      "autoencoder training epoch 135: decode layer difference: 633.3047904819789\n",
      "autoencoder training epoch 136: decode layer difference: 632.077791029334\n",
      "autoencoder training epoch 137: decode layer difference: 633.9211428752715\n",
      "autoencoder training epoch 138: decode layer difference: 633.5322735351684\n",
      "autoencoder training epoch 139: decode layer difference: 632.8711171524847\n",
      "autoencoder training epoch 140: decode layer difference: 632.6860380075996\n",
      "autoencoder training epoch 141: decode layer difference: 632.9100328601937\n",
      "autoencoder training epoch 142: decode layer difference: 634.0326840372519\n",
      "autoencoder training epoch 143: decode layer difference: 633.055636163014\n",
      "autoencoder training epoch 144: decode layer difference: 632.4405411712376\n",
      "autoencoder training epoch 145: decode layer difference: 633.2962085952252\n",
      "autoencoder training epoch 146: decode layer difference: 632.6331011831363\n",
      "autoencoder training epoch 147: decode layer difference: 633.0677904008141\n",
      "autoencoder training epoch 148: decode layer difference: 633.3235839183446\n",
      "autoencoder training epoch 149: decode layer difference: 633.1197200582018\n",
      "autoencoder training epoch 150: decode layer difference: 632.2143637549334\n",
      "autoencoder training epoch 151: decode layer difference: 633.4628930877565\n",
      "autoencoder training epoch 152: decode layer difference: 633.4135777599352\n",
      "autoencoder training epoch 153: decode layer difference: 632.894783566685\n",
      "autoencoder training epoch 154: decode layer difference: 633.646876204423\n",
      "autoencoder training epoch 155: decode layer difference: 633.7964368148855\n",
      "autoencoder training epoch 156: decode layer difference: 632.4150396390917\n",
      "autoencoder training epoch 157: decode layer difference: 632.9087790663297\n",
      "autoencoder training epoch 158: decode layer difference: 634.4560439118204\n",
      "autoencoder training epoch 159: decode layer difference: 633.0401751247861\n",
      "autoencoder training epoch 160: decode layer difference: 632.7288272384685\n",
      "autoencoder training epoch 161: decode layer difference: 633.7365858492899\n",
      "autoencoder training epoch 162: decode layer difference: 633.8843727257193\n",
      "autoencoder training epoch 163: decode layer difference: 633.4295356905168\n",
      "autoencoder training epoch 164: decode layer difference: 634.4981659569474\n",
      "autoencoder training epoch 165: decode layer difference: 632.9722120813458\n",
      "autoencoder training epoch 166: decode layer difference: 633.6565124867375\n",
      "autoencoder training epoch 167: decode layer difference: 633.9708364322997\n",
      "autoencoder training epoch 168: decode layer difference: 633.6354497931421\n",
      "autoencoder training epoch 169: decode layer difference: 633.220766858125\n",
      "autoencoder training epoch 170: decode layer difference: 634.3641499158396\n",
      "autoencoder training epoch 171: decode layer difference: 633.5331788008818\n",
      "autoencoder training epoch 172: decode layer difference: 633.6689627169934\n",
      "autoencoder training epoch 173: decode layer difference: 633.6661321788596\n",
      "autoencoder training epoch 174: decode layer difference: 634.5002752403559\n",
      "autoencoder training epoch 175: decode layer difference: 633.2582962393737\n",
      "autoencoder training epoch 176: decode layer difference: 632.9195017750603\n",
      "autoencoder training epoch 177: decode layer difference: 634.0841147954766\n",
      "autoencoder training epoch 178: decode layer difference: 633.1904713729539\n",
      "autoencoder training epoch 179: decode layer difference: 633.1527696219032\n",
      "autoencoder training epoch 180: decode layer difference: 633.3633443812345\n",
      "autoencoder training epoch 181: decode layer difference: 633.6911414923412\n",
      "autoencoder training epoch 182: decode layer difference: 633.3860817652238\n",
      "autoencoder training epoch 183: decode layer difference: 633.3816318838657\n",
      "autoencoder training epoch 184: decode layer difference: 633.4497064588384\n",
      "autoencoder training epoch 185: decode layer difference: 632.8286926628433\n",
      "autoencoder training epoch 186: decode layer difference: 631.4585823768875\n",
      "autoencoder training epoch 187: decode layer difference: 631.7936516481635\n",
      "autoencoder training epoch 188: decode layer difference: 632.4540013124627\n",
      "autoencoder training epoch 189: decode layer difference: 633.3829622286396\n",
      "autoencoder training epoch 190: decode layer difference: 632.0887765160392\n",
      "autoencoder training epoch 191: decode layer difference: 632.5352403094495\n",
      "autoencoder training epoch 192: decode layer difference: 632.8065844935821\n",
      "autoencoder training epoch 193: decode layer difference: 632.5793692454849\n",
      "autoencoder training epoch 194: decode layer difference: 632.8788877320475\n",
      "autoencoder training epoch 195: decode layer difference: 631.8558537623177\n",
      "autoencoder training epoch 196: decode layer difference: 634.0031270973019\n",
      "autoencoder training epoch 197: decode layer difference: 632.0389004781596\n",
      "autoencoder training epoch 198: decode layer difference: 631.9817633326538\n",
      "autoencoder training epoch 199: decode layer difference: 632.0128143975886\n",
      "autoencoder training epoch 200: decode layer difference: 632.3241030761178\n",
      "training epoch 1: error: 0.8647792164004244\n",
      "training epoch 2: error: 0.8702820239084308\n",
      "training epoch 3: error: 0.8791635384121842\n",
      "training epoch 4: error: 0.8548626545479725\n",
      "training epoch 5: error: 0.8577670139250376\n",
      "training epoch 6: error: 0.8548685830098021\n",
      "training epoch 7: error: 0.8698436410163947\n",
      "training epoch 8: error: 0.8856304841144097\n",
      "training epoch 9: error: 0.8809098255328419\n",
      "training epoch 10: error: 0.8802048726377882\n",
      "training epoch 11: error: 0.8709891499363768\n",
      "training epoch 12: error: 0.8724816270826427\n",
      "training epoch 13: error: 0.8594287353455891\n",
      "training epoch 14: error: 0.8998615559086931\n",
      "training epoch 15: error: 0.8818319917979278\n",
      "training epoch 16: error: 0.8756170051995211\n",
      "training epoch 17: error: 0.8566909784513603\n",
      "training epoch 18: error: 0.86413748116436\n",
      "training epoch 19: error: 0.8555707131968648\n",
      "training epoch 20: error: 1.0043983972086885\n",
      "\n",
      "fold 5: mse: 0.0732325945567439\n",
      "\n",
      "average error: 0.20477270032033928\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.20477270032033928"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_layered(forest_data, 'area', train_autoencoder_network_regression, (5,10), z_norm=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1: error: 0.8534199172288401\n",
      "training epoch 2: error: 0.8527924380117325\n",
      "training epoch 3: error: 0.928470821851764\n",
      "training epoch 4: error: 0.8668839306535688\n",
      "training epoch 5: error: 0.8725467634748891\n",
      "training epoch 6: error: 0.84297353732261\n",
      "training epoch 7: error: 0.8531986536340532\n",
      "training epoch 8: error: 0.8625269965072716\n",
      "training epoch 9: error: 0.8678732646486426\n",
      "training epoch 10: error: 0.8403720019470965\n",
      "training epoch 11: error: 0.8602323915796194\n",
      "training epoch 12: error: 0.8542272847585438\n",
      "training epoch 13: error: 2.0502373069451174\n",
      "training epoch 14: error: 0.864064703570891\n",
      "training epoch 15: error: 1.06187787294882\n",
      "training epoch 16: error: 0.8538596926606393\n",
      "training epoch 17: error: 0.8574208957152725\n",
      "training epoch 18: error: 0.8624966007854684\n",
      "training epoch 19: error: 0.8617646967948996\n",
      "training epoch 20: error: 0.862219418231985\n",
      "\n",
      "fold 1: mse: 0.03810064661472318\n",
      "\n",
      "training epoch 1: error: 0.860829433343635\n",
      "training epoch 2: error: 0.8456157945526434\n",
      "training epoch 3: error: 0.8422985698378284\n",
      "training epoch 4: error: 0.85307564589164\n",
      "training epoch 5: error: 0.8621553172008656\n",
      "training epoch 6: error: 0.8454351446844393\n",
      "training epoch 7: error: 0.8633451694054775\n",
      "training epoch 8: error: 0.8456457001144114\n",
      "training epoch 9: error: 0.8533981276743466\n",
      "training epoch 10: error: 0.8373893441959086\n",
      "training epoch 11: error: 0.8552986146030072\n",
      "training epoch 12: error: 0.8529699317805385\n",
      "training epoch 13: error: 0.8656935191977294\n",
      "training epoch 14: error: 0.8521707185757176\n",
      "training epoch 15: error: 0.8451868321353673\n",
      "training epoch 16: error: 0.8522680052529079\n",
      "training epoch 17: error: 0.8529905214084104\n",
      "training epoch 18: error: 0.832912326654905\n",
      "training epoch 19: error: 0.8360426503034588\n",
      "training epoch 20: error: 1.0119015615121976\n",
      "\n",
      "fold 2: mse: 0.09428598183028183\n",
      "\n",
      "training epoch 1: error: 0.3892357266363088\n",
      "training epoch 2: error: 0.3717750438156709\n",
      "training epoch 3: error: 0.4009062266467096\n",
      "training epoch 4: error: 0.39055926086815573\n",
      "training epoch 5: error: 0.38328252118112266\n",
      "training epoch 6: error: 0.3719468827413885\n",
      "training epoch 7: error: 0.3885147534575341\n",
      "training epoch 8: error: 0.3928144494346325\n",
      "training epoch 9: error: 0.5926732115511925\n",
      "training epoch 10: error: 0.3782198184013452\n",
      "training epoch 11: error: 0.38842236965244564\n",
      "training epoch 12: error: 0.38659675606179134\n",
      "training epoch 13: error: 0.37177212203184584\n",
      "training epoch 14: error: 0.3918231405033188\n",
      "training epoch 15: error: 0.38944480618847743\n",
      "training epoch 16: error: 0.3732112262865377\n",
      "training epoch 17: error: 0.39448001435255264\n",
      "training epoch 18: error: 0.38894100541898513\n",
      "training epoch 19: error: 0.3840325561728641\n",
      "training epoch 20: error: 0.3880910378631073\n",
      "\n",
      "fold 3: mse: 0.5153321185377178\n",
      "\n",
      "training epoch 1: error: 0.6130647308385576\n",
      "training epoch 2: error: 0.6282468198401167\n",
      "training epoch 3: error: 0.6170452488438933\n",
      "training epoch 4: error: 0.6255214370514572\n",
      "training epoch 5: error: 0.6261158106485348\n",
      "training epoch 6: error: 0.613437329350491\n",
      "training epoch 7: error: 0.6861187826366572\n",
      "training epoch 8: error: 0.6137923513493797\n",
      "training epoch 9: error: 0.6137080979034419\n",
      "training epoch 10: error: 0.6140155624649563\n",
      "training epoch 11: error: 0.6162600094490505\n",
      "training epoch 12: error: 0.6211904763595489\n",
      "training epoch 13: error: 0.6321880891578926\n",
      "training epoch 14: error: 0.6138622705826393\n",
      "training epoch 15: error: 0.7324282709071558\n",
      "training epoch 16: error: 0.6248115219225195\n",
      "training epoch 17: error: 0.6355675247199625\n",
      "training epoch 18: error: 0.6368946354691619\n",
      "training epoch 19: error: 0.6312301963933324\n",
      "training epoch 20: error: 4.352875458485487\n",
      "\n",
      "fold 4: mse: 1.1549222241637982\n",
      "\n",
      "training epoch 1: error: 0.8493161000358956\n",
      "training epoch 2: error: 0.88049842636479\n",
      "training epoch 3: error: 0.8493917565184053\n",
      "training epoch 4: error: 0.8605159448099956\n",
      "training epoch 5: error: 0.8640628890835891\n",
      "training epoch 6: error: 0.8772727902382724\n",
      "training epoch 7: error: 0.857973599343907\n",
      "training epoch 8: error: 0.867785680142234\n",
      "training epoch 9: error: 5.4429306144953555\n",
      "training epoch 10: error: 0.8505552874360541\n",
      "training epoch 11: error: 1.01828758638058\n",
      "training epoch 12: error: 0.8602502392694686\n",
      "training epoch 13: error: 0.99013429293295\n",
      "training epoch 14: error: 0.8645173720941369\n",
      "training epoch 15: error: 0.8582645320698703\n",
      "training epoch 16: error: 0.9311102729979273\n",
      "training epoch 17: error: 0.866079136482808\n",
      "training epoch 18: error: 1.8965412313041927\n",
      "training epoch 19: error: 0.894085479415977\n",
      "training epoch 20: error: 0.9319025634035433\n",
      "\n",
      "fold 5: mse: 0.07008343120165501\n",
      "\n",
      "average error: 0.3745448804696352\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.3745448804696352"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_layered(forest_data, 'area', train_layered_regression_network, (5,10), z_norm=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1: mse: 0.6067519760297929\n",
      "training epoch 2: mse: 0.6061619687431403\n",
      "training epoch 3: mse: 0.6057963341191944\n",
      "training epoch 4: mse: 0.6059462251790279\n",
      "training epoch 5: mse: 0.6051603362432595\n",
      "training epoch 6: mse: 0.6057749473343559\n",
      "training epoch 7: mse: 0.6047789015043378\n",
      "training epoch 8: mse: 0.60474452041555\n",
      "training epoch 9: mse: 0.6044414035889675\n",
      "training epoch 10: mse: 0.6046237141800156\n",
      "training epoch 11: mse: 0.6041629455178965\n",
      "training epoch 12: mse: 0.6040759754095778\n",
      "training epoch 13: mse: 0.6039000970550126\n",
      "training epoch 14: mse: 0.6038535602117401\n",
      "training epoch 15: mse: 0.6036234657919701\n",
      "training epoch 16: mse: 0.6035978907003523\n",
      "training epoch 17: mse: 0.6037123579239632\n",
      "training epoch 18: mse: 0.6038602323699542\n",
      "training epoch 19: mse: 0.6032254481710926\n",
      "training epoch 20: mse: 0.6031252542367739\n",
      "\n",
      "fold 1: mse: 0.2656800949744482\n",
      "\n",
      "training epoch 1: mse: 0.37019357635423605\n",
      "training epoch 2: mse: 0.3665911563781711\n",
      "training epoch 3: mse: 0.3662582104632094\n",
      "training epoch 4: mse: 0.3653768802915983\n",
      "training epoch 5: mse: 0.36507851194405905\n",
      "training epoch 6: mse: 0.36479967683767645\n",
      "training epoch 7: mse: 0.3647255300643353\n",
      "training epoch 8: mse: 0.3643723688640637\n",
      "training epoch 9: mse: 0.3641203009993733\n",
      "training epoch 10: mse: 0.3639890164224686\n",
      "training epoch 11: mse: 0.3649674468640025\n",
      "training epoch 12: mse: 0.36369091605402276\n",
      "training epoch 13: mse: 0.36345130786062085\n",
      "training epoch 14: mse: 0.36327852887703865\n",
      "training epoch 15: mse: 0.36322358319628995\n",
      "training epoch 16: mse: 0.36303015180727594\n",
      "training epoch 17: mse: 0.36291618429492434\n",
      "training epoch 18: mse: 0.3628021816611187\n",
      "training epoch 19: mse: 0.36286280778888924\n",
      "training epoch 20: mse: 0.36276359882361775\n",
      "\n",
      "fold 2: mse: 0.5056744473616321\n",
      "\n",
      "training epoch 1: mse: 0.8092777722587274\n",
      "training epoch 2: mse: 0.8065060751777147\n",
      "training epoch 3: mse: 0.8068589548800426\n",
      "training epoch 4: mse: 0.8071657838546205\n",
      "training epoch 5: mse: 0.8055576456631635\n",
      "training epoch 6: mse: 0.8053898184504271\n",
      "training epoch 7: mse: 0.8050228937605969\n",
      "training epoch 8: mse: 0.8048366049409528\n",
      "training epoch 9: mse: 0.8054857953199503\n",
      "training epoch 10: mse: 0.8043540691045372\n",
      "training epoch 11: mse: 0.8044451512439194\n",
      "training epoch 12: mse: 0.8042398023760928\n",
      "training epoch 13: mse: 0.8040909375010925\n",
      "training epoch 14: mse: 0.8046981600115739\n",
      "training epoch 15: mse: 0.8036969829989131\n",
      "training epoch 16: mse: 0.8031130552437276\n",
      "training epoch 17: mse: 0.8031579904518287\n",
      "training epoch 18: mse: 0.8028624245639226\n",
      "training epoch 19: mse: 0.8027013787512036\n",
      "training epoch 20: mse: 0.804069733553594\n",
      "\n",
      "fold 3: mse: 0.05966568139565482\n",
      "\n",
      "training epoch 1: mse: 0.8478386344681241\n",
      "training epoch 2: mse: 0.8460090569379849\n",
      "training epoch 3: mse: 0.8451245279861429\n",
      "training epoch 4: mse: 0.8442757462520801\n",
      "training epoch 5: mse: 0.84352016931119\n",
      "training epoch 6: mse: 0.8430963104856484\n",
      "training epoch 7: mse: 0.8424098124994767\n",
      "training epoch 8: mse: 0.8422759356335898\n",
      "training epoch 9: mse: 0.8410530487303658\n",
      "training epoch 10: mse: 0.8405819908235032\n",
      "training epoch 11: mse: 0.8401095267910949\n",
      "training epoch 12: mse: 0.8418920052729052\n",
      "training epoch 13: mse: 0.8393290656930364\n",
      "training epoch 14: mse: 0.8393073624061149\n",
      "training epoch 15: mse: 0.839810533593687\n",
      "training epoch 16: mse: 0.8383523632447327\n",
      "training epoch 17: mse: 0.8380839503467327\n",
      "training epoch 18: mse: 0.8378374699442155\n",
      "training epoch 19: mse: 0.8376421190685384\n",
      "training epoch 20: mse: 0.8379960533562703\n",
      "\n",
      "fold 4: mse: 0.026053805196760395\n",
      "\n",
      "training epoch 1: mse: 0.85641435177334\n",
      "training epoch 2: mse: 0.854322426246731\n",
      "training epoch 3: mse: 0.8540374337181666\n",
      "training epoch 4: mse: 0.8532640625902048\n",
      "training epoch 5: mse: 0.8530216586028622\n",
      "training epoch 6: mse: 0.8526224669203244\n",
      "training epoch 7: mse: 0.85217446673002\n",
      "training epoch 8: mse: 0.8518696620647048\n",
      "training epoch 9: mse: 0.8515848909726973\n",
      "training epoch 10: mse: 0.8516887589015607\n",
      "training epoch 11: mse: 0.8512392703432661\n",
      "training epoch 12: mse: 0.8506409877103365\n",
      "training epoch 13: mse: 0.8509669798214787\n",
      "training epoch 14: mse: 0.8504955241095637\n",
      "training epoch 15: mse: 0.850114833428065\n",
      "training epoch 16: mse: 0.84985306993755\n",
      "training epoch 17: mse: 0.8500074563846957\n",
      "training epoch 18: mse: 0.8495991904269856\n",
      "training epoch 19: mse: 0.8490352091285034\n",
      "training epoch 20: mse: 0.8494434046473014\n",
      "\n",
      "fold 5: mse: 0.014290707415629288\n",
      "\n",
      "average error: 0.17427294726882497\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.17427294726882497"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation(forest_data, 'area', z_norm=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_data = load_abalone('datasets/abalone.data')\n",
    "abalone_data.name = 'abalone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "        length  diameter    height  whole weight  shucked weight  \\\n0     0.513514  0.521008  0.084071      0.181335        0.150303   \n1     0.371622  0.352941  0.079646      0.079157        0.066241   \n2     0.614865  0.613445  0.119469      0.239065        0.171822   \n3     0.493243  0.521008  0.110619      0.182044        0.144250   \n4     0.344595  0.336134  0.070796      0.071897        0.059516   \n...        ...       ...       ...           ...             ...   \n4172  0.662162  0.663866  0.146018      0.313441        0.248151   \n4173  0.695946  0.647059  0.119469      0.341420        0.294553   \n4174  0.709459  0.705882  0.181416      0.415796        0.352724   \n4175  0.743243  0.722689  0.132743      0.386931        0.356422   \n4176  0.858108  0.840336  0.172566      0.689393        0.635171   \n\n      viscera weight  shell weight     rings  \n0           0.132324      0.147982  0.500000  \n1           0.063199      0.068261  0.214286  \n2           0.185648      0.207773  0.285714  \n3           0.149440      0.152965  0.321429  \n4           0.051350      0.053313  0.214286  \n...              ...           ...       ...  \n4172        0.314022      0.246637  0.357143  \n4173        0.281764      0.258097  0.321429  \n4174        0.377880      0.305431  0.285714  \n4175        0.342989      0.293473  0.321429  \n4176        0.495063      0.491779  0.392857  \n\n[4177 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>length</th>\n      <th>diameter</th>\n      <th>height</th>\n      <th>whole weight</th>\n      <th>shucked weight</th>\n      <th>viscera weight</th>\n      <th>shell weight</th>\n      <th>rings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.513514</td>\n      <td>0.521008</td>\n      <td>0.084071</td>\n      <td>0.181335</td>\n      <td>0.150303</td>\n      <td>0.132324</td>\n      <td>0.147982</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.371622</td>\n      <td>0.352941</td>\n      <td>0.079646</td>\n      <td>0.079157</td>\n      <td>0.066241</td>\n      <td>0.063199</td>\n      <td>0.068261</td>\n      <td>0.214286</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.614865</td>\n      <td>0.613445</td>\n      <td>0.119469</td>\n      <td>0.239065</td>\n      <td>0.171822</td>\n      <td>0.185648</td>\n      <td>0.207773</td>\n      <td>0.285714</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.493243</td>\n      <td>0.521008</td>\n      <td>0.110619</td>\n      <td>0.182044</td>\n      <td>0.144250</td>\n      <td>0.149440</td>\n      <td>0.152965</td>\n      <td>0.321429</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.344595</td>\n      <td>0.336134</td>\n      <td>0.070796</td>\n      <td>0.071897</td>\n      <td>0.059516</td>\n      <td>0.051350</td>\n      <td>0.053313</td>\n      <td>0.214286</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4172</th>\n      <td>0.662162</td>\n      <td>0.663866</td>\n      <td>0.146018</td>\n      <td>0.313441</td>\n      <td>0.248151</td>\n      <td>0.314022</td>\n      <td>0.246637</td>\n      <td>0.357143</td>\n    </tr>\n    <tr>\n      <th>4173</th>\n      <td>0.695946</td>\n      <td>0.647059</td>\n      <td>0.119469</td>\n      <td>0.341420</td>\n      <td>0.294553</td>\n      <td>0.281764</td>\n      <td>0.258097</td>\n      <td>0.321429</td>\n    </tr>\n    <tr>\n      <th>4174</th>\n      <td>0.709459</td>\n      <td>0.705882</td>\n      <td>0.181416</td>\n      <td>0.415796</td>\n      <td>0.352724</td>\n      <td>0.377880</td>\n      <td>0.305431</td>\n      <td>0.285714</td>\n    </tr>\n    <tr>\n      <th>4175</th>\n      <td>0.743243</td>\n      <td>0.722689</td>\n      <td>0.132743</td>\n      <td>0.386931</td>\n      <td>0.356422</td>\n      <td>0.342989</td>\n      <td>0.293473</td>\n      <td>0.321429</td>\n    </tr>\n    <tr>\n      <th>4176</th>\n      <td>0.858108</td>\n      <td>0.840336</td>\n      <td>0.172566</td>\n      <td>0.689393</td>\n      <td>0.635171</td>\n      <td>0.495063</td>\n      <td>0.491779</td>\n      <td>0.392857</td>\n    </tr>\n  </tbody>\n</table>\n<p>4177 rows  8 columns</p>\n</div>"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoencoder training epoch 1: decode layer difference: 4808.8832926314835\n",
      "autoencoder training epoch 2: decode layer difference: 4742.996972570839\n",
      "autoencoder training epoch 3: decode layer difference: 4729.889415308417\n",
      "autoencoder training epoch 4: decode layer difference: 4724.039155798744\n",
      "autoencoder training epoch 5: decode layer difference: 4723.548158473917\n",
      "autoencoder training epoch 6: decode layer difference: 4724.864898043699\n",
      "autoencoder training epoch 7: decode layer difference: 4722.276007586149\n",
      "autoencoder training epoch 8: decode layer difference: 4721.311069093171\n",
      "autoencoder training epoch 9: decode layer difference: 4722.62769993209\n",
      "autoencoder training epoch 10: decode layer difference: 4722.184894799037\n",
      "autoencoder training epoch 11: decode layer difference: 4721.8491112340225\n",
      "autoencoder training epoch 12: decode layer difference: 4721.569515623075\n",
      "autoencoder training epoch 13: decode layer difference: 4731.914893376698\n",
      "autoencoder training epoch 14: decode layer difference: 4722.0499167998705\n",
      "autoencoder training epoch 15: decode layer difference: 4721.603331871802\n",
      "autoencoder training epoch 16: decode layer difference: 4724.126010118875\n",
      "autoencoder training epoch 17: decode layer difference: 4721.362873258173\n",
      "autoencoder training epoch 18: decode layer difference: 4719.634343762083\n",
      "autoencoder training epoch 19: decode layer difference: 4721.941441401068\n",
      "autoencoder training epoch 20: decode layer difference: 4721.094225444889\n",
      "autoencoder training epoch 21: decode layer difference: 4720.731952622739\n",
      "autoencoder training epoch 22: decode layer difference: 4720.039331065604\n",
      "autoencoder training epoch 23: decode layer difference: 4721.619108092838\n",
      "autoencoder training epoch 24: decode layer difference: 4721.553929715408\n",
      "autoencoder training epoch 25: decode layer difference: 4720.78153490891\n",
      "autoencoder training epoch 26: decode layer difference: 4720.650103643316\n",
      "autoencoder training epoch 27: decode layer difference: 4721.199679344504\n",
      "autoencoder training epoch 28: decode layer difference: 4720.867457324821\n",
      "autoencoder training epoch 29: decode layer difference: 4720.060642631864\n",
      "autoencoder training epoch 30: decode layer difference: 4720.083203089306\n",
      "autoencoder training epoch 31: decode layer difference: 4720.37910321058\n",
      "autoencoder training epoch 32: decode layer difference: 4721.082049178553\n",
      "autoencoder training epoch 33: decode layer difference: 4721.01906180619\n",
      "autoencoder training epoch 34: decode layer difference: 4718.969789243347\n",
      "autoencoder training epoch 35: decode layer difference: 4720.601063203808\n",
      "autoencoder training epoch 36: decode layer difference: 4721.127141443909\n",
      "autoencoder training epoch 37: decode layer difference: 4721.387035834696\n",
      "autoencoder training epoch 38: decode layer difference: 4719.658475179571\n",
      "autoencoder training epoch 39: decode layer difference: 4720.23984534177\n",
      "autoencoder training epoch 40: decode layer difference: 4721.396352970765\n",
      "autoencoder training epoch 41: decode layer difference: 4720.183139823741\n",
      "autoencoder training epoch 42: decode layer difference: 4720.391833438857\n",
      "autoencoder training epoch 43: decode layer difference: 4718.952938406722\n",
      "autoencoder training epoch 44: decode layer difference: 4721.143609573863\n",
      "autoencoder training epoch 45: decode layer difference: 4719.634234023693\n",
      "autoencoder training epoch 46: decode layer difference: 4721.048598496202\n",
      "autoencoder training epoch 47: decode layer difference: 4719.375962143571\n",
      "autoencoder training epoch 48: decode layer difference: 4718.818585142652\n",
      "autoencoder training epoch 49: decode layer difference: 4719.735440318664\n",
      "autoencoder training epoch 50: decode layer difference: 4718.858082761773\n",
      "autoencoder training epoch 51: decode layer difference: 4719.314551200795\n",
      "autoencoder training epoch 52: decode layer difference: 4719.5617929948885\n",
      "autoencoder training epoch 53: decode layer difference: 4719.39311592245\n",
      "autoencoder training epoch 54: decode layer difference: 4718.608362855142\n",
      "autoencoder training epoch 55: decode layer difference: 4717.767388982108\n",
      "autoencoder training epoch 56: decode layer difference: 4716.9249655064095\n",
      "autoencoder training epoch 57: decode layer difference: 4716.26073189778\n",
      "autoencoder training epoch 58: decode layer difference: 4716.275409833272\n",
      "autoencoder training epoch 59: decode layer difference: 4715.962062400633\n",
      "autoencoder training epoch 60: decode layer difference: 4716.345195380222\n",
      "autoencoder training epoch 61: decode layer difference: 4714.1605595935325\n",
      "autoencoder training epoch 62: decode layer difference: 4715.527248565597\n",
      "autoencoder training epoch 63: decode layer difference: 4715.043234943685\n",
      "autoencoder training epoch 64: decode layer difference: 4714.932613057909\n",
      "autoencoder training epoch 65: decode layer difference: 4712.811238696742\n",
      "autoencoder training epoch 66: decode layer difference: 4722.9167988252075\n",
      "autoencoder training epoch 67: decode layer difference: 4713.749647562139\n",
      "autoencoder training epoch 68: decode layer difference: 4716.1701712024615\n",
      "autoencoder training epoch 69: decode layer difference: 4713.561140853111\n",
      "autoencoder training epoch 70: decode layer difference: 4713.21642830647\n",
      "autoencoder training epoch 71: decode layer difference: 4712.706106631627\n",
      "autoencoder training epoch 72: decode layer difference: 4714.233315865673\n",
      "autoencoder training epoch 73: decode layer difference: 4713.017697887\n",
      "autoencoder training epoch 74: decode layer difference: 4712.171609385705\n",
      "autoencoder training epoch 75: decode layer difference: 4713.57605925707\n",
      "autoencoder training epoch 76: decode layer difference: 4713.47982638788\n",
      "autoencoder training epoch 77: decode layer difference: 4712.623052086132\n",
      "autoencoder training epoch 78: decode layer difference: 4711.76371502029\n",
      "autoencoder training epoch 79: decode layer difference: 4712.350700701596\n",
      "autoencoder training epoch 80: decode layer difference: 4714.001198807826\n",
      "autoencoder training epoch 81: decode layer difference: 4712.60875538749\n",
      "autoencoder training epoch 82: decode layer difference: 4711.937557267921\n",
      "autoencoder training epoch 83: decode layer difference: 4712.628196697389\n",
      "autoencoder training epoch 84: decode layer difference: 4715.610052774135\n",
      "autoencoder training epoch 85: decode layer difference: 4715.273179082222\n",
      "autoencoder training epoch 86: decode layer difference: 4712.820999401376\n",
      "autoencoder training epoch 87: decode layer difference: 4711.759732281082\n",
      "autoencoder training epoch 88: decode layer difference: 4713.219459442272\n",
      "autoencoder training epoch 89: decode layer difference: 4718.001920247754\n",
      "autoencoder training epoch 90: decode layer difference: 4715.506759606609\n",
      "autoencoder training epoch 91: decode layer difference: 4713.013438509751\n",
      "autoencoder training epoch 92: decode layer difference: 4711.346951170586\n",
      "autoencoder training epoch 93: decode layer difference: 4713.125396052426\n",
      "autoencoder training epoch 94: decode layer difference: 4712.500255022482\n",
      "autoencoder training epoch 95: decode layer difference: 4713.291939915047\n",
      "autoencoder training epoch 96: decode layer difference: 4712.276391270516\n",
      "autoencoder training epoch 97: decode layer difference: 4716.837066526415\n",
      "autoencoder training epoch 98: decode layer difference: 4711.877981807255\n",
      "autoencoder training epoch 99: decode layer difference: 4713.394719762417\n",
      "autoencoder training epoch 100: decode layer difference: 4715.77341470343\n",
      "autoencoder training epoch 101: decode layer difference: 4716.548106821866\n",
      "autoencoder training epoch 102: decode layer difference: 4711.502662622412\n",
      "autoencoder training epoch 103: decode layer difference: 4712.117221277009\n",
      "autoencoder training epoch 104: decode layer difference: 4711.398316040457\n",
      "autoencoder training epoch 105: decode layer difference: 4711.934887749143\n",
      "autoencoder training epoch 106: decode layer difference: 4713.371650124076\n",
      "autoencoder training epoch 107: decode layer difference: 4711.974948574151\n",
      "autoencoder training epoch 108: decode layer difference: 4711.859583395326\n",
      "autoencoder training epoch 109: decode layer difference: 4712.50720645849\n",
      "autoencoder training epoch 110: decode layer difference: 4711.1415769882715\n",
      "autoencoder training epoch 111: decode layer difference: 4711.622959705293\n",
      "autoencoder training epoch 112: decode layer difference: 4713.961538159267\n",
      "autoencoder training epoch 113: decode layer difference: 4711.383620076602\n",
      "autoencoder training epoch 114: decode layer difference: 4712.9013384479795\n",
      "autoencoder training epoch 115: decode layer difference: 4713.718339068109\n",
      "autoencoder training epoch 116: decode layer difference: 4711.575554857503\n",
      "autoencoder training epoch 117: decode layer difference: 4717.263324685067\n",
      "autoencoder training epoch 118: decode layer difference: 4711.704485788572\n",
      "autoencoder training epoch 119: decode layer difference: 4714.387210387004\n",
      "autoencoder training epoch 120: decode layer difference: 4713.191375999616\n",
      "autoencoder training epoch 121: decode layer difference: 4713.366942490135\n",
      "autoencoder training epoch 122: decode layer difference: 4713.892590069597\n",
      "autoencoder training epoch 123: decode layer difference: 4712.104968782658\n",
      "autoencoder training epoch 124: decode layer difference: 4711.746599955512\n",
      "autoencoder training epoch 125: decode layer difference: 4711.899475788522\n",
      "autoencoder training epoch 126: decode layer difference: 4712.447945359197\n",
      "autoencoder training epoch 127: decode layer difference: 4711.619044871752\n",
      "autoencoder training epoch 128: decode layer difference: 4714.208254209459\n",
      "autoencoder training epoch 129: decode layer difference: 4712.51460873117\n",
      "autoencoder training epoch 130: decode layer difference: 4715.2558920926695\n",
      "autoencoder training epoch 131: decode layer difference: 4712.450994361911\n",
      "autoencoder training epoch 132: decode layer difference: 4711.759723640421\n",
      "autoencoder training epoch 133: decode layer difference: 4713.459437832788\n",
      "autoencoder training epoch 134: decode layer difference: 4712.763945375474\n",
      "autoencoder training epoch 135: decode layer difference: 4713.314512959007\n",
      "autoencoder training epoch 136: decode layer difference: 4711.809954781826\n",
      "autoencoder training epoch 137: decode layer difference: 4711.502095100352\n",
      "autoencoder training epoch 138: decode layer difference: 4716.671318888583\n",
      "autoencoder training epoch 139: decode layer difference: 4712.962978016843\n",
      "autoencoder training epoch 140: decode layer difference: 4713.252814277423\n",
      "autoencoder training epoch 141: decode layer difference: 4711.06876791659\n",
      "autoencoder training epoch 142: decode layer difference: 4711.791940433471\n",
      "autoencoder training epoch 143: decode layer difference: 4713.183002685863\n",
      "autoencoder training epoch 144: decode layer difference: 4713.492713809483\n",
      "autoencoder training epoch 145: decode layer difference: 4742.122923504774\n",
      "autoencoder training epoch 146: decode layer difference: 4711.794387882998\n",
      "autoencoder training epoch 147: decode layer difference: 4711.466172614168\n",
      "autoencoder training epoch 148: decode layer difference: 4721.855742773161\n",
      "autoencoder training epoch 149: decode layer difference: 4713.079558655321\n",
      "autoencoder training epoch 150: decode layer difference: 4713.104035462571\n",
      "autoencoder training epoch 151: decode layer difference: 4713.274906972123\n",
      "autoencoder training epoch 152: decode layer difference: 4712.905874803773\n",
      "autoencoder training epoch 153: decode layer difference: 4713.386879543025\n",
      "autoencoder training epoch 154: decode layer difference: 4712.467517084592\n",
      "autoencoder training epoch 155: decode layer difference: 4711.957869454154\n",
      "autoencoder training epoch 156: decode layer difference: 4711.290017860422\n",
      "autoencoder training epoch 157: decode layer difference: 4714.179608020497\n",
      "autoencoder training epoch 158: decode layer difference: 4713.751922755651\n",
      "autoencoder training epoch 159: decode layer difference: 4713.446363178149\n",
      "autoencoder training epoch 160: decode layer difference: 4711.300109398788\n",
      "autoencoder training epoch 161: decode layer difference: 4712.744808757288\n",
      "autoencoder training epoch 162: decode layer difference: 4712.249212297903\n",
      "autoencoder training epoch 163: decode layer difference: 4711.566437236841\n",
      "autoencoder training epoch 164: decode layer difference: 4711.96956678085\n",
      "autoencoder training epoch 165: decode layer difference: 4710.95002053595\n",
      "autoencoder training epoch 166: decode layer difference: 4711.4095400672795\n",
      "autoencoder training epoch 167: decode layer difference: 4712.358734350321\n",
      "autoencoder training epoch 168: decode layer difference: 4711.859416592185\n",
      "autoencoder training epoch 169: decode layer difference: 4710.513614452733\n",
      "autoencoder training epoch 170: decode layer difference: 4711.440756060253\n",
      "autoencoder training epoch 171: decode layer difference: 4713.726292024062\n",
      "autoencoder training epoch 172: decode layer difference: 4713.177844202202\n",
      "autoencoder training epoch 173: decode layer difference: 4711.711123349672\n",
      "autoencoder training epoch 174: decode layer difference: 4713.236236131577\n",
      "autoencoder training epoch 175: decode layer difference: 4711.923188489641\n",
      "autoencoder training epoch 176: decode layer difference: 4724.706193485868\n",
      "autoencoder training epoch 177: decode layer difference: 4714.349972914039\n",
      "autoencoder training epoch 178: decode layer difference: 4711.651178743002\n",
      "autoencoder training epoch 179: decode layer difference: 4711.378733320656\n",
      "autoencoder training epoch 180: decode layer difference: 4715.207013046372\n",
      "autoencoder training epoch 181: decode layer difference: 4711.462255408394\n",
      "autoencoder training epoch 182: decode layer difference: 4712.991111502271\n",
      "autoencoder training epoch 183: decode layer difference: 4711.171581117525\n",
      "autoencoder training epoch 184: decode layer difference: 4711.697975294215\n",
      "autoencoder training epoch 185: decode layer difference: 4711.66801076689\n",
      "autoencoder training epoch 186: decode layer difference: 4711.623441107593\n",
      "autoencoder training epoch 187: decode layer difference: 4711.064164500652\n",
      "autoencoder training epoch 188: decode layer difference: 4711.861450657898\n",
      "autoencoder training epoch 189: decode layer difference: 4712.609706799081\n",
      "autoencoder training epoch 190: decode layer difference: 4716.926601809998\n",
      "autoencoder training epoch 191: decode layer difference: 4713.494209179946\n",
      "autoencoder training epoch 192: decode layer difference: 4714.3740047107185\n",
      "autoencoder training epoch 193: decode layer difference: 4715.656408225711\n",
      "autoencoder training epoch 194: decode layer difference: 4713.642977773143\n",
      "autoencoder training epoch 195: decode layer difference: 4712.550453815993\n",
      "autoencoder training epoch 196: decode layer difference: 4711.275821299536\n",
      "autoencoder training epoch 197: decode layer difference: 4711.429992337715\n",
      "autoencoder training epoch 198: decode layer difference: 4715.460613076247\n",
      "autoencoder training epoch 199: decode layer difference: 4712.581629067989\n",
      "autoencoder training epoch 200: decode layer difference: 4710.881472587808\n",
      "training epoch 1: error: 26.960168789847906\n",
      "training epoch 2: error: 37.16925152618835\n",
      "training epoch 3: error: 62.92098037443104\n",
      "training epoch 4: error: 17.526627322493958\n",
      "training epoch 5: error: 16.65100196319387\n",
      "training epoch 6: error: 15.483739855982119\n",
      "training epoch 7: error: 12.366412988697459\n",
      "training epoch 8: error: 12.011311317496789\n",
      "training epoch 9: error: 11.466431237432982\n",
      "training epoch 10: error: 10.950825898349894\n",
      "training epoch 11: error: 10.843715910209056\n",
      "training epoch 12: error: 21.27313526498733\n",
      "training epoch 13: error: 9.560176932968307\n",
      "training epoch 14: error: 10.243386272072357\n",
      "training epoch 15: error: 11.550935190698889\n",
      "training epoch 16: error: 9.118924355674078\n",
      "training epoch 17: error: 9.620976822500708\n",
      "training epoch 18: error: 9.188255361993237\n",
      "training epoch 19: error: 9.621486167088856\n",
      "training epoch 20: error: 9.232493458935435\n",
      "\n",
      "fold 1: mse: 1.9349875179143115\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 4808.443569957918\n",
      "autoencoder training epoch 2: decode layer difference: 4724.2824656971725\n",
      "autoencoder training epoch 3: decode layer difference: 4712.126457413724\n",
      "autoencoder training epoch 4: decode layer difference: 4710.813845125094\n",
      "autoencoder training epoch 5: decode layer difference: 4708.20098185176\n",
      "autoencoder training epoch 6: decode layer difference: 4707.954538601541\n",
      "autoencoder training epoch 7: decode layer difference: 4706.4762105099735\n",
      "autoencoder training epoch 8: decode layer difference: 4707.30011864308\n",
      "autoencoder training epoch 9: decode layer difference: 4706.301697846883\n",
      "autoencoder training epoch 10: decode layer difference: 4705.981739993406\n",
      "autoencoder training epoch 11: decode layer difference: 4705.365654145195\n",
      "autoencoder training epoch 12: decode layer difference: 4705.281348147872\n",
      "autoencoder training epoch 13: decode layer difference: 4705.306776025509\n",
      "autoencoder training epoch 14: decode layer difference: 4704.36880749046\n",
      "autoencoder training epoch 15: decode layer difference: 4705.515975889291\n",
      "autoencoder training epoch 16: decode layer difference: 4705.435668087606\n",
      "autoencoder training epoch 17: decode layer difference: 4708.649292559654\n",
      "autoencoder training epoch 18: decode layer difference: 4704.381153315048\n",
      "autoencoder training epoch 19: decode layer difference: 4703.845102016978\n",
      "autoencoder training epoch 20: decode layer difference: 4704.453949091602\n",
      "autoencoder training epoch 21: decode layer difference: 4704.140400594084\n",
      "autoencoder training epoch 22: decode layer difference: 4743.925476657281\n",
      "autoencoder training epoch 23: decode layer difference: 4703.788335410717\n",
      "autoencoder training epoch 24: decode layer difference: 4704.794515070899\n",
      "autoencoder training epoch 25: decode layer difference: 4706.178966669606\n",
      "autoencoder training epoch 26: decode layer difference: 4704.672919279221\n",
      "autoencoder training epoch 27: decode layer difference: 4703.05392291682\n",
      "autoencoder training epoch 28: decode layer difference: 4702.436743659253\n",
      "autoencoder training epoch 29: decode layer difference: 4703.744178216668\n",
      "autoencoder training epoch 30: decode layer difference: 4703.768588887073\n",
      "autoencoder training epoch 31: decode layer difference: 4703.412804299371\n",
      "autoencoder training epoch 32: decode layer difference: 4704.262534647164\n",
      "autoencoder training epoch 33: decode layer difference: 4703.415841644757\n",
      "autoencoder training epoch 34: decode layer difference: 4711.035337912352\n",
      "autoencoder training epoch 35: decode layer difference: 4703.608279198162\n",
      "autoencoder training epoch 36: decode layer difference: 4701.683484759949\n",
      "autoencoder training epoch 37: decode layer difference: 4703.486091741599\n",
      "autoencoder training epoch 38: decode layer difference: 4702.840785268467\n",
      "autoencoder training epoch 39: decode layer difference: 4702.537605703453\n",
      "autoencoder training epoch 40: decode layer difference: 4701.829849466287\n",
      "autoencoder training epoch 41: decode layer difference: 4702.590908488589\n",
      "autoencoder training epoch 42: decode layer difference: 4701.475113585269\n",
      "autoencoder training epoch 43: decode layer difference: 4700.939153274744\n",
      "autoencoder training epoch 44: decode layer difference: 4701.826842946773\n",
      "autoencoder training epoch 45: decode layer difference: 4705.411386911087\n",
      "autoencoder training epoch 46: decode layer difference: 4700.93972333983\n",
      "autoencoder training epoch 47: decode layer difference: 4702.6297519558775\n",
      "autoencoder training epoch 48: decode layer difference: 4699.392551840105\n",
      "autoencoder training epoch 49: decode layer difference: 4701.191236182018\n",
      "autoencoder training epoch 50: decode layer difference: 4697.980714202192\n",
      "autoencoder training epoch 51: decode layer difference: 4698.517741727164\n",
      "autoencoder training epoch 52: decode layer difference: 4698.854286986105\n",
      "autoencoder training epoch 53: decode layer difference: 4696.4746674014805\n",
      "autoencoder training epoch 54: decode layer difference: 4698.80036285208\n",
      "autoencoder training epoch 55: decode layer difference: 4698.996398067341\n",
      "autoencoder training epoch 56: decode layer difference: 4698.341530693006\n",
      "autoencoder training epoch 57: decode layer difference: 4699.656047895044\n",
      "autoencoder training epoch 58: decode layer difference: 4697.619801782863\n",
      "autoencoder training epoch 59: decode layer difference: 4699.30593775675\n",
      "autoencoder training epoch 60: decode layer difference: 4697.23854129805\n",
      "autoencoder training epoch 61: decode layer difference: 4697.486052768733\n",
      "autoencoder training epoch 62: decode layer difference: 4696.676509515695\n",
      "autoencoder training epoch 63: decode layer difference: 4698.426350330725\n",
      "autoencoder training epoch 64: decode layer difference: 4696.288200971341\n",
      "autoencoder training epoch 65: decode layer difference: 4696.962912816232\n",
      "autoencoder training epoch 66: decode layer difference: 4697.480460168491\n",
      "autoencoder training epoch 67: decode layer difference: 4699.728658250225\n",
      "autoencoder training epoch 68: decode layer difference: 4697.6387501422\n",
      "autoencoder training epoch 69: decode layer difference: 4697.8977445612945\n",
      "autoencoder training epoch 70: decode layer difference: 4704.234229158676\n",
      "autoencoder training epoch 71: decode layer difference: 4697.136407329246\n",
      "autoencoder training epoch 72: decode layer difference: 4700.205282665823\n",
      "autoencoder training epoch 73: decode layer difference: 4740.585806460717\n",
      "autoencoder training epoch 74: decode layer difference: 4697.070378436739\n",
      "autoencoder training epoch 75: decode layer difference: 4696.2762290977535\n",
      "autoencoder training epoch 76: decode layer difference: 4700.834345918889\n",
      "autoencoder training epoch 77: decode layer difference: 4697.901412397741\n",
      "autoencoder training epoch 78: decode layer difference: 4698.406810900752\n",
      "autoencoder training epoch 79: decode layer difference: 4696.611920904361\n",
      "autoencoder training epoch 80: decode layer difference: 4696.928588778799\n",
      "autoencoder training epoch 81: decode layer difference: 4698.550831930522\n",
      "autoencoder training epoch 82: decode layer difference: 4697.994911628462\n",
      "autoencoder training epoch 83: decode layer difference: 4696.2061890702535\n",
      "autoencoder training epoch 84: decode layer difference: 4696.237650314408\n",
      "autoencoder training epoch 85: decode layer difference: 4696.390218751053\n",
      "autoencoder training epoch 86: decode layer difference: 4697.684033101723\n",
      "autoencoder training epoch 87: decode layer difference: 4698.097976410021\n",
      "autoencoder training epoch 88: decode layer difference: 4696.455902135487\n",
      "autoencoder training epoch 89: decode layer difference: 4697.190201411976\n",
      "autoencoder training epoch 90: decode layer difference: 4697.077907621052\n",
      "autoencoder training epoch 91: decode layer difference: 4698.613615765036\n",
      "autoencoder training epoch 92: decode layer difference: 4699.299561023008\n",
      "autoencoder training epoch 93: decode layer difference: 4696.842064644754\n",
      "autoencoder training epoch 94: decode layer difference: 4699.142114268024\n",
      "autoencoder training epoch 95: decode layer difference: 4697.893517751719\n",
      "autoencoder training epoch 96: decode layer difference: 4696.610094280519\n",
      "autoencoder training epoch 97: decode layer difference: 4695.729631990216\n",
      "autoencoder training epoch 98: decode layer difference: 4697.081516458315\n",
      "autoencoder training epoch 99: decode layer difference: 4696.369026922672\n",
      "autoencoder training epoch 100: decode layer difference: 4701.486599061945\n",
      "autoencoder training epoch 101: decode layer difference: 4697.5691837035765\n",
      "autoencoder training epoch 102: decode layer difference: 4695.963988197083\n",
      "autoencoder training epoch 103: decode layer difference: 4696.299817448728\n",
      "autoencoder training epoch 104: decode layer difference: 4705.529102692892\n",
      "autoencoder training epoch 105: decode layer difference: 4699.68960997963\n",
      "autoencoder training epoch 106: decode layer difference: 4697.9394101118\n",
      "autoencoder training epoch 107: decode layer difference: 4696.208398180021\n",
      "autoencoder training epoch 108: decode layer difference: 4696.727660090994\n",
      "autoencoder training epoch 109: decode layer difference: 4702.483548534761\n",
      "autoencoder training epoch 110: decode layer difference: 4698.587832659465\n",
      "autoencoder training epoch 111: decode layer difference: 4697.566945222854\n",
      "autoencoder training epoch 112: decode layer difference: 4696.515348789725\n",
      "autoencoder training epoch 113: decode layer difference: 4697.250915027998\n",
      "autoencoder training epoch 114: decode layer difference: 4697.58109172021\n",
      "autoencoder training epoch 115: decode layer difference: 4697.885078294313\n",
      "autoencoder training epoch 116: decode layer difference: 4698.194455975874\n",
      "autoencoder training epoch 117: decode layer difference: 4696.943282619756\n",
      "autoencoder training epoch 118: decode layer difference: 4696.997223024999\n",
      "autoencoder training epoch 119: decode layer difference: 4695.8279041410915\n",
      "autoencoder training epoch 120: decode layer difference: 4696.714297642706\n",
      "autoencoder training epoch 121: decode layer difference: 4698.022643652563\n",
      "autoencoder training epoch 122: decode layer difference: 4696.478770548675\n",
      "autoencoder training epoch 123: decode layer difference: 4696.298418365062\n",
      "autoencoder training epoch 124: decode layer difference: 4699.985426776948\n",
      "autoencoder training epoch 125: decode layer difference: 4697.353233962148\n",
      "autoencoder training epoch 126: decode layer difference: 4696.765729967016\n",
      "autoencoder training epoch 127: decode layer difference: 4712.012536437238\n",
      "autoencoder training epoch 128: decode layer difference: 4699.303166068844\n",
      "autoencoder training epoch 129: decode layer difference: 4695.4369978247305\n",
      "autoencoder training epoch 130: decode layer difference: 4696.536667166004\n",
      "autoencoder training epoch 131: decode layer difference: 4699.1336563011255\n",
      "autoencoder training epoch 132: decode layer difference: 4696.3118607273445\n",
      "autoencoder training epoch 133: decode layer difference: 4697.240608450061\n",
      "autoencoder training epoch 134: decode layer difference: 4695.281601398176\n",
      "autoencoder training epoch 135: decode layer difference: 4695.390779956416\n",
      "autoencoder training epoch 136: decode layer difference: 4699.02088689007\n",
      "autoencoder training epoch 137: decode layer difference: 4696.868373990414\n",
      "autoencoder training epoch 138: decode layer difference: 4705.979285446749\n",
      "autoencoder training epoch 139: decode layer difference: 4703.589830148597\n",
      "autoencoder training epoch 140: decode layer difference: 4695.375509737753\n",
      "autoencoder training epoch 141: decode layer difference: 4697.067934012915\n",
      "autoencoder training epoch 142: decode layer difference: 4696.209486592723\n",
      "autoencoder training epoch 143: decode layer difference: 4696.525100139562\n",
      "autoencoder training epoch 144: decode layer difference: 4695.500818463689\n",
      "autoencoder training epoch 145: decode layer difference: 4697.938641163237\n",
      "autoencoder training epoch 146: decode layer difference: 4696.1676544172515\n",
      "autoencoder training epoch 147: decode layer difference: 4695.774948330401\n",
      "autoencoder training epoch 148: decode layer difference: 4696.604079368739\n",
      "autoencoder training epoch 149: decode layer difference: 4695.82381866226\n",
      "autoencoder training epoch 150: decode layer difference: 4727.955527089063\n",
      "autoencoder training epoch 151: decode layer difference: 4696.068391054849\n",
      "autoencoder training epoch 152: decode layer difference: 4695.361978166318\n",
      "autoencoder training epoch 153: decode layer difference: 4707.195030602405\n",
      "autoencoder training epoch 154: decode layer difference: 4700.038138980279\n",
      "autoencoder training epoch 155: decode layer difference: 4695.3869047756225\n",
      "autoencoder training epoch 156: decode layer difference: 4699.046372623232\n",
      "autoencoder training epoch 157: decode layer difference: 4699.188192813571\n",
      "autoencoder training epoch 158: decode layer difference: 4699.575696392787\n",
      "autoencoder training epoch 159: decode layer difference: 4696.174789007153\n",
      "autoencoder training epoch 160: decode layer difference: 4695.501861143848\n",
      "autoencoder training epoch 161: decode layer difference: 4699.656851253788\n",
      "autoencoder training epoch 162: decode layer difference: 4695.701259850391\n",
      "autoencoder training epoch 163: decode layer difference: 4694.951470747811\n",
      "autoencoder training epoch 164: decode layer difference: 4707.520603882925\n",
      "autoencoder training epoch 165: decode layer difference: 4695.401661688193\n",
      "autoencoder training epoch 166: decode layer difference: 4696.198055021806\n",
      "autoencoder training epoch 167: decode layer difference: 4695.640389159187\n",
      "autoencoder training epoch 168: decode layer difference: 4695.563712786811\n",
      "autoencoder training epoch 169: decode layer difference: 4696.17396214386\n",
      "autoencoder training epoch 170: decode layer difference: 4696.798394932623\n",
      "autoencoder training epoch 171: decode layer difference: 4696.037980790062\n",
      "autoencoder training epoch 172: decode layer difference: 4695.672782629683\n",
      "autoencoder training epoch 173: decode layer difference: 4700.411509372832\n",
      "autoencoder training epoch 174: decode layer difference: 4696.174342702053\n",
      "autoencoder training epoch 175: decode layer difference: 4695.76713629852\n",
      "autoencoder training epoch 176: decode layer difference: 4704.820637638493\n",
      "autoencoder training epoch 177: decode layer difference: 4695.721815970037\n",
      "autoencoder training epoch 178: decode layer difference: 4696.122615437422\n",
      "autoencoder training epoch 179: decode layer difference: 4695.804219831244\n",
      "autoencoder training epoch 180: decode layer difference: 4696.673992438905\n",
      "autoencoder training epoch 181: decode layer difference: 4698.419765934272\n",
      "autoencoder training epoch 182: decode layer difference: 4697.609161106439\n",
      "autoencoder training epoch 183: decode layer difference: 4720.356064572541\n",
      "autoencoder training epoch 184: decode layer difference: 4698.333031930708\n",
      "autoencoder training epoch 185: decode layer difference: 4697.403554992634\n",
      "autoencoder training epoch 186: decode layer difference: 4694.955367559331\n",
      "autoencoder training epoch 187: decode layer difference: 4695.061373654815\n",
      "autoencoder training epoch 188: decode layer difference: 4695.46997642186\n",
      "autoencoder training epoch 189: decode layer difference: 4697.0565817929\n",
      "autoencoder training epoch 190: decode layer difference: 4697.126455383725\n",
      "autoencoder training epoch 191: decode layer difference: 4699.832786330405\n",
      "autoencoder training epoch 192: decode layer difference: 4695.749144902689\n",
      "autoencoder training epoch 193: decode layer difference: 4699.4796939026855\n",
      "autoencoder training epoch 194: decode layer difference: 4694.883444956537\n",
      "autoencoder training epoch 195: decode layer difference: 4695.252192079655\n",
      "autoencoder training epoch 196: decode layer difference: 4697.084854108774\n",
      "autoencoder training epoch 197: decode layer difference: 4694.7801123736035\n",
      "autoencoder training epoch 198: decode layer difference: 4697.427779392822\n",
      "autoencoder training epoch 199: decode layer difference: 4697.119160176741\n",
      "autoencoder training epoch 200: decode layer difference: 4696.598639198233\n",
      "training epoch 1: error: 18.47001819349495\n",
      "training epoch 2: error: 19.58902994550039\n",
      "training epoch 3: error: 17.860554221988245\n",
      "training epoch 4: error: 22.867577000484705\n",
      "training epoch 5: error: 15.786904464100125\n",
      "training epoch 6: error: 12.108568042644896\n",
      "training epoch 7: error: 12.157084253846875\n",
      "training epoch 8: error: 13.05236004283362\n",
      "training epoch 9: error: 11.921485220298575\n",
      "training epoch 10: error: 11.084201204253345\n",
      "training epoch 11: error: 10.38502899865427\n",
      "training epoch 12: error: 9.556343463414466\n",
      "training epoch 13: error: 9.257980054151002\n",
      "training epoch 14: error: 9.214177150771437\n",
      "training epoch 15: error: 12.416211306446048\n",
      "training epoch 16: error: 8.968414906300346\n",
      "training epoch 17: error: 12.801782647020998\n",
      "training epoch 18: error: 9.393846523414632\n",
      "training epoch 19: error: 9.183759345770358\n",
      "training epoch 20: error: 8.873007914997286\n",
      "\n",
      "fold 2: mse: 2.152364159392275\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 4853.355666197034\n",
      "autoencoder training epoch 2: decode layer difference: 4743.558577496547\n",
      "autoencoder training epoch 3: decode layer difference: 4731.717537659652\n",
      "autoencoder training epoch 4: decode layer difference: 4732.941193594813\n",
      "autoencoder training epoch 5: decode layer difference: 4730.737571811175\n",
      "autoencoder training epoch 6: decode layer difference: 4729.4526550813825\n",
      "autoencoder training epoch 7: decode layer difference: 4731.37354132148\n",
      "autoencoder training epoch 8: decode layer difference: 4731.89604895959\n",
      "autoencoder training epoch 9: decode layer difference: 4727.560989329096\n",
      "autoencoder training epoch 10: decode layer difference: 4728.248087330173\n",
      "autoencoder training epoch 11: decode layer difference: 4727.51563619941\n",
      "autoencoder training epoch 12: decode layer difference: 4726.000293747019\n",
      "autoencoder training epoch 13: decode layer difference: 4727.2824777535525\n",
      "autoencoder training epoch 14: decode layer difference: 4726.493986141164\n",
      "autoencoder training epoch 15: decode layer difference: 4726.325032875267\n",
      "autoencoder training epoch 16: decode layer difference: 4726.542690631834\n",
      "autoencoder training epoch 17: decode layer difference: 4727.268035140238\n",
      "autoencoder training epoch 18: decode layer difference: 4726.725738732251\n",
      "autoencoder training epoch 19: decode layer difference: 4729.780588305589\n",
      "autoencoder training epoch 20: decode layer difference: 4727.086925038119\n",
      "autoencoder training epoch 21: decode layer difference: 4727.484219715666\n",
      "autoencoder training epoch 22: decode layer difference: 4726.342854561026\n",
      "autoencoder training epoch 23: decode layer difference: 4725.357198968981\n",
      "autoencoder training epoch 24: decode layer difference: 4726.098138681587\n",
      "autoencoder training epoch 25: decode layer difference: 4726.289238322822\n",
      "autoencoder training epoch 26: decode layer difference: 4726.59076068094\n",
      "autoencoder training epoch 27: decode layer difference: 4725.571976255641\n",
      "autoencoder training epoch 28: decode layer difference: 4724.544131046402\n",
      "autoencoder training epoch 29: decode layer difference: 4727.724813720942\n",
      "autoencoder training epoch 30: decode layer difference: 4724.678296771079\n",
      "autoencoder training epoch 31: decode layer difference: 4724.3761410180905\n",
      "autoencoder training epoch 32: decode layer difference: 4725.644883083999\n",
      "autoencoder training epoch 33: decode layer difference: 4724.677369230327\n",
      "autoencoder training epoch 34: decode layer difference: 4725.826852491861\n",
      "autoencoder training epoch 35: decode layer difference: 4725.097233229449\n",
      "autoencoder training epoch 36: decode layer difference: 4725.854821808836\n",
      "autoencoder training epoch 37: decode layer difference: 4725.744577695243\n",
      "autoencoder training epoch 38: decode layer difference: 4725.450799144786\n",
      "autoencoder training epoch 39: decode layer difference: 4725.038092538123\n",
      "autoencoder training epoch 40: decode layer difference: 4725.330409111409\n",
      "autoencoder training epoch 41: decode layer difference: 4725.207774686859\n",
      "autoencoder training epoch 42: decode layer difference: 4724.471425435456\n",
      "autoencoder training epoch 43: decode layer difference: 4726.8888046373395\n",
      "autoencoder training epoch 44: decode layer difference: 4723.978894309139\n",
      "autoencoder training epoch 45: decode layer difference: 4724.084152718406\n",
      "autoencoder training epoch 46: decode layer difference: 4729.732743784507\n",
      "autoencoder training epoch 47: decode layer difference: 4723.586976948796\n",
      "autoencoder training epoch 48: decode layer difference: 4724.176485950816\n",
      "autoencoder training epoch 49: decode layer difference: 4723.587185228121\n",
      "autoencoder training epoch 50: decode layer difference: 4722.575225074999\n",
      "autoencoder training epoch 51: decode layer difference: 4723.117131090099\n",
      "autoencoder training epoch 52: decode layer difference: 4723.382974210823\n",
      "autoencoder training epoch 53: decode layer difference: 4722.26024842441\n",
      "autoencoder training epoch 54: decode layer difference: 4722.281504404798\n",
      "autoencoder training epoch 55: decode layer difference: 4723.288811467522\n",
      "autoencoder training epoch 56: decode layer difference: 4722.003930164765\n",
      "autoencoder training epoch 57: decode layer difference: 4720.8526009460475\n",
      "autoencoder training epoch 58: decode layer difference: 4718.614930106331\n",
      "autoencoder training epoch 59: decode layer difference: 4718.937337950326\n",
      "autoencoder training epoch 60: decode layer difference: 4718.517461945701\n",
      "autoencoder training epoch 61: decode layer difference: 4718.573464433048\n",
      "autoencoder training epoch 62: decode layer difference: 4717.748004985392\n",
      "autoencoder training epoch 63: decode layer difference: 4717.940399821601\n",
      "autoencoder training epoch 64: decode layer difference: 4718.888491400662\n",
      "autoencoder training epoch 65: decode layer difference: 4718.544989899427\n",
      "autoencoder training epoch 66: decode layer difference: 4719.341946100206\n",
      "autoencoder training epoch 67: decode layer difference: 4719.423603356762\n",
      "autoencoder training epoch 68: decode layer difference: 4719.5524125128195\n",
      "autoencoder training epoch 69: decode layer difference: 4722.7189707662055\n",
      "autoencoder training epoch 70: decode layer difference: 4723.517956961419\n",
      "autoencoder training epoch 71: decode layer difference: 4717.237177683691\n",
      "autoencoder training epoch 72: decode layer difference: 4715.588052252985\n",
      "autoencoder training epoch 73: decode layer difference: 4717.522663560815\n",
      "autoencoder training epoch 74: decode layer difference: 4717.704197081008\n",
      "autoencoder training epoch 75: decode layer difference: 4719.037013330377\n",
      "autoencoder training epoch 76: decode layer difference: 4716.857899475803\n",
      "autoencoder training epoch 77: decode layer difference: 4718.38273266463\n",
      "autoencoder training epoch 78: decode layer difference: 4717.905696311695\n",
      "autoencoder training epoch 79: decode layer difference: 4730.59591307576\n",
      "autoencoder training epoch 80: decode layer difference: 4719.476624058672\n",
      "autoencoder training epoch 81: decode layer difference: 4719.07813177288\n",
      "autoencoder training epoch 82: decode layer difference: 4718.914582892488\n",
      "autoencoder training epoch 83: decode layer difference: 4716.23172038017\n",
      "autoencoder training epoch 84: decode layer difference: 4718.7510728384805\n",
      "autoencoder training epoch 85: decode layer difference: 4715.622776482178\n",
      "autoencoder training epoch 86: decode layer difference: 4719.5405414674115\n",
      "autoencoder training epoch 87: decode layer difference: 4716.02741871972\n",
      "autoencoder training epoch 88: decode layer difference: 4716.43372230263\n",
      "autoencoder training epoch 89: decode layer difference: 4716.5125041486\n",
      "autoencoder training epoch 90: decode layer difference: 4721.363847085161\n",
      "autoencoder training epoch 91: decode layer difference: 4716.825648672766\n",
      "autoencoder training epoch 92: decode layer difference: 4715.501988018677\n",
      "autoencoder training epoch 93: decode layer difference: 4723.475016947845\n",
      "autoencoder training epoch 94: decode layer difference: 4719.815562163447\n",
      "autoencoder training epoch 95: decode layer difference: 4717.034995904483\n",
      "autoencoder training epoch 96: decode layer difference: 4715.745337017709\n",
      "autoencoder training epoch 97: decode layer difference: 4716.265872168669\n",
      "autoencoder training epoch 98: decode layer difference: 4718.3553416662435\n",
      "autoencoder training epoch 99: decode layer difference: 4715.5855479666\n",
      "autoencoder training epoch 100: decode layer difference: 4715.6858853542935\n",
      "autoencoder training epoch 101: decode layer difference: 4715.01111910836\n",
      "autoencoder training epoch 102: decode layer difference: 4716.539485682411\n",
      "autoencoder training epoch 103: decode layer difference: 4716.830791314182\n",
      "autoencoder training epoch 104: decode layer difference: 4717.098682984115\n",
      "autoencoder training epoch 105: decode layer difference: 4718.104585127712\n",
      "autoencoder training epoch 106: decode layer difference: 4715.593825279109\n",
      "autoencoder training epoch 107: decode layer difference: 4717.487464506505\n",
      "autoencoder training epoch 108: decode layer difference: 4715.7864419764\n",
      "autoencoder training epoch 109: decode layer difference: 4715.493151618012\n",
      "autoencoder training epoch 110: decode layer difference: 4717.804723608417\n",
      "autoencoder training epoch 111: decode layer difference: 4718.037543501365\n",
      "autoencoder training epoch 112: decode layer difference: 4718.659814754814\n",
      "autoencoder training epoch 113: decode layer difference: 4714.866048086369\n",
      "autoencoder training epoch 114: decode layer difference: 4722.191661198619\n",
      "autoencoder training epoch 115: decode layer difference: 4716.229110399841\n",
      "autoencoder training epoch 116: decode layer difference: 4717.978910839216\n",
      "autoencoder training epoch 117: decode layer difference: 4721.974046935253\n",
      "autoencoder training epoch 118: decode layer difference: 4717.056949867816\n",
      "autoencoder training epoch 119: decode layer difference: 4717.497138544391\n",
      "autoencoder training epoch 120: decode layer difference: 4716.178457473202\n",
      "autoencoder training epoch 121: decode layer difference: 4721.39873937322\n",
      "autoencoder training epoch 122: decode layer difference: 4718.604134354867\n",
      "autoencoder training epoch 123: decode layer difference: 4716.665871281111\n",
      "autoencoder training epoch 124: decode layer difference: 4715.697992862514\n",
      "autoencoder training epoch 125: decode layer difference: 4716.079195566809\n",
      "autoencoder training epoch 126: decode layer difference: 4719.950748947759\n",
      "autoencoder training epoch 127: decode layer difference: 4716.411977569079\n",
      "autoencoder training epoch 128: decode layer difference: 4715.437754641692\n",
      "autoencoder training epoch 129: decode layer difference: 4715.569705461554\n",
      "autoencoder training epoch 130: decode layer difference: 4721.997458842496\n",
      "autoencoder training epoch 131: decode layer difference: 4715.557787928046\n",
      "autoencoder training epoch 132: decode layer difference: 4717.945016240175\n",
      "autoencoder training epoch 133: decode layer difference: 4715.091124253719\n",
      "autoencoder training epoch 134: decode layer difference: 4721.367207454007\n",
      "autoencoder training epoch 135: decode layer difference: 4717.915837980301\n",
      "autoencoder training epoch 136: decode layer difference: 4715.132993505166\n",
      "autoencoder training epoch 137: decode layer difference: 4716.368372567738\n",
      "autoencoder training epoch 138: decode layer difference: 4715.442445344226\n",
      "autoencoder training epoch 139: decode layer difference: 4716.392967338192\n",
      "autoencoder training epoch 140: decode layer difference: 4715.714042444312\n",
      "autoencoder training epoch 141: decode layer difference: 4719.948349968845\n",
      "autoencoder training epoch 142: decode layer difference: 4716.363315415541\n",
      "autoencoder training epoch 143: decode layer difference: 4717.591822985779\n",
      "autoencoder training epoch 144: decode layer difference: 4715.24244940601\n",
      "autoencoder training epoch 145: decode layer difference: 4718.566636409787\n",
      "autoencoder training epoch 146: decode layer difference: 4714.5201705440695\n",
      "autoencoder training epoch 147: decode layer difference: 4715.209756212804\n",
      "autoencoder training epoch 148: decode layer difference: 4717.206742561191\n",
      "autoencoder training epoch 149: decode layer difference: 4715.573176146496\n",
      "autoencoder training epoch 150: decode layer difference: 4722.351864558291\n",
      "autoencoder training epoch 151: decode layer difference: 4718.027631884953\n",
      "autoencoder training epoch 152: decode layer difference: 4715.939553646338\n",
      "autoencoder training epoch 153: decode layer difference: 4715.084223599353\n",
      "autoencoder training epoch 154: decode layer difference: 4715.862521679505\n",
      "autoencoder training epoch 155: decode layer difference: 4714.958552613035\n",
      "autoencoder training epoch 156: decode layer difference: 4714.814640006342\n",
      "autoencoder training epoch 157: decode layer difference: 4716.690157320817\n",
      "autoencoder training epoch 158: decode layer difference: 4715.130983378514\n",
      "autoencoder training epoch 159: decode layer difference: 4728.595934130166\n",
      "autoencoder training epoch 160: decode layer difference: 4715.316793864702\n",
      "autoencoder training epoch 161: decode layer difference: 4715.6329957971475\n",
      "autoencoder training epoch 162: decode layer difference: 4715.960655437214\n",
      "autoencoder training epoch 163: decode layer difference: 4715.774675286382\n",
      "autoencoder training epoch 164: decode layer difference: 4716.5566483424045\n",
      "autoencoder training epoch 165: decode layer difference: 4727.494124035499\n",
      "autoencoder training epoch 166: decode layer difference: 4717.887194202152\n",
      "autoencoder training epoch 167: decode layer difference: 4727.588643848236\n",
      "autoencoder training epoch 168: decode layer difference: 4717.426977835367\n",
      "autoencoder training epoch 169: decode layer difference: 4715.0869090103115\n",
      "autoencoder training epoch 170: decode layer difference: 4717.359256573882\n",
      "autoencoder training epoch 171: decode layer difference: 4717.526899441422\n",
      "autoencoder training epoch 172: decode layer difference: 4715.193000850786\n",
      "autoencoder training epoch 173: decode layer difference: 4715.62948144425\n",
      "autoencoder training epoch 174: decode layer difference: 4716.364096607348\n",
      "autoencoder training epoch 175: decode layer difference: 4715.022916503791\n",
      "autoencoder training epoch 176: decode layer difference: 4715.40727025435\n",
      "autoencoder training epoch 177: decode layer difference: 4715.552221863635\n",
      "autoencoder training epoch 178: decode layer difference: 4715.537253640639\n",
      "autoencoder training epoch 179: decode layer difference: 4714.579768644708\n",
      "autoencoder training epoch 180: decode layer difference: 4715.208723451183\n",
      "autoencoder training epoch 181: decode layer difference: 4720.836739931189\n",
      "autoencoder training epoch 182: decode layer difference: 4715.768305733691\n",
      "autoencoder training epoch 183: decode layer difference: 4716.622580473796\n",
      "autoencoder training epoch 184: decode layer difference: 4725.009601486137\n",
      "autoencoder training epoch 185: decode layer difference: 4715.481035421753\n",
      "autoencoder training epoch 186: decode layer difference: 4715.164159074812\n",
      "autoencoder training epoch 187: decode layer difference: 4723.728680371726\n",
      "autoencoder training epoch 188: decode layer difference: 4717.48788935373\n",
      "autoencoder training epoch 189: decode layer difference: 4718.209037575396\n",
      "autoencoder training epoch 190: decode layer difference: 4714.464292707168\n",
      "autoencoder training epoch 191: decode layer difference: 4735.317546074895\n",
      "autoencoder training epoch 192: decode layer difference: 4731.4591096312815\n",
      "autoencoder training epoch 193: decode layer difference: 4715.73826064512\n",
      "autoencoder training epoch 194: decode layer difference: 4717.8253462646535\n",
      "autoencoder training epoch 195: decode layer difference: 4723.592597950197\n",
      "autoencoder training epoch 196: decode layer difference: 4716.593447125529\n",
      "autoencoder training epoch 197: decode layer difference: 4715.0606172081525\n",
      "autoencoder training epoch 198: decode layer difference: 4715.806275196192\n",
      "autoencoder training epoch 199: decode layer difference: 4716.184722483928\n",
      "autoencoder training epoch 200: decode layer difference: 4723.1153080203485\n",
      "training epoch 1: error: 17.662674433636703\n",
      "training epoch 2: error: 17.33911600821932\n",
      "training epoch 3: error: 17.73695703358755\n",
      "training epoch 4: error: 14.246974863159933\n",
      "training epoch 5: error: 12.655186593812367\n",
      "training epoch 6: error: 11.359882819734581\n",
      "training epoch 7: error: 12.445837649838207\n",
      "training epoch 8: error: 13.81448167979777\n",
      "training epoch 9: error: 12.63433284221988\n",
      "training epoch 10: error: 9.908306597029888\n",
      "training epoch 11: error: 9.46770352424953\n",
      "training epoch 12: error: 9.126566890678696\n",
      "training epoch 13: error: 8.925199247847681\n",
      "training epoch 14: error: 8.841458963807828\n",
      "training epoch 15: error: 10.424680805824666\n",
      "training epoch 16: error: 8.86369761893438\n",
      "training epoch 17: error: 8.756025859258722\n",
      "training epoch 18: error: 10.1895804355446\n",
      "training epoch 19: error: 9.319345600275522\n",
      "training epoch 20: error: 8.772987923884113\n",
      "\n",
      "fold 3: mse: 2.277690947484603\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 4819.676155013836\n",
      "autoencoder training epoch 2: decode layer difference: 4717.553802462151\n",
      "autoencoder training epoch 3: decode layer difference: 4707.451271531307\n",
      "autoencoder training epoch 4: decode layer difference: 4705.563764556087\n",
      "autoencoder training epoch 5: decode layer difference: 4707.73002991072\n",
      "autoencoder training epoch 6: decode layer difference: 4702.400146447015\n",
      "autoencoder training epoch 7: decode layer difference: 4699.39950108871\n",
      "autoencoder training epoch 8: decode layer difference: 4701.373056996411\n",
      "autoencoder training epoch 9: decode layer difference: 4699.476477054507\n",
      "autoencoder training epoch 10: decode layer difference: 4699.518894620724\n",
      "autoencoder training epoch 11: decode layer difference: 4701.332498477255\n",
      "autoencoder training epoch 12: decode layer difference: 4700.195240926494\n",
      "autoencoder training epoch 13: decode layer difference: 4698.165033946571\n",
      "autoencoder training epoch 14: decode layer difference: 4700.12782678552\n",
      "autoencoder training epoch 15: decode layer difference: 4699.6312732578945\n",
      "autoencoder training epoch 16: decode layer difference: 4699.669323035804\n",
      "autoencoder training epoch 17: decode layer difference: 4698.227437444102\n",
      "autoencoder training epoch 18: decode layer difference: 4701.27628409591\n",
      "autoencoder training epoch 19: decode layer difference: 4698.087245597127\n",
      "autoencoder training epoch 20: decode layer difference: 4697.813122581956\n",
      "autoencoder training epoch 21: decode layer difference: 4701.917869218819\n",
      "autoencoder training epoch 22: decode layer difference: 4698.532481693476\n",
      "autoencoder training epoch 23: decode layer difference: 4697.6989072139595\n",
      "autoencoder training epoch 24: decode layer difference: 4699.912979736565\n",
      "autoencoder training epoch 25: decode layer difference: 4698.0469742796495\n",
      "autoencoder training epoch 26: decode layer difference: 4697.491502273309\n",
      "autoencoder training epoch 27: decode layer difference: 4697.649936062673\n",
      "autoencoder training epoch 28: decode layer difference: 4697.379355108469\n",
      "autoencoder training epoch 29: decode layer difference: 4697.486471507163\n",
      "autoencoder training epoch 30: decode layer difference: 4696.824371804845\n",
      "autoencoder training epoch 31: decode layer difference: 4696.418145261709\n",
      "autoencoder training epoch 32: decode layer difference: 4700.500949419957\n",
      "autoencoder training epoch 33: decode layer difference: 4696.703582746327\n",
      "autoencoder training epoch 34: decode layer difference: 4697.747217303711\n",
      "autoencoder training epoch 35: decode layer difference: 4697.063167728865\n",
      "autoencoder training epoch 36: decode layer difference: 4696.63334281325\n",
      "autoencoder training epoch 37: decode layer difference: 4696.847312973948\n",
      "autoencoder training epoch 38: decode layer difference: 4695.505793436086\n",
      "autoencoder training epoch 39: decode layer difference: 4696.121874213794\n",
      "autoencoder training epoch 40: decode layer difference: 4696.177511593611\n",
      "autoencoder training epoch 41: decode layer difference: 4695.359538793988\n",
      "autoencoder training epoch 42: decode layer difference: 4696.138813295789\n",
      "autoencoder training epoch 43: decode layer difference: 4695.498282824859\n",
      "autoencoder training epoch 44: decode layer difference: 4704.5886414064225\n",
      "autoencoder training epoch 45: decode layer difference: 4694.1620258720195\n",
      "autoencoder training epoch 46: decode layer difference: 4694.396037554036\n",
      "autoencoder training epoch 47: decode layer difference: 4694.040929223342\n",
      "autoencoder training epoch 48: decode layer difference: 4694.8807268132\n",
      "autoencoder training epoch 49: decode layer difference: 4694.058234038213\n",
      "autoencoder training epoch 50: decode layer difference: 4693.369745595365\n",
      "autoencoder training epoch 51: decode layer difference: 4692.6328765130265\n",
      "autoencoder training epoch 52: decode layer difference: 4693.8647289585\n",
      "autoencoder training epoch 53: decode layer difference: 4692.106590807534\n",
      "autoencoder training epoch 54: decode layer difference: 4692.792264533471\n",
      "autoencoder training epoch 55: decode layer difference: 4693.011121511607\n",
      "autoencoder training epoch 56: decode layer difference: 4693.263085559977\n",
      "autoencoder training epoch 57: decode layer difference: 4691.40743644705\n",
      "autoencoder training epoch 58: decode layer difference: 4694.924090463266\n",
      "autoencoder training epoch 59: decode layer difference: 4692.615497242751\n",
      "autoencoder training epoch 60: decode layer difference: 4692.574174735118\n",
      "autoencoder training epoch 61: decode layer difference: 4691.465071285984\n",
      "autoencoder training epoch 62: decode layer difference: 4697.878531086592\n",
      "autoencoder training epoch 63: decode layer difference: 4699.05880527672\n",
      "autoencoder training epoch 64: decode layer difference: 4697.186104552583\n",
      "autoencoder training epoch 65: decode layer difference: 4691.5825985919955\n",
      "autoencoder training epoch 66: decode layer difference: 4690.966089618598\n",
      "autoencoder training epoch 67: decode layer difference: 4692.120492541223\n",
      "autoencoder training epoch 68: decode layer difference: 4694.551246879454\n",
      "autoencoder training epoch 69: decode layer difference: 4691.787648640672\n",
      "autoencoder training epoch 70: decode layer difference: 4691.459329925576\n",
      "autoencoder training epoch 71: decode layer difference: 4692.393089838218\n",
      "autoencoder training epoch 72: decode layer difference: 4690.986014918756\n",
      "autoencoder training epoch 73: decode layer difference: 4690.825914948555\n",
      "autoencoder training epoch 74: decode layer difference: 4691.09369635644\n",
      "autoencoder training epoch 75: decode layer difference: 4692.023104115015\n",
      "autoencoder training epoch 76: decode layer difference: 4691.6054483906555\n",
      "autoencoder training epoch 77: decode layer difference: 4691.460406207714\n",
      "autoencoder training epoch 78: decode layer difference: 4690.902997297404\n",
      "autoencoder training epoch 79: decode layer difference: 4690.9934439748595\n",
      "autoencoder training epoch 80: decode layer difference: 4690.270453492007\n",
      "autoencoder training epoch 81: decode layer difference: 4692.377502613046\n",
      "autoencoder training epoch 82: decode layer difference: 4690.992182177297\n",
      "autoencoder training epoch 83: decode layer difference: 4691.149016356256\n",
      "autoencoder training epoch 84: decode layer difference: 4691.267081684679\n",
      "autoencoder training epoch 85: decode layer difference: 4690.84915589803\n",
      "autoencoder training epoch 86: decode layer difference: 4691.924212208063\n",
      "autoencoder training epoch 87: decode layer difference: 4693.490353495672\n",
      "autoencoder training epoch 88: decode layer difference: 4692.010506007061\n",
      "autoencoder training epoch 89: decode layer difference: 4691.421146959933\n",
      "autoencoder training epoch 90: decode layer difference: 4693.129729390635\n",
      "autoencoder training epoch 91: decode layer difference: 4690.930346321494\n",
      "autoencoder training epoch 92: decode layer difference: 4690.166593964661\n",
      "autoencoder training epoch 93: decode layer difference: 4691.317070404386\n",
      "autoencoder training epoch 94: decode layer difference: 4690.544459141517\n",
      "autoencoder training epoch 95: decode layer difference: 4692.346339851481\n",
      "autoencoder training epoch 96: decode layer difference: 4691.121709819846\n",
      "autoencoder training epoch 97: decode layer difference: 4691.395232987554\n",
      "autoencoder training epoch 98: decode layer difference: 4691.221939569247\n",
      "autoencoder training epoch 99: decode layer difference: 4691.067356459216\n",
      "autoencoder training epoch 100: decode layer difference: 4690.3041506609425\n",
      "autoencoder training epoch 101: decode layer difference: 4710.460918391527\n",
      "autoencoder training epoch 102: decode layer difference: 4691.537500185981\n",
      "autoencoder training epoch 103: decode layer difference: 4691.339955413123\n",
      "autoencoder training epoch 104: decode layer difference: 4691.028207212674\n",
      "autoencoder training epoch 105: decode layer difference: 4694.143839961888\n",
      "autoencoder training epoch 106: decode layer difference: 4690.724052971102\n",
      "autoencoder training epoch 107: decode layer difference: 4691.152890210953\n",
      "autoencoder training epoch 108: decode layer difference: 4692.524288002205\n",
      "autoencoder training epoch 109: decode layer difference: 4690.449672368783\n",
      "autoencoder training epoch 110: decode layer difference: 4691.210802342687\n",
      "autoencoder training epoch 111: decode layer difference: 4690.710348044595\n",
      "autoencoder training epoch 112: decode layer difference: 4690.354127895487\n",
      "autoencoder training epoch 113: decode layer difference: 4691.18499393487\n",
      "autoencoder training epoch 114: decode layer difference: 4703.618723969659\n",
      "autoencoder training epoch 115: decode layer difference: 4691.160626922774\n",
      "autoencoder training epoch 116: decode layer difference: 4691.389835614422\n",
      "autoencoder training epoch 117: decode layer difference: 4689.613559701744\n",
      "autoencoder training epoch 118: decode layer difference: 4691.869856293873\n",
      "autoencoder training epoch 119: decode layer difference: 4690.418934547819\n",
      "autoencoder training epoch 120: decode layer difference: 4692.3053776485385\n",
      "autoencoder training epoch 121: decode layer difference: 4691.077505461166\n",
      "autoencoder training epoch 122: decode layer difference: 4697.398703104471\n",
      "autoencoder training epoch 123: decode layer difference: 4694.93288885445\n",
      "autoencoder training epoch 124: decode layer difference: 4690.820584816056\n",
      "autoencoder training epoch 125: decode layer difference: 4694.257298636788\n",
      "autoencoder training epoch 126: decode layer difference: 4690.706956533148\n",
      "autoencoder training epoch 127: decode layer difference: 4693.374422274758\n",
      "autoencoder training epoch 128: decode layer difference: 4690.015593806018\n",
      "autoencoder training epoch 129: decode layer difference: 4691.068105999645\n",
      "autoencoder training epoch 130: decode layer difference: 4690.643573984928\n",
      "autoencoder training epoch 131: decode layer difference: 4690.837773432292\n",
      "autoencoder training epoch 132: decode layer difference: 4689.895000042032\n",
      "autoencoder training epoch 133: decode layer difference: 4692.001773359013\n",
      "autoencoder training epoch 134: decode layer difference: 4690.278349028965\n",
      "autoencoder training epoch 135: decode layer difference: 4691.2184571586895\n",
      "autoencoder training epoch 136: decode layer difference: 4690.241460086869\n",
      "autoencoder training epoch 137: decode layer difference: 4691.4582406856225\n",
      "autoencoder training epoch 138: decode layer difference: 4690.811249162602\n",
      "autoencoder training epoch 139: decode layer difference: 4691.1737252746625\n",
      "autoencoder training epoch 140: decode layer difference: 4690.611379310502\n",
      "autoencoder training epoch 141: decode layer difference: 4690.372125399874\n",
      "autoencoder training epoch 142: decode layer difference: 4692.093610349055\n",
      "autoencoder training epoch 143: decode layer difference: 4690.911542361665\n",
      "autoencoder training epoch 144: decode layer difference: 4690.739036719604\n",
      "autoencoder training epoch 145: decode layer difference: 4690.751384899918\n",
      "autoencoder training epoch 146: decode layer difference: 4690.851612355347\n",
      "autoencoder training epoch 147: decode layer difference: 4691.195075264369\n",
      "autoencoder training epoch 148: decode layer difference: 4690.4489989842905\n",
      "autoencoder training epoch 149: decode layer difference: 4697.647012562258\n",
      "autoencoder training epoch 150: decode layer difference: 4697.007633206589\n",
      "autoencoder training epoch 151: decode layer difference: 4690.897231602754\n",
      "autoencoder training epoch 152: decode layer difference: 4690.226629053586\n",
      "autoencoder training epoch 153: decode layer difference: 4689.672540437547\n",
      "autoencoder training epoch 154: decode layer difference: 4690.38603788333\n",
      "autoencoder training epoch 155: decode layer difference: 4690.932538579147\n",
      "autoencoder training epoch 156: decode layer difference: 4690.462525105927\n",
      "autoencoder training epoch 157: decode layer difference: 4694.131579280884\n",
      "autoencoder training epoch 158: decode layer difference: 4696.485037224915\n",
      "autoencoder training epoch 159: decode layer difference: 4691.437992903308\n",
      "autoencoder training epoch 160: decode layer difference: 4691.1031606235865\n",
      "autoencoder training epoch 161: decode layer difference: 4690.257468264039\n",
      "autoencoder training epoch 162: decode layer difference: 4690.300870746995\n",
      "autoencoder training epoch 163: decode layer difference: 4690.281661854379\n",
      "autoencoder training epoch 164: decode layer difference: 4690.316056758219\n",
      "autoencoder training epoch 165: decode layer difference: 4690.919280871667\n",
      "autoencoder training epoch 166: decode layer difference: 4689.665053239896\n",
      "autoencoder training epoch 167: decode layer difference: 4691.019503123008\n",
      "autoencoder training epoch 168: decode layer difference: 4690.7798297750505\n",
      "autoencoder training epoch 169: decode layer difference: 4690.826057904292\n",
      "autoencoder training epoch 170: decode layer difference: 4690.463919580605\n",
      "autoencoder training epoch 171: decode layer difference: 4690.696777863444\n",
      "autoencoder training epoch 172: decode layer difference: 4692.931581501331\n",
      "autoencoder training epoch 173: decode layer difference: 4693.245542976641\n",
      "autoencoder training epoch 174: decode layer difference: 4689.331847673641\n",
      "autoencoder training epoch 175: decode layer difference: 4696.232343547492\n",
      "autoencoder training epoch 176: decode layer difference: 4697.259258887452\n",
      "autoencoder training epoch 177: decode layer difference: 4692.780749130033\n",
      "autoencoder training epoch 178: decode layer difference: 4690.725733343943\n",
      "autoencoder training epoch 179: decode layer difference: 4690.6909165654415\n",
      "autoencoder training epoch 180: decode layer difference: 4690.919387196759\n",
      "autoencoder training epoch 181: decode layer difference: 4690.516577091121\n",
      "autoencoder training epoch 182: decode layer difference: 4690.3848339248025\n",
      "autoencoder training epoch 183: decode layer difference: 4690.487086913665\n",
      "autoencoder training epoch 184: decode layer difference: 4690.396135700039\n",
      "autoencoder training epoch 185: decode layer difference: 4689.772091908925\n",
      "autoencoder training epoch 186: decode layer difference: 4690.358901785845\n",
      "autoencoder training epoch 187: decode layer difference: 4690.280921774855\n",
      "autoencoder training epoch 188: decode layer difference: 4690.098947435115\n",
      "autoencoder training epoch 189: decode layer difference: 4690.4271615395355\n",
      "autoencoder training epoch 190: decode layer difference: 4691.109547134537\n",
      "autoencoder training epoch 191: decode layer difference: 4690.339519624616\n",
      "autoencoder training epoch 192: decode layer difference: 4689.8704415193915\n",
      "autoencoder training epoch 193: decode layer difference: 4691.891956953812\n",
      "autoencoder training epoch 194: decode layer difference: 4690.116135199666\n",
      "autoencoder training epoch 195: decode layer difference: 4690.449907630335\n",
      "autoencoder training epoch 196: decode layer difference: 4690.937619224385\n",
      "autoencoder training epoch 197: decode layer difference: 4694.516650498776\n",
      "autoencoder training epoch 198: decode layer difference: 4694.918720255907\n",
      "autoencoder training epoch 199: decode layer difference: 4690.114043395413\n",
      "autoencoder training epoch 200: decode layer difference: 4691.386153573069\n",
      "training epoch 1: error: 21.021837062061607\n",
      "training epoch 2: error: 17.448179967635596\n",
      "training epoch 3: error: 16.878158897003846\n",
      "training epoch 4: error: 16.163661551200413\n",
      "training epoch 5: error: 12.93190745839949\n",
      "training epoch 6: error: 13.373255919501338\n",
      "training epoch 7: error: 13.636290177685385\n",
      "training epoch 8: error: 11.568675915145331\n",
      "training epoch 9: error: 10.451926142288197\n",
      "training epoch 10: error: 11.337367849609112\n",
      "training epoch 11: error: 9.340395174700916\n",
      "training epoch 12: error: 10.117010823684037\n",
      "training epoch 13: error: 9.379611016812952\n",
      "training epoch 14: error: 9.255888014476975\n",
      "training epoch 15: error: 10.281916498139665\n",
      "training epoch 16: error: 9.421108276697677\n",
      "training epoch 17: error: 27.439131797164954\n",
      "training epoch 18: error: 8.50380893055471\n",
      "training epoch 19: error: 10.985713631571487\n",
      "training epoch 20: error: 9.229142754399259\n",
      "\n",
      "fold 4: mse: 2.7785204073983403\n",
      "\n",
      "autoencoder training epoch 1: decode layer difference: 4790.48743907665\n",
      "autoencoder training epoch 2: decode layer difference: 4727.564312107046\n",
      "autoencoder training epoch 3: decode layer difference: 4713.270416964464\n",
      "autoencoder training epoch 4: decode layer difference: 4707.317635219701\n",
      "autoencoder training epoch 5: decode layer difference: 4703.580284343245\n",
      "autoencoder training epoch 6: decode layer difference: 4704.663700204248\n",
      "autoencoder training epoch 7: decode layer difference: 4702.227476535952\n",
      "autoencoder training epoch 8: decode layer difference: 4702.7716058174365\n",
      "autoencoder training epoch 9: decode layer difference: 4704.259748067616\n",
      "autoencoder training epoch 10: decode layer difference: 4701.662288239376\n",
      "autoencoder training epoch 11: decode layer difference: 4702.536600127518\n",
      "autoencoder training epoch 12: decode layer difference: 4701.104376209409\n",
      "autoencoder training epoch 13: decode layer difference: 4701.203877855144\n",
      "autoencoder training epoch 14: decode layer difference: 4701.072949983143\n",
      "autoencoder training epoch 15: decode layer difference: 4702.050556346844\n",
      "autoencoder training epoch 16: decode layer difference: 4702.109417916882\n",
      "autoencoder training epoch 17: decode layer difference: 4701.596388477224\n",
      "autoencoder training epoch 18: decode layer difference: 4705.017696479184\n",
      "autoencoder training epoch 19: decode layer difference: 4700.909245725887\n",
      "autoencoder training epoch 20: decode layer difference: 4700.672753633411\n",
      "autoencoder training epoch 21: decode layer difference: 4700.0101502085245\n",
      "autoencoder training epoch 22: decode layer difference: 4699.884027322483\n",
      "autoencoder training epoch 23: decode layer difference: 4700.886245112334\n",
      "autoencoder training epoch 24: decode layer difference: 4699.640078411635\n",
      "autoencoder training epoch 25: decode layer difference: 4703.72419641928\n",
      "autoencoder training epoch 26: decode layer difference: 4700.841084508496\n",
      "autoencoder training epoch 27: decode layer difference: 4703.409094959984\n",
      "autoencoder training epoch 28: decode layer difference: 4699.879831536001\n",
      "autoencoder training epoch 29: decode layer difference: 4701.504764432779\n",
      "autoencoder training epoch 30: decode layer difference: 4699.670459872037\n",
      "autoencoder training epoch 31: decode layer difference: 4743.30369723621\n",
      "autoencoder training epoch 32: decode layer difference: 4729.440098320729\n",
      "autoencoder training epoch 33: decode layer difference: 4699.74738486542\n",
      "autoencoder training epoch 34: decode layer difference: 4699.940425687197\n",
      "autoencoder training epoch 35: decode layer difference: 4699.701468533246\n",
      "autoencoder training epoch 36: decode layer difference: 4699.171213607231\n",
      "autoencoder training epoch 37: decode layer difference: 4699.311293024616\n",
      "autoencoder training epoch 38: decode layer difference: 4700.2319096753845\n",
      "autoencoder training epoch 39: decode layer difference: 4698.908327205363\n",
      "autoencoder training epoch 40: decode layer difference: 4699.207250963494\n",
      "autoencoder training epoch 41: decode layer difference: 4699.878956581276\n",
      "autoencoder training epoch 42: decode layer difference: 4700.117577303343\n",
      "autoencoder training epoch 43: decode layer difference: 4699.278234132081\n",
      "autoencoder training epoch 44: decode layer difference: 4699.901564723414\n",
      "autoencoder training epoch 45: decode layer difference: 4699.097620738891\n",
      "autoencoder training epoch 46: decode layer difference: 4699.576410677317\n",
      "autoencoder training epoch 47: decode layer difference: 4701.999164698896\n",
      "autoencoder training epoch 48: decode layer difference: 4699.83226137972\n",
      "autoencoder training epoch 49: decode layer difference: 4698.4127705368055\n",
      "autoencoder training epoch 50: decode layer difference: 4698.0582752445325\n",
      "autoencoder training epoch 51: decode layer difference: 4698.263776316509\n",
      "autoencoder training epoch 52: decode layer difference: 4698.436659713621\n",
      "autoencoder training epoch 53: decode layer difference: 4697.571506131365\n",
      "autoencoder training epoch 54: decode layer difference: 4698.451996329702\n",
      "autoencoder training epoch 55: decode layer difference: 4698.646590068343\n",
      "autoencoder training epoch 56: decode layer difference: 4696.69131832045\n",
      "autoencoder training epoch 57: decode layer difference: 4706.05394052872\n",
      "autoencoder training epoch 58: decode layer difference: 4700.30762965621\n",
      "autoencoder training epoch 59: decode layer difference: 4695.877580371885\n",
      "autoencoder training epoch 60: decode layer difference: 4696.907551009251\n",
      "autoencoder training epoch 61: decode layer difference: 4696.915285498133\n",
      "autoencoder training epoch 62: decode layer difference: 4695.849097881858\n",
      "autoencoder training epoch 63: decode layer difference: 4693.780621948195\n",
      "autoencoder training epoch 64: decode layer difference: 4695.183413390014\n",
      "autoencoder training epoch 65: decode layer difference: 4726.204363683783\n",
      "autoencoder training epoch 66: decode layer difference: 4694.541048631195\n",
      "autoencoder training epoch 67: decode layer difference: 4694.670981357419\n",
      "autoencoder training epoch 68: decode layer difference: 4694.608984023509\n",
      "autoencoder training epoch 69: decode layer difference: 4692.311653629242\n",
      "autoencoder training epoch 70: decode layer difference: 4694.097948059811\n",
      "autoencoder training epoch 71: decode layer difference: 4693.438457519544\n",
      "autoencoder training epoch 72: decode layer difference: 4693.379865638827\n",
      "autoencoder training epoch 73: decode layer difference: 4693.004765065508\n",
      "autoencoder training epoch 74: decode layer difference: 4693.414671198534\n",
      "autoencoder training epoch 75: decode layer difference: 4693.8328507999995\n",
      "autoencoder training epoch 76: decode layer difference: 4697.120916587314\n",
      "autoencoder training epoch 77: decode layer difference: 4693.622488761905\n",
      "autoencoder training epoch 78: decode layer difference: 4699.739441910644\n",
      "autoencoder training epoch 79: decode layer difference: 4692.522358659213\n",
      "autoencoder training epoch 80: decode layer difference: 4693.435283034394\n",
      "autoencoder training epoch 81: decode layer difference: 4695.229759918791\n",
      "autoencoder training epoch 82: decode layer difference: 4693.734744467844\n",
      "autoencoder training epoch 83: decode layer difference: 4693.045264743465\n",
      "autoencoder training epoch 84: decode layer difference: 4694.398245191175\n",
      "autoencoder training epoch 85: decode layer difference: 4693.084320482791\n",
      "autoencoder training epoch 86: decode layer difference: 4692.798229870161\n",
      "autoencoder training epoch 87: decode layer difference: 4694.211332637535\n",
      "autoencoder training epoch 88: decode layer difference: 4692.642473755718\n",
      "autoencoder training epoch 89: decode layer difference: 4694.3191764928215\n",
      "autoencoder training epoch 90: decode layer difference: 4696.1439794547305\n",
      "autoencoder training epoch 91: decode layer difference: 4693.967979993732\n",
      "autoencoder training epoch 92: decode layer difference: 4693.433886682357\n",
      "autoencoder training epoch 93: decode layer difference: 4692.75544503024\n",
      "autoencoder training epoch 94: decode layer difference: 4693.367255629281\n",
      "autoencoder training epoch 95: decode layer difference: 4692.5213458364315\n",
      "autoencoder training epoch 96: decode layer difference: 4709.342295596161\n",
      "autoencoder training epoch 97: decode layer difference: 4692.573265246816\n",
      "autoencoder training epoch 98: decode layer difference: 4692.165568387845\n",
      "autoencoder training epoch 99: decode layer difference: 4693.627744274151\n",
      "autoencoder training epoch 100: decode layer difference: 4694.238693981032\n",
      "autoencoder training epoch 101: decode layer difference: 4694.801582912714\n",
      "autoencoder training epoch 102: decode layer difference: 4692.630058765798\n",
      "autoencoder training epoch 103: decode layer difference: 4693.963128799566\n",
      "autoencoder training epoch 104: decode layer difference: 4695.39126208428\n",
      "autoencoder training epoch 105: decode layer difference: 4693.079749451089\n",
      "autoencoder training epoch 106: decode layer difference: 4692.047152498632\n",
      "autoencoder training epoch 107: decode layer difference: 4692.61701925001\n",
      "autoencoder training epoch 108: decode layer difference: 4692.66841266732\n",
      "autoencoder training epoch 109: decode layer difference: 4693.512486728301\n",
      "autoencoder training epoch 110: decode layer difference: 4706.634438227046\n",
      "autoencoder training epoch 111: decode layer difference: 4693.250702628672\n",
      "autoencoder training epoch 112: decode layer difference: 4692.354194108927\n",
      "autoencoder training epoch 113: decode layer difference: 4693.226008221862\n",
      "autoencoder training epoch 114: decode layer difference: 4692.287474999881\n",
      "autoencoder training epoch 115: decode layer difference: 4693.498960108631\n",
      "autoencoder training epoch 116: decode layer difference: 4692.900385400479\n",
      "autoencoder training epoch 117: decode layer difference: 4699.100621849238\n",
      "autoencoder training epoch 118: decode layer difference: 4692.633230729448\n",
      "autoencoder training epoch 119: decode layer difference: 4692.472625452711\n",
      "autoencoder training epoch 120: decode layer difference: 4693.576351810423\n",
      "autoencoder training epoch 121: decode layer difference: 4691.770346366509\n",
      "autoencoder training epoch 122: decode layer difference: 4692.482501154733\n",
      "autoencoder training epoch 123: decode layer difference: 4694.623861869342\n",
      "autoencoder training epoch 124: decode layer difference: 4693.861364427295\n",
      "autoencoder training epoch 125: decode layer difference: 4692.218599959097\n",
      "autoencoder training epoch 126: decode layer difference: 4692.12407550635\n",
      "autoencoder training epoch 127: decode layer difference: 4693.327267786768\n",
      "autoencoder training epoch 128: decode layer difference: 4691.761474169828\n",
      "autoencoder training epoch 129: decode layer difference: 4692.1550940212865\n",
      "autoencoder training epoch 130: decode layer difference: 4691.978327737313\n",
      "autoencoder training epoch 131: decode layer difference: 4692.631938875497\n",
      "autoencoder training epoch 132: decode layer difference: 4700.056993420114\n",
      "autoencoder training epoch 133: decode layer difference: 4694.824537874925\n",
      "autoencoder training epoch 134: decode layer difference: 4698.561530919291\n",
      "autoencoder training epoch 135: decode layer difference: 4691.843636963212\n",
      "autoencoder training epoch 136: decode layer difference: 4692.55772817405\n",
      "autoencoder training epoch 137: decode layer difference: 4701.972164853701\n",
      "autoencoder training epoch 138: decode layer difference: 4692.589106412899\n",
      "autoencoder training epoch 139: decode layer difference: 4693.434641137227\n",
      "autoencoder training epoch 140: decode layer difference: 4691.828454201604\n",
      "autoencoder training epoch 141: decode layer difference: 4691.399048018346\n",
      "autoencoder training epoch 142: decode layer difference: 4693.293067912691\n",
      "autoencoder training epoch 143: decode layer difference: 4696.525727948253\n",
      "autoencoder training epoch 144: decode layer difference: 4692.563771266257\n",
      "autoencoder training epoch 145: decode layer difference: 4722.9610944191545\n",
      "autoencoder training epoch 146: decode layer difference: 4695.884772134448\n",
      "autoencoder training epoch 147: decode layer difference: 4692.181530051005\n",
      "autoencoder training epoch 148: decode layer difference: 4752.675579784807\n",
      "autoencoder training epoch 149: decode layer difference: 4697.140513465168\n",
      "autoencoder training epoch 150: decode layer difference: 4691.173344999419\n",
      "autoencoder training epoch 151: decode layer difference: 4693.343573562506\n",
      "autoencoder training epoch 152: decode layer difference: 4693.709167101536\n",
      "autoencoder training epoch 153: decode layer difference: 4746.482891022642\n",
      "autoencoder training epoch 154: decode layer difference: 4694.167540434967\n",
      "autoencoder training epoch 155: decode layer difference: 4692.975535282134\n",
      "autoencoder training epoch 156: decode layer difference: 4721.5567853168595\n",
      "autoencoder training epoch 157: decode layer difference: 4701.979115193475\n",
      "autoencoder training epoch 158: decode layer difference: 4693.388472059985\n",
      "autoencoder training epoch 159: decode layer difference: 4693.992683682731\n",
      "autoencoder training epoch 160: decode layer difference: 4692.335793174455\n",
      "autoencoder training epoch 161: decode layer difference: 4691.566743030741\n",
      "autoencoder training epoch 162: decode layer difference: 4697.829517126974\n",
      "autoencoder training epoch 163: decode layer difference: 4693.417138535559\n",
      "autoencoder training epoch 164: decode layer difference: 4692.764894770242\n",
      "autoencoder training epoch 165: decode layer difference: 4694.635431806214\n",
      "autoencoder training epoch 166: decode layer difference: 4692.34647936155\n",
      "autoencoder training epoch 167: decode layer difference: 4691.7152633631795\n",
      "autoencoder training epoch 168: decode layer difference: 4696.635022842969\n",
      "autoencoder training epoch 169: decode layer difference: 4691.288878211127\n",
      "autoencoder training epoch 170: decode layer difference: 4694.138841666908\n",
      "autoencoder training epoch 171: decode layer difference: 4692.7765508586\n",
      "autoencoder training epoch 172: decode layer difference: 4692.833079119263\n",
      "autoencoder training epoch 173: decode layer difference: 4692.813398620396\n",
      "autoencoder training epoch 174: decode layer difference: 4694.567129129147\n",
      "autoencoder training epoch 175: decode layer difference: 4693.03971336548\n",
      "autoencoder training epoch 176: decode layer difference: 4692.772360722401\n",
      "autoencoder training epoch 177: decode layer difference: 4692.273619508663\n",
      "autoencoder training epoch 178: decode layer difference: 4694.497677651611\n",
      "autoencoder training epoch 179: decode layer difference: 4692.946489537154\n",
      "autoencoder training epoch 180: decode layer difference: 4692.156548638615\n",
      "autoencoder training epoch 181: decode layer difference: 4694.523349413219\n",
      "autoencoder training epoch 182: decode layer difference: 4698.742783691031\n",
      "autoencoder training epoch 183: decode layer difference: 4692.200224712605\n",
      "autoencoder training epoch 184: decode layer difference: 4691.981436544377\n",
      "autoencoder training epoch 185: decode layer difference: 4730.630215047015\n",
      "autoencoder training epoch 186: decode layer difference: 4691.586848547315\n",
      "autoencoder training epoch 187: decode layer difference: 4693.310126540396\n",
      "autoencoder training epoch 188: decode layer difference: 4694.033342757408\n",
      "autoencoder training epoch 189: decode layer difference: 4695.807868700745\n",
      "autoencoder training epoch 190: decode layer difference: 4694.709624768046\n",
      "autoencoder training epoch 191: decode layer difference: 4692.949866699933\n",
      "autoencoder training epoch 192: decode layer difference: 4699.965674521164\n",
      "autoencoder training epoch 193: decode layer difference: 4692.70984922564\n",
      "autoencoder training epoch 194: decode layer difference: 4693.685522927776\n",
      "autoencoder training epoch 195: decode layer difference: 4692.2444917384155\n",
      "autoencoder training epoch 196: decode layer difference: 4692.029227163189\n",
      "autoencoder training epoch 197: decode layer difference: 4696.089947256058\n",
      "autoencoder training epoch 198: decode layer difference: 4698.12187264827\n",
      "autoencoder training epoch 199: decode layer difference: 4691.479274654529\n",
      "autoencoder training epoch 200: decode layer difference: 4692.496521539474\n",
      "training epoch 1: error: 19.186265772714155\n",
      "training epoch 2: error: 23.32182130678115\n",
      "training epoch 3: error: 17.68269706179216\n",
      "training epoch 4: error: 16.905856430108784\n",
      "training epoch 5: error: 14.475092731992346\n",
      "training epoch 6: error: 15.626394889788601\n",
      "training epoch 7: error: 16.301987336473832\n",
      "training epoch 8: error: 11.231541459338912\n",
      "training epoch 9: error: 10.91431804255198\n",
      "training epoch 10: error: 18.78744212217151\n",
      "training epoch 11: error: 10.277114494460836\n",
      "training epoch 12: error: 10.608825707439195\n",
      "training epoch 13: error: 10.802778665514094\n",
      "training epoch 14: error: 9.003000108368933\n",
      "training epoch 15: error: 9.828217308589556\n",
      "training epoch 16: error: 8.952353302313131\n",
      "training epoch 17: error: 11.742711723049064\n",
      "training epoch 18: error: 8.801607677213642\n",
      "training epoch 19: error: 15.810888811659648\n",
      "training epoch 20: error: 9.07256476439806\n",
      "\n",
      "fold 5: mse: 2.2861484508460475\n",
      "\n",
      "average error: 2.2859422966071152\n"
     ]
    },
    {
     "data": {
      "text/plain": "2.2859422966071152"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_layered(abalone_data, 'rings', train_autoencoder_network_regression, (3,10), z_norm=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1: error: 30.925654991929022\n",
      "training epoch 2: error: 28.39111952342882\n",
      "training epoch 3: error: 24.745586801369363\n",
      "training epoch 4: error: 19.017018541981095\n",
      "training epoch 5: error: 17.884583906430763\n",
      "training epoch 6: error: 19.149815018971548\n",
      "training epoch 7: error: 17.40632464860544\n",
      "training epoch 8: error: 17.651676282093717\n",
      "training epoch 9: error: 18.572801077408513\n",
      "training epoch 10: error: 20.191711618079637\n",
      "training epoch 11: error: 13.783250421814412\n",
      "training epoch 12: error: 12.673538903019335\n",
      "training epoch 13: error: 12.399552227888833\n",
      "training epoch 14: error: 14.55072195555346\n",
      "training epoch 15: error: 15.00672828346552\n",
      "training epoch 16: error: 15.571287532050519\n",
      "training epoch 17: error: 17.401361207282875\n",
      "training epoch 18: error: 12.17453207397839\n",
      "training epoch 19: error: 13.828229577905283\n",
      "training epoch 20: error: 11.752591319927078\n",
      "\n",
      "fold 1: mse: 2.793017087838275\n",
      "\n",
      "training epoch 1: error: 17.751534139340187\n",
      "training epoch 2: error: 22.372314595621695\n",
      "training epoch 3: error: 19.217166280972442\n",
      "training epoch 4: error: 20.641488014134055\n",
      "training epoch 5: error: 24.348634655491807\n",
      "training epoch 6: error: 17.153131727771925\n",
      "training epoch 7: error: 18.8024716862675\n",
      "training epoch 8: error: 18.678941694819507\n",
      "training epoch 9: error: 19.60395683004571\n",
      "training epoch 10: error: 15.972044635841012\n",
      "training epoch 11: error: 13.331687786830216\n",
      "training epoch 12: error: 12.325875320238836\n",
      "training epoch 13: error: 18.638416523261874\n",
      "training epoch 14: error: 13.037791369904387\n",
      "training epoch 15: error: 12.228358230539722\n",
      "training epoch 16: error: 12.582148191561618\n",
      "training epoch 17: error: 13.377448053484553\n",
      "training epoch 18: error: 14.308754486330766\n",
      "training epoch 19: error: 18.857339644654196\n",
      "training epoch 20: error: 11.393348738541382\n",
      "\n",
      "fold 2: mse: 3.2283500590997516\n",
      "\n",
      "training epoch 1: error: 17.689553091551872\n",
      "training epoch 2: error: 17.849638067063907\n",
      "training epoch 3: error: 21.85651428024294\n",
      "training epoch 4: error: 19.926092302122953\n",
      "training epoch 5: error: 18.650120023547704\n",
      "training epoch 6: error: 16.88207121100524\n",
      "training epoch 7: error: 17.44038108617184\n",
      "training epoch 8: error: 19.321557693039715\n",
      "training epoch 9: error: 15.51272597831782\n",
      "training epoch 10: error: 14.95380434810707\n",
      "training epoch 11: error: 19.54304168356054\n",
      "training epoch 12: error: 13.645119058991863\n",
      "training epoch 13: error: 13.865019024759674\n",
      "training epoch 14: error: 12.4467496469985\n",
      "training epoch 15: error: 12.007475948903078\n",
      "training epoch 16: error: 13.77640829104417\n",
      "training epoch 17: error: 12.23017314001571\n",
      "training epoch 18: error: 11.792860126465294\n",
      "training epoch 19: error: 12.437916268760056\n",
      "training epoch 20: error: 11.318616510258124\n",
      "\n",
      "fold 3: mse: 3.0638581336165833\n",
      "\n",
      "training epoch 1: error: 21.30760537701705\n",
      "training epoch 2: error: 25.263083883529166\n",
      "training epoch 3: error: 18.58117153518791\n",
      "training epoch 4: error: 20.59429432475995\n",
      "training epoch 5: error: 28.16583318700875\n",
      "training epoch 6: error: 23.79974426734639\n",
      "training epoch 7: error: 19.47620089149745\n",
      "training epoch 8: error: 18.864476877017673\n",
      "training epoch 9: error: 15.780388998758207\n",
      "training epoch 10: error: 15.925543630931132\n",
      "training epoch 11: error: 19.34087957057416\n",
      "training epoch 12: error: 15.22816386907293\n",
      "training epoch 13: error: 12.992551559403076\n",
      "training epoch 14: error: 12.834423117723741\n",
      "training epoch 15: error: 13.248271996264354\n",
      "training epoch 16: error: 12.315716384855605\n",
      "training epoch 17: error: 11.751914288180291\n",
      "training epoch 18: error: 13.045233006572275\n",
      "training epoch 19: error: 21.61506223709918\n",
      "training epoch 20: error: 11.370189831051798\n",
      "\n",
      "fold 4: mse: 2.498867222488181\n",
      "\n",
      "training epoch 1: error: 17.938135664078153\n",
      "training epoch 2: error: 26.10965892789483\n",
      "training epoch 3: error: 17.951128179906238\n",
      "training epoch 4: error: 24.613460414634247\n",
      "training epoch 5: error: 19.412694155223093\n",
      "training epoch 6: error: 17.257811002630756\n",
      "training epoch 7: error: 29.28660042343606\n",
      "training epoch 8: error: 28.153866044823417\n",
      "training epoch 9: error: 17.440229192091007\n",
      "training epoch 10: error: 17.48365842863788\n",
      "training epoch 11: error: 15.958668126426005\n",
      "training epoch 12: error: 14.368771085528746\n",
      "training epoch 13: error: 15.232082620764356\n",
      "training epoch 14: error: 17.425495426435607\n",
      "training epoch 15: error: 12.395325573680852\n",
      "training epoch 16: error: 16.417439371621306\n",
      "training epoch 17: error: 13.417714916358019\n",
      "training epoch 18: error: 11.987112689965134\n",
      "training epoch 19: error: 11.886481524591467\n",
      "training epoch 20: error: 11.255959460018708\n",
      "\n",
      "fold 5: mse: 2.7024013594476415\n",
      "\n",
      "average error: 2.8572987724980865\n"
     ]
    },
    {
     "data": {
      "text/plain": "2.8572987724980865"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation_layered(abalone_data, 'rings', train_layered_regression_network, (3,10), z_norm=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1: mse: 14.274319256447988\n",
      "training epoch 2: mse: 13.747673605408487\n",
      "training epoch 3: mse: 13.310641538842933\n",
      "training epoch 4: mse: 12.968417209175888\n",
      "training epoch 5: mse: 12.64759973813668\n",
      "training epoch 6: mse: 12.395209883875065\n",
      "training epoch 7: mse: 12.17673678776882\n",
      "training epoch 8: mse: 12.005305554003735\n",
      "training epoch 9: mse: 11.843161088425695\n",
      "training epoch 10: mse: 11.695806472415315\n",
      "training epoch 11: mse: 11.609807238563386\n",
      "training epoch 12: mse: 11.467861249523109\n",
      "training epoch 13: mse: 11.37160389198279\n",
      "training epoch 14: mse: 11.281586264173786\n",
      "training epoch 15: mse: 11.206241396338921\n",
      "training epoch 16: mse: 11.127143597532903\n",
      "training epoch 17: mse: 11.085457877809043\n",
      "training epoch 18: mse: 11.028193815013918\n",
      "training epoch 19: mse: 10.934512551253786\n",
      "training epoch 20: mse: 10.889430212365294\n",
      "\n",
      "fold 1: mse: 2.8772961039332223\n",
      "\n",
      "training epoch 1: mse: 14.431781304702305\n",
      "training epoch 2: mse: 13.92033418645236\n",
      "training epoch 3: mse: 13.46650158262608\n",
      "training epoch 4: mse: 13.108550117805674\n",
      "training epoch 5: mse: 12.831837979168048\n",
      "training epoch 6: mse: 12.542503501699239\n",
      "training epoch 7: mse: 12.3265732297054\n",
      "training epoch 8: mse: 12.135754028534468\n",
      "training epoch 9: mse: 11.975086594518741\n",
      "training epoch 10: mse: 11.835409709268198\n",
      "training epoch 11: mse: 11.752081029845627\n",
      "training epoch 12: mse: 11.603319621769124\n",
      "training epoch 13: mse: 11.50919409589181\n",
      "training epoch 14: mse: 11.41663910704267\n",
      "training epoch 15: mse: 11.335385055268514\n",
      "training epoch 16: mse: 11.260645856940195\n",
      "training epoch 17: mse: 11.210597833166556\n",
      "training epoch 18: mse: 11.16221800653198\n",
      "training epoch 19: mse: 11.069904150930196\n",
      "training epoch 20: mse: 11.012759975868903\n",
      "\n",
      "fold 2: mse: 2.681570192455287\n",
      "\n",
      "training epoch 1: mse: 14.475884738231741\n",
      "training epoch 2: mse: 13.954116215798106\n",
      "training epoch 3: mse: 13.472279794351216\n",
      "training epoch 4: mse: 13.11601764967569\n",
      "training epoch 5: mse: 12.837579089128088\n",
      "training epoch 6: mse: 12.564436942402136\n",
      "training epoch 7: mse: 12.3586562942722\n",
      "training epoch 8: mse: 12.184724955852007\n",
      "training epoch 9: mse: 12.02330523187672\n",
      "training epoch 10: mse: 11.890615911409348\n",
      "training epoch 11: mse: 11.763541076825902\n",
      "training epoch 12: mse: 11.663280105973968\n",
      "training epoch 13: mse: 11.566053077478802\n",
      "training epoch 14: mse: 11.493039357301733\n",
      "training epoch 15: mse: 11.404767661633882\n",
      "training epoch 16: mse: 11.328319727190351\n",
      "training epoch 17: mse: 11.270550444536067\n",
      "training epoch 18: mse: 11.216541374839007\n",
      "training epoch 19: mse: 11.139484075947426\n",
      "training epoch 20: mse: 11.087791748123635\n",
      "\n",
      "fold 3: mse: 2.634002906744424\n",
      "\n",
      "training epoch 1: mse: 14.099346586526249\n",
      "training epoch 2: mse: 13.541003520265932\n",
      "training epoch 3: mse: 13.181183874717286\n",
      "training epoch 4: mse: 12.821721902946322\n",
      "training epoch 5: mse: 12.521066074793893\n",
      "training epoch 6: mse: 12.284575836601876\n",
      "training epoch 7: mse: 12.110834296969603\n",
      "training epoch 8: mse: 11.91193325749984\n",
      "training epoch 9: mse: 11.785197845850218\n",
      "training epoch 10: mse: 11.641957927135355\n",
      "training epoch 11: mse: 11.535236791653547\n",
      "training epoch 12: mse: 11.447614005505141\n",
      "training epoch 13: mse: 11.333149133570291\n",
      "training epoch 14: mse: 11.258909573870508\n",
      "training epoch 15: mse: 11.176002488732983\n",
      "training epoch 16: mse: 11.116476091636633\n",
      "training epoch 17: mse: 11.056430265939419\n",
      "training epoch 18: mse: 10.990606934568262\n",
      "training epoch 19: mse: 10.929779097499232\n",
      "training epoch 20: mse: 10.889614046762322\n",
      "\n",
      "fold 4: mse: 2.8905532862172834\n",
      "\n",
      "training epoch 1: mse: 14.503302613321486\n",
      "training epoch 2: mse: 13.957955561583619\n",
      "training epoch 3: mse: 13.505806135011358\n",
      "training epoch 4: mse: 13.150126062013399\n",
      "training epoch 5: mse: 12.844141426822295\n",
      "training epoch 6: mse: 12.59410849240584\n",
      "training epoch 7: mse: 12.40122838990103\n",
      "training epoch 8: mse: 12.19284240419875\n",
      "training epoch 9: mse: 12.041877011836153\n",
      "training epoch 10: mse: 11.89996602703243\n",
      "training epoch 11: mse: 11.77362540790842\n",
      "training epoch 12: mse: 11.688415751853874\n",
      "training epoch 13: mse: 11.571905043797457\n",
      "training epoch 14: mse: 11.47572416644876\n",
      "training epoch 15: mse: 11.423972982723322\n",
      "training epoch 16: mse: 11.33891963723664\n",
      "training epoch 17: mse: 11.261256407093583\n",
      "training epoch 18: mse: 11.18538279454463\n",
      "training epoch 19: mse: 11.134120374169061\n",
      "training epoch 20: mse: 11.062005826305912\n",
      "\n",
      "fold 5: mse: 2.669967533491021\n",
      "\n",
      "average error: 2.7506780045682477\n"
     ]
    },
    {
     "data": {
      "text/plain": "2.7506780045682477"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation(abalone_data, 'rings', z_norm=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
